<html>
<head>
<title>Generating Cool Storylines Using a T5 Transformer and Having Fun</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨T5å˜å‹å™¨ç”Ÿæˆé…·ç‚«çš„æ•…äº‹æƒ…èŠ‚ï¼Œäº«å—ä¹è¶£</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://pub.towardsai.net/generating-cool-storylines-using-a-t5-transformer-and-having-fun-4a79f6ab8adb?source=collection_archive---------0-----------------------#2021-04-26">https://pub.towardsai.net/generating-cool-storylines-using-a-t5-transformer-and-having-fun-4a79f6ab8adb?source=collection_archive---------0-----------------------#2021-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ed51" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">è‡ªç„¶è¯­è¨€å¤„ç†</a></h2><div class=""/><div class=""><h2 id="425c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">t5â€”â€”ä¸€ä¸ªç©·äººçš„GPT 3</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/be447db577aea8c75da2477617463199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKKpveyt8vg4mxzWeJVn2Q.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@techdailyca?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">ç§‘æŠ€æ—¥æŠ¥</a>åœ¨<a class="ae lh" href="https://unsplash.com/s/photos/streaming?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šæ‹æ‘„çš„ç…§ç‰‡</figcaption></figure><h1 id="dd4e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">T5å˜å‹å™¨ç®€ä»‹</h1><p id="ef42" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">Google AIçš„äººå‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡â€œæ¢ç´¢ä½¿ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨è¿›è¡Œè¿ç§»å­¦ä¹ çš„é™åˆ¶â€ï¼Œå¹¶ä»‹ç»äº†ä¸€é¡¹å…³äºä»€ä¹ˆç±»å‹çš„é¢„è®­ç»ƒæ–¹æ³•æˆ–è¿ç§»å­¦ä¹ æŠ€æœ¯æœ€æœ‰æ•ˆçš„å®è¯ç ”ç©¶ï¼Œç„¶åä½¿ç”¨è¯¥ç ”ç©¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œå³æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨(T5)ã€‚è¿™ä¸ªtransformeræ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªæ›´å¹²å‡€çš„æ™®é€šçˆ¬è¡Œè¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè°·æ­Œå°†å…¶å‘½åä¸ºå·¨å¤§çš„å¹²å‡€çˆ¬è¡Œè¯­æ–™åº“(C4)ã€‚å¬èµ·æ¥å¾ˆé…·å§ğŸ˜ã€‚å½“æ‚¨æ£€æŸ¥è¯¥æ¨¡å‹çš„èƒ½åŠ›å’Œçµæ´»æ€§ï¼Œä»¥ä¾¿ç”¨å¾ˆå°‘åˆ°ä¸­ç­‰çš„æ•°æ®å¯¹å¤§é‡çš„ä¸‹æ¸¸NLPé—®é¢˜è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒçš„æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚</p><h1 id="336e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">ç°åœ¨ä½ å¯èƒ½ä¼šæƒ³ï¼ŒT5å’Œå…¶ä»–å‹å·çš„å˜å½¢é‡‘åˆšæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ</h1><p id="877c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦çœ‹çœ‹å…¶ä»–è½¬æ¢å™¨ï¼Œå¦‚ä¼¯ç‰¹ã€GPTç­‰ã€‚æ‰€æœ‰è¿™äº›è½¬æ¢å™¨éƒ½é’ˆå¯¹å¤§é‡æ•°æ®è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä½†ä¸ºäº†å¯¹åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä¼šæ·»åŠ ä¸€ä¸ªåˆ†ç±»å±‚å¹¶è¾“å‡ºä¸€ä¸ªåˆ†ç±»æ ‡ç­¾ï¼Œæˆ–è€…ä¸ºNERè¾“å‡ºä¸€ä¸ªè¾“å…¥èŒƒå›´ã€‚ä½†æ˜¯åœ¨T5ï¼Œä¸€åˆ‡éƒ½æ˜¯æœ‰åºçš„ï¼Œå°±åƒä»–ä»¬è¯´çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€ã€‚å› æ­¤ï¼Œå¯¹äºåˆ†ç±»ï¼Œå®ƒå°†æ˜¯å­—ç¬¦ä¸²è¾“å‡ºï¼Œå¯¹äºNERï¼Œå®ƒå°†æ˜¯å­—ç¬¦ä¸²ï¼Œç”šè‡³å¯¹äºå›å½’ï¼Œå®ƒä¹Ÿæ˜¯å­—ç¬¦ä¸²ã€‚ä¸ä¿¡æˆ‘çœ‹æŠ¥çº¸ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œè¯¥æ¨¡å‹ä¸å—ä¸€å®šæ•°é‡çš„ç±»çš„é™åˆ¶ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å¤„ç†å¤šä¸ªæ–‡æœ¬åˆ†ç±»æ•°æ®å¹¶è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼Œè€Œæ— éœ€æ‹…å¿ƒæ ‡ç­¾çš„å¤„ç†ã€‚å› ä¸ºä½ ç»™å®ƒçš„ä¸€åˆ‡éƒ½åªæ˜¯ä¸€ä¸ªåºåˆ—ã€‚è¿™ç¡®å®ä¸ºæ¨¡å‹æä¾›äº†å¾ˆå¤šè¡¨è¾¾èƒ½åŠ›ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒå¯èƒ½ä¼šé¢„æµ‹æ ‡ç­¾ç©ºé—´ä¹‹å¤–çš„ä¸€äº›ä¸œè¥¿ã€‚æˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•äººæŠ¥å‘Šè¿™ä¸€ç‚¹æˆ–æœ‰è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥å®ƒåº”è¯¥å·¥ä½œè‰¯å¥½ã€‚</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/783a9f227591d7fa91226e177255914f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*Ps6f66o2rr-sDEAM.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">æ¥æº:è°·æ­Œäººå·¥æ™ºèƒ½åšå®¢</figcaption></figure><p id="a35d" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">è¿™æ˜¯å¾ˆå¤šå…³äºå˜å½¢é‡‘åˆšåŠå…¶èƒŒåçš„æƒ³æ³•çš„è®¨è®ºï¼Œå¦‚æœä½ æƒ³é˜…è¯»æ›´å¤šï¼Œè¯·æŸ¥çœ‹è®ºæ–‡å’Œè°·æ­Œå®˜æ–¹äººå·¥æ™ºèƒ½åšå®¢ã€‚</p><ul class=""><li id="f899" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™</a></li><li id="fed4" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated"><a class="ae lh" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">è°·æ­Œäººå·¥æ™ºèƒ½åšå®¢</a></li></ul><p id="5197" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ä»£ç ï¼Œæ„å»ºä¸€äº›éå¸¸æœ‰è¶£çš„ä¸œè¥¿ã€‚</p><h1 id="7557" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">æˆ‘ä»¬ä»Šå¤©åœ¨å»ºé€ ä»€ä¹ˆï¼Ÿ</h1><p id="4c28" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">æ­£å¦‚åšå®¢çš„æ ‡é¢˜æ‰€æš—ç¤ºçš„ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªæ•…äº‹æƒ…èŠ‚ç”Ÿæˆå™¨ã€‚ä½†æ˜¯å¯¹äºæˆ‘ä»¬å°†ç”¨æ¥è¾“å‡ºæ•…äº‹æƒ…èŠ‚/æƒ…èŠ‚çš„è¾“å…¥æœ‰ä¸€ç‚¹å°å°çš„å˜åŒ–ã€‚ç”±äºtransformeræ˜¯Sequence2Sequenceæˆ–è°·æ­Œæ‰€è¯´çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€,æˆ‘ä»¬å°†é€šè¿‡è¾“å…¥ç±»å‹ã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç§æ—å’Œæ•…äº‹çº¿/æƒ…èŠ‚çš„è®­ç»ƒä½œä¸ºè¾“å‡ºæ¥å¾®è°ƒè¿™ä¸ªæ¨¡å‹ã€‚è¿™æ ·åšæœ‰åŠ©äºæˆ‘ä»¬çœ‹åˆ°å»ºè®®çš„Sequence2Sequence(â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€)è½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p><p id="b267" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">è¦äº†è§£æœ¬æ•™ç¨‹çš„æ•ˆæœå¦‚ä½•ï¼Œä½ å¯ä»¥ç‚¹å‡»è¿™ä¸ª<a class="ae lh" href="https://movie-plot-generator.vercel.app/" rel="noopener ugc nofollow" target="_blank">é“¾æ¥</a>å»æœªæ¥çœ‹çœ‹è¿™ä¸ªæ¨¡å‹å¦‚ä½•è¿è¡Œï¼Œç„¶åå›åˆ°è¿™é‡Œä¸ºä½ è‡ªå·±å»ºé€ å®ƒğŸ˜‚ã€‚</p><div class="nq nr gp gr ns nt"><a href="https://movie-plot-generator.vercel.app/" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">ç”µå½±æƒ…èŠ‚ç”Ÿæˆå™¨</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">æˆ‘åœ¨ç½‘ä¸Šç”Ÿæˆæ¨¡ç³Šçš„ç”µå½±æƒ…èŠ‚(ä½†æœ‰æ—¶å¾ˆå¥½)ã€‚ä½†æˆ‘å¯ä»¥å‘ä½ ä¿è¯ï¼Œå®ƒå°†æ°¸è¿œæ˜¯â€¦</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">ç”µå½±-æƒ…èŠ‚-ç”Ÿæˆå™¨. vercel.app</p></div></div></div></a></div><h1 id="6a91" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">æˆ‘ä»¬å°†ä½¿ç”¨å“ªäº›æ•°æ®ï¼Ÿ</h1><p id="f22e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†æ˜¯<a class="ae lh" href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots" rel="noopener ugc nofollow" target="_blank">ç»´åŸºç™¾ç§‘ç”µå½±æƒ…èŠ‚</a>æ•°æ®é›†ã€‚å®ƒæœ‰å…³äºå‘è¡Œå¹´ä»½ã€æ ‡é¢˜ã€ç§æ—ã€å¯¼æ¼”ã€æ¼”å‘˜ã€æƒ…èŠ‚ã€æµæ´¾ã€ç»´åŸºé¡µé¢å’Œæƒ…èŠ‚çš„æ•°æ®ã€‚æˆ‘ä»¬å°†åªä½¿ç”¨ç±»å‹ï¼Œå¯¼æ¼”ï¼Œæ¼”å‘˜ï¼Œç§æ—å’Œæƒ…èŠ‚æ¥å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‚¨å¯ä»¥éšæ„å°è¯•ä»»ä½•å…¶ä»–é¢å¤–çš„åˆ—æˆ–ä»»ä½•å…¶ä»–æ•°æ®é›†ã€‚</p><h1 id="5230" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">å¦‚ä½•å‡†å¤‡èµ„æ–™ï¼Ÿ</h1><p id="c0b7" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">è¿™æ˜¯å»ºæ¨¡æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ã€‚å› æ­¤ï¼Œè¯·ä»ä¸Šé¢çš„é“¾æ¥ä¸‹è½½æ•°æ®ï¼Œå¹¶æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤æ“ä½œã€‚</p><ul class=""><li id="7298" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">ä¸€äº›æ­£è§„è¿›å£</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="732a" class="oi lj it oe b gy oj ok l ol om">import os<br/>import re<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm_notebook, tnrange<br/>from sklearn.utils import shuffle<br/>import pickle</span></pre><ul class=""><li id="b4ba" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">ä¸€äº›ä¿å­˜æ•°æ®çš„åŠŸèƒ½</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="cde0" class="oi lj it oe b gy oj ok l ol om">def save_pickle(path, obj):<br/>  with open(path, 'wb') as fp:<br/>    pickle.dump(obj, fp)</span><span id="be8a" class="oi lj it oe b gy on ok l ol om">def load_pickle(path):<br/>  with open(path, 'rb') as fp:<br/>    return pickle.load(fp)</span></pre><ul class=""><li id="c4bb" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">è§£å‹æ•°æ®ï¼ŒåŠ è½½ç†ŠçŒ«</em>ğŸ¼ ğŸ¼ ğŸ¼</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="1ccb" class="oi lj it oe b gy oj ok l ol om">movieDf = pd.read_csv('drive/MyDrive/MoviePlotsModels/data/wiki_movie_plots_deduped.csv')</span></pre><ul class=""><li id="e983" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">æ”¶é›†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹æ‰€éœ€çš„ä»»ä½•ä¸œè¥¿</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="17e7" class="oi lj it oe b gy oj ok l ol om">outputs = []</span><span id="413d" class="oi lj it oe b gy on ok l ol om">with tqdm_notebook(total = len(movieDf)) as pbar:<br/>  for ix, row in movieDf.iterrows():<br/>    </span><span id="7e4d" class="oi lj it oe b gy on ok l ol om">    genre_to_plot_input = f'generate plot for genre: {row.Genre}'<br/>    genre_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_to_plot_input,<br/>        "target": genre_to_plot_output<br/>    })</span><span id="519a" class="oi lj it oe b gy on ok l ol om">    genre_director_to_plot_input = f'generate plot for: genre {row.Genre} and director: {row.Director}'<br/>    genre_director_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_director_to_plot_input,<br/>        "target": genre_director_to_plot_output<br/>    })</span><span id="8511" class="oi lj it oe b gy on ok l ol om">    genre_director_ethinicity_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and ethinicity {row["Origin/Ethnicity"]}'<br/>    genre_director_ethinicity_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_director_ethinicity_to_plot_input,<br/>        "target": genre_director_ethinicity_to_plot_output<br/>    })</span><span id="bcc4" class="oi lj it oe b gy on ok l ol om">    if not pd.isna(row.Cast):<br/>      <br/>      genre_director_cast_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and cast: {row.Cast}'<br/>      genre_director_cast_to_plot_output = row.Plot</span><span id="6e85" class="oi lj it oe b gy on ok l ol om">      outputs.append({<br/>          "source": genre_director_cast_to_plot_input,<br/>          "target": genre_director_cast_to_plot_output<br/>      })</span><span id="46db" class="oi lj it oe b gy on ok l ol om">    <br/>    pbar.update(1)</span></pre><ul class=""><li id="5b55" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">è®©æˆ‘ä»¬ä¿å­˜æ•°æ®</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="9a41" class="oi lj it oe b gy oj ok l ol om">save_pickle('t5-source-target-data.pkl', outputs)</span></pre><h1 id="ab66" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">è®©æˆ‘ä»¬è®­ç»ƒ(å¾®è°ƒğŸ˜…)æˆ‘ä»¬çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€è½¬æ¢å™¨</h1><p id="62f2" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">ä»ä¸Šé¢çš„æ ‡é¢˜ä½ å¯èƒ½å·²ç»æ˜ç™½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæ¥è‡ª<strong class="mc jd"> HuggingFaceçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬ç¡®å®è¦ä½¿ç”¨<code class="fe oo op oq oe b"><strong class="mc jd">transformers</strong></code>åº“ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹è®­ç»ƒä»£ç ã€‚åœ¨å¼€å§‹ç¼–ç ä¹‹å‰ï¼Œå®‰è£…ä¸‹é¢çš„åŒ…ï¼Œé‡å¯ä½ çš„Colabå¹¶å¼€å§‹è®­ç»ƒã€‚å¦‚æœä½ ä¸é‡å¯ï¼Œä½ å¯èƒ½ä¼šåœ¨åˆå§‹åŒ–æ¨¡å‹å’Œä»¤ç‰ŒåŒ–å™¨çš„æ—¶å€™é‡åˆ°ä¸€äº›é”™è¯¯ï¼Œå½“ä½ åœ¨Stackoverflowå’ŒGithubä¸Šæœç´¢äº†ä¸€ä¸ªå°æ—¶ä¹‹åï¼Œä½ å¯èƒ½ä¼šæ‰¾åˆ°è§£å†³æ–¹æ³•ï¼Œé‚£å°±æ˜¯åœ¨å®‰è£…å®ŒåŒ…ä¹‹åé‡å¯Colabç¬”è®°æœ¬ã€‚å› æ­¤ï¼Œåªéœ€æŒ‰ç…§å®‰è£…åé‡æ–°å¯åŠ¨Colabç¬”è®°æœ¬çš„æ­¥éª¤æ“ä½œï¼Œå®ƒå°†ä¸ºæ‚¨èŠ‚çœæ—¶é—´ã€‚</strong></p><ul class=""><li id="bfb3" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">å®‰è£…è¿™äº›åŒ…(ä½ ä¼šçœ‹åˆ°å®ƒä»¬çš„é‡è¦æ€§)</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="d38e" class="oi lj it oe b gy oj ok l ol om">!pip install sentencepiece<br/>!pip install transformers<br/>!pip install torch<br/>!pip install rich[jupyter]</span></pre><p id="2211" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated"><strong class="mc jd"> <em class="oc">é¡ºä¾¿è¯´ä¸€å¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨PyTorchï¼Œæ‰€ä»¥å¦‚æœä½ ä¸€ç›´è®¤ä¸ºè¿™æ˜¯TensorFlowï¼Œæˆ‘å¾ˆæŠ±æ­‰è®©ä½ å¤±æœ›äº†ã€‚æˆ‘çŒœ</em> </strong>ä½ å¯èƒ½æƒ³æ”¹å˜ä½ çš„æ¡†æ¶ğŸ¤£</p><ul class=""><li id="dd13" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">å†æ¬¡å¸¸è§„è¿›å£</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="5ae5" class="oi lj it oe b gy oj ok l ol om">import os<br/>import re<br/>import random<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm_notebook, tnrange<br/>from sklearn.utils import shuffle<br/>import pickle<br/>import math<br/>## use if working on jupyter notebook or colab<br/>from IPython.display import clear_output</span></pre><ul class=""><li id="c322" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">ä¸€äº›ä¿å­˜å’ŒåŠ è½½æ•°æ®çš„åŠŸèƒ½</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="b3de" class="oi lj it oe b gy oj ok l ol om">def save_pickle(path, obj):<br/>  with open(path, 'wb') as fp:<br/>    pickle.dump(obj, fp)</span><span id="8756" class="oi lj it oe b gy on ok l ol om">def load_pickle(path):<br/>  with open(path, 'rb') as fp:<br/>    return pickle.load(fp)</span></pre><p id="3233" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">D <em class="oc">å¦‚æœä½ ç”¨çš„æ˜¯åŒä¸€ä¸ªColabç¬”è®°æœ¬</em>å°±ä¸è¦å†å¤åˆ¶ç²˜è´´äº†</p><ul class=""><li id="31df" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">å‹å·ç›¸å…³è¿›å£</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="a919" class="oi lj it oe b gy oj ok l ol om">import numpy as np<br/>import torch<br/>import torch.nn.functional as F<br/>from torch.utils.data import Dataset, DataLoader<br/>from transformers import T5Tokenizer, T5ForConditionalGeneration<br/>from rich.table import Column, Table<br/>from rich import box<br/>from rich.console import Console</span><span id="d0b9" class="oi lj it oe b gy on ok l ol om">from torch import cuda<br/>device = 'cuda' if cuda.is_available() else 'cpu'</span></pre><ul class=""><li id="351d" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">è®¾ç½®ä¸°å¯Œçš„æ§åˆ¶å°ï¼Œä»¥æ›´å¥½çš„æ–¹å¼æ˜¾ç¤ºåˆ†æ•°</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="c5be" class="oi lj it oe b gy oj ok l ol om">console = Console(record=True)</span><span id="d1f5" class="oi lj it oe b gy on ok l ol om">training_logger = Table(<br/>    Column("Random Selection", justify = "center"),<br/>    Column("Epoch", justify="center"),<br/>    Column("Loss", justify="center"),<br/>    title="Training Status",<br/>    pad_edge=False,<br/>    box=box.ASCII,<br/>)</span><span id="b202" class="oi lj it oe b gy on ok l ol om">valid_loggger = Table(<br/>    Column("Random Selection", justify = "center"),<br/>    Column("Loss", justify = "center"),<br/>    title="Validation Status",<br/>    pad_edge=False,<br/>    box=box.ASCII,<br/>)</span></pre><ul class=""><li id="caef" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">åŠ è½½é¢„å¤„ç†æ•°æ®</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="dd54" class="oi lj it oe b gy oj ok l ol om">data = load_pickle('/content/drive/MyDrive/T5MovieWikiTraining2_0/t5-source-target-data-2.pkl')</span></pre><ul class=""><li id="0787" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc"> PyTorch </em> <code class="fe oo op oq oe b"><em class="oc">Dataset</em></code> <em class="oc">ç‰©ä½“</em></li></ul><p id="ada0" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">åˆå§‹åŒ–æ­¤å¯¹è±¡æ—¶ä¼ é€’æ ‡è®°åŒ–å™¨å®ä¾‹</p><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="583e" class="oi lj it oe b gy oj ok l ol om">class T5Dataset(Dataset):</span><span id="8829" class="oi lj it oe b gy on ok l ol om">  def __init__(self, tokenizer, data, source_len, target_len):</span><span id="005a" class="oi lj it oe b gy on ok l ol om">    super(T5Dataset, self).__init__()<br/>    self.tokenizer = tokenizer<br/>    self.source_len = source_len<br/>    self.target_len = target_len<br/>    self.data = data</span><span id="fc95" class="oi lj it oe b gy on ok l ol om">  def __len__(self):</span><span id="bdc2" class="oi lj it oe b gy on ok l ol om">    return len(self.data)</span><span id="ce57" class="oi lj it oe b gy on ok l ol om">  def __getitem__(self, index):</span><span id="08a8" class="oi lj it oe b gy on ok l ol om">    source_seq = self.data[index]['source']<br/>    target_seq = self.data[index]['target']</span><span id="20cb" class="oi lj it oe b gy on ok l ol om">    source = self.tokenizer.batch_encode_plus(<br/>        [source_seq],<br/>        max_length = self.source_len,<br/>        pad_to_max_length = True,<br/>        truncation = True,<br/>        padding = "max_length",<br/>        return_tensors = "pt"<br/>    )</span><span id="80a2" class="oi lj it oe b gy on ok l ol om">    target = self.tokenizer.batch_encode_plus(<br/>        [target_seq],<br/>        max_length = self.target_len,<br/>        pad_to_max_length = True,<br/>        truncation = True,<br/>        padding = "max_length",<br/>        return_tensors = "pt"<br/>    )</span><span id="3aea" class="oi lj it oe b gy on ok l ol om">    source_ids = source["input_ids"].squeeze()<br/>    source_mask = source["attention_mask"].squeeze()<br/>    target_ids = target["input_ids"].squeeze()<br/>    target_mask = target["attention_mask"].squeeze()</span><span id="8e2a" class="oi lj it oe b gy on ok l ol om">    return {<br/>        "source_ids": source_ids,<br/>        "source_mask": source_mask,<br/>        "target_ids": target_ids,<br/>        "target_mask": target_mask<br/>    }</span></pre><ul class=""><li id="999e" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">è®­ç»ƒåŠŸèƒ½</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="ee96" class="oi lj it oe b gy oj ok l ol om">def train(epoch, tokenizer, model, device, loader, optimizer):</span><span id="483a" class="oi lj it oe b gy on ok l ol om">    model.train()</span><span id="5630" class="oi lj it oe b gy on ok l ol om">    total_loss = 0<br/>    total_counts = 0</span><span id="0875" class="oi lj it oe b gy on ok l ol om">    for _, data in enumerate(tqdm_notebook(loader, desc = "Train DL")):</span><span id="44a3" class="oi lj it oe b gy on ok l ol om">        y = data["target_ids"].to(device, dtype = torch.long)<br/>        y_ids = y[:, :-1].contiguous()<br/>        lm_labels = y[:, 1:].clone().detach()<br/>        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100<br/>        ids = data["source_ids"].to(device, dtype = torch.long)<br/>        mask = data["source_mask"].to(device, dtype = torch.long)</span><span id="9ed0" class="oi lj it oe b gy on ok l ol om">        optimizer.zero_grad()</span><span id="e701" class="oi lj it oe b gy on ok l ol om">        outputs = model(<br/>            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels<br/>        )</span><span id="bc08" class="oi lj it oe b gy on ok l ol om">        loss = outputs[0]</span><span id="7788" class="oi lj it oe b gy on ok l ol om">        total_counts += 1<br/>        total_loss += loss.item()</span><span id="217b" class="oi lj it oe b gy on ok l ol om">        <br/>        loss.backward()<br/>        optimizer.step()</span><span id="a0dc" class="oi lj it oe b gy on ok l ol om">    <br/>    return total_loss/total_counts</span></pre><ul class=""><li id="da31" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">éªŒè¯åŠŸèƒ½</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="cb77" class="oi lj it oe b gy oj ok l ol om">def validate(epoch, tokenizer, model, device, loader):</span><span id="2d91" class="oi lj it oe b gy on ok l ol om">    model.eval()<br/>    total_loss = 0<br/>    total_counts = 0</span><span id="0c23" class="oi lj it oe b gy on ok l ol om">    with torch.no_grad():</span><span id="968b" class="oi lj it oe b gy on ok l ol om">        for _, data in enumerate(tqdm_notebook(loader, desc = "Valid DL")):</span><span id="e4ae" class="oi lj it oe b gy on ok l ol om">            y = data["target_ids"].to(device, dtype = torch.long)<br/>            y_ids = y[:, :-1].contiguous()<br/>            lm_labels = y[:, 1:].clone().detach()<br/>            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100<br/>            ids = data["source_ids"].to(device, dtype = torch.long)<br/>            mask = data["source_mask"].to(device, dtype = torch.long)</span><span id="6dfd" class="oi lj it oe b gy on ok l ol om">            outputs = model(<br/>            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels<br/>            )</span><span id="8e3b" class="oi lj it oe b gy on ok l ol om">            loss = outputs[0]</span><span id="b992" class="oi lj it oe b gy on ok l ol om">            total_loss += loss.item()<br/>            total_counts += 1</span><span id="5fdf" class="oi lj it oe b gy on ok l ol om">    return total_loss / total_counts</span></pre><ul class=""><li id="2004" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">è®©æˆ‘ä»¬æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="4b6b" class="oi lj it oe b gy oj ok l ol om">def trainer(<br/>    data, model_params, output_dir = "./outputs/"<br/>):</span><span id="6923" class="oi lj it oe b gy on ok l ol om">    torch.manual_seed(model_params["SEED"])<br/>    torch.cuda.manual_seed(model_params["SEED"])<br/>    np.random.seed(model_params["SEED"])<br/>    torch.backends.cudnn.deterministic = True<br/>    </span><span id="eb4c" class="oi lj it oe b gy on ok l ol om">    console.log(f'''Model: Loading {model_params['MODEL']}.....''')</span><span id="b2a5" class="oi lj it oe b gy on ok l ol om">    tokenizer = T5Tokenizer.from_pretrained(model_params["MODEL"])</span><span id="ea84" class="oi lj it oe b gy on ok l ol om">    model = T5ForConditionalGeneration.from_pretrained(model_params["MODEL"])</span><span id="04b2" class="oi lj it oe b gy on ok l ol om">    model = model.to(device)</span><span id="23c6" class="oi lj it oe b gy on ok l ol om">    console.log(f"[DATA]: READING DATA.......")</span><span id="c353" class="oi lj it oe b gy on ok l ol om">    optimizer = torch.optim.Adam(<br/>        params=model.parameters(), lr=model_params["LEARNING_RATE"]<br/>    )</span><span id="bc39" class="oi lj it oe b gy on ok l ol om">    # Training loop<br/>    console.log(f"[Initiating Fine Tuning]...\\n")</span><span id="355b" class="oi lj it oe b gy on ok l ol om">    ## model save path</span><span id="5adc" class="oi lj it oe b gy on ok l ol om">    path = os.path.join(output_dir, "model_files")</span><span id="0ed4" class="oi lj it oe b gy on ok l ol om">    <br/>    console.log("Starting with Random Selection")<br/>    ## random selection<br/>    prev_loss = []<br/>    for randomSelection in tnrange(model_params["RANDOM_TRAIN_STEPS"], desc = 'Random Selection'):<br/>        <br/>        copyData = data.copy()<br/>        copyData = shuffle(copyData)<br/>      </span><span id="6a45" class="oi lj it oe b gy on ok l ol om">        train_size = 0.75<br/>        random_permuts = np.random.permutation(len(copyData))<br/>        train_nums = round(len(random_permuts) * train_size)<br/>        train_dataset = [copyData[i] for i in random_permuts[:train_nums]]<br/>        valid_dataset = [copyData[i] for i in random_permuts[train_nums:]] </span><span id="e490" class="oi lj it oe b gy on ok l ol om">        training_set = T5Dataset(<br/>            tokenizer, train_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]<br/>        )</span><span id="4161" class="oi lj it oe b gy on ok l ol om">        val_set = T5Dataset(<br/>            tokenizer, valid_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]<br/>        )</span><span id="73a7" class="oi lj it oe b gy on ok l ol om">        train_params = {<br/>            "batch_size": model_params["TRAIN_BATCH_SIZE"],<br/>            "shuffle": True,<br/>            "num_workers": 0<br/>        }</span><span id="de20" class="oi lj it oe b gy on ok l ol om">        val_params = {<br/>            "batch_size": model_params["VALID_BATCH_SIZE"],<br/>            "shuffle": False,<br/>            "num_workers": 0<br/>        }<br/>        <br/>        train_dl = DataLoader(training_set, **train_params)<br/>        </span><span id="5218" class="oi lj it oe b gy on ok l ol om">        ## training <br/>        console.log(f'[MODEL TRAINING]')<br/>        clear_output(wait = True)<br/>        for epoch in tnrange(model_params["TRAIN_EPOCHS"], desc = "Training"):<br/>            <br/>            total_loss = train(epoch, tokenizer, model, device, train_dl, optimizer)</span><span id="b090" class="oi lj it oe b gy on ok l ol om">            training_logger.add_row(str(randomSelection), str(epoch), str(total_loss))<br/>            console.log(training_logger)<br/>            if epoch == 0:<br/>              console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")<br/>              model.save_pretrained(os.path.join(output_dir, "model_files_initial"))<br/>              tokenizer.save_pretrained(os.path.join(output_dir, "model_files_initial"))</span><span id="c4d8" class="oi lj it oe b gy on ok l ol om">            if epoch &gt; 0:</span><span id="2b9e" class="oi lj it oe b gy on ok l ol om">                if min(prev_loss) &gt; total_loss:<br/>                    console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")<br/>                    model.save_pretrained(path)<br/>                    tokenizer.save_pretrained(path)</span><span id="e47a" class="oi lj it oe b gy on ok l ol om">            prev_loss.append(total_loss)<br/>        del train_dl, training_set<br/>        ## validation<br/>        valid_dl = DataLoader(val_set, **val_params)<br/>        console.log(f'[MODEL VALIDATION]')<br/>        for epoch in tnrange(model_params["VAL_EPOCHS"], desc = "Validation"):</span><span id="94dc" class="oi lj it oe b gy on ok l ol om">            val_loss = validate(epoch, tokenizer, model, device, valid_dl)<br/>            <br/>            valid_loggger.add_row(str(randomSelection), str(val_loss))<br/>            console.log(valid_loggger)<br/>        console.save_text(os.path.join(output_dir, f"logs-random-{randomSelection}.txt"))</span><span id="fd48" class="oi lj it oe b gy on ok l ol om">        console.log(f"[VALIDATAION DONE]")     <br/>        del valid_dl, val_set</span></pre><p id="776c" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">æ‚¨å¯ä»¥ä½¿ç”¨éšæœºé€‰æ‹©ç­–ç•¥å¯¹æ¯Nä¸ªæ—¶æœŸçš„å°æ•°æ®å—è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨Nä¸ªæ—¶æœŸåéšæœºé‡‡æ ·å¦ä¸€ä¸ªæ•°æ®å—ï¼Œå¹¶å†æ¬¡å¯¹æ–°é‡‡æ ·çš„æ•°æ®å—è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æ‚¨æ›´å¿«åœ°è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”æ¨¡å‹ä¹Ÿå¯ä»¥çœ‹åˆ°æ‰€æœ‰æ•°æ®ã€‚ä½†æ˜¯å»ºè®®ä»…åœ¨ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ä½¿ç”¨ã€‚</p><ul class=""><li id="875c" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">æ¨¡å‹å‚æ•°</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="c21b" class="oi lj it oe b gy oj ok l ol om">model_params = {  <br/>    "MODEL": "t5-base", # if you have a bigger machine you can use t5-large<br/>    "TRAIN_BATCH_SIZE": 2,<br/>    "VALID_BATCH_SIZE": 2,<br/>    "TRAIN_EPOCHS": 10,<br/>    "VAL_EPOCHS": 1,<br/>    "LEARNING_RATE": 1e-4,<br/>    "MAX_SOURCE_TEXT_LENGTH": 128,<br/>    "MAX_TARGET_TEXT_LENGTH": 786,<br/>    "SEED": 3007,<br/>    "RANDOM_TRAIN_STEPS": 50<br/>}</span></pre><ul class=""><li id="55ab" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">è®©æˆ‘ä»¬è®­ç»ƒè¿™åªé‡å…½ğŸ»</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="9f8a" class="oi lj it oe b gy oj ok l ol om">trainer(<br/>    data = data,<br/>    model_params = model_params,<br/>    output_dir = "./outputs"<br/>)</span></pre><p id="cc4b" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">è¿™å°†å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¯Nä¸ªè®­ç»ƒæ—¶æœŸåï¼Œæ‚¨ä¹Ÿå°†æ”¶åˆ°ä¸€ä¸ªéªŒè¯åˆ†æ•°ã€‚</p><h1 id="cdc0" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">æ¦‚æ‹¬èµ·æ¥</h1><p id="9496" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†Sequence2Sequence TransformerèƒŒåçš„æ–¹æ³•ï¼Œæˆ–è€…è°·æ­Œå–œæ¬¢ç§°ä¹‹ä¸ºâ€œæ–‡æœ¬åˆ°æ–‡æœ¬â€è½¬æ¢å™¨(T5)ã€‚æˆ‘ä»¬è¿˜å¿«é€Ÿè¿è¡Œäº†æ•°æ®å‡†å¤‡æµç¨‹å’ŒåŸ¹è®­æµç¨‹ï¼Œä»¥ä¾¿è½»æ¾å¿«é€Ÿåœ°å¯¹é¢„åŸ¹è®­çš„T5è½¬æ¢å™¨è¿›è¡Œæ–‡æœ¬ç”Ÿæˆä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒã€‚å¸Œæœ›ä½ åœ¨å»ºé€ è¿™ä¸ªçš„è¿‡ç¨‹ä¸­å¾—åˆ°ä¸€äº›ä¹è¶£ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•è¿›è¡Œæ¨ç†å’Œéƒ¨ç½²è¿™ä¸ªæ¨¡å‹ã€‚</p><p id="c625" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated"><strong class="mc jd"> <em class="oc"> Colabç¬”è®°æœ¬:</em> </strong></p><ul class=""><li id="d82b" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1Wgcqn_hFDyUIi5-IDUiJz3feeidjzI60?usp=sharing" rel="noopener ugc nofollow" target="_blank">æ•°æ®å‡†å¤‡</a></li><li id="7fd8" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1tVYHhqYbcNnF12309p2vvF2bDMvmWVa6?usp=sharing" rel="noopener ugc nofollow" target="_blank">åŸ¹è®­</a></li></ul></div></div>    
</body>
</html>