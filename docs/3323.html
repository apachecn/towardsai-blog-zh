<html>
<head>
<title>Transformers for Multi-Regression â€” [PART1]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¤šå…ƒå›å½’çš„å˜å½¢é‡‘åˆšâ€”[ç¬¬ä¸€éƒ¨åˆ†]</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://pub.towardsai.net/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9?source=collection_archive---------1-----------------------#2022-11-18">https://pub.towardsai.net/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9?source=collection_archive---------1-----------------------#2022-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="292b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ’ä½œä¸ºç‰¹å¾æå–å™¨çš„å˜å‹å™¨ğŸ’</h1><p id="0335" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">æˆ‘åœ¨Kaggleå‚åŠ çš„<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning" rel="noopener ugc nofollow" target="_blank"> FB3ç«èµ›</a>æ¿€åŠ±æˆ‘å†™ä¸€ç¯‡å…³äºæˆ‘æµ‹è¯•è¿‡çš„æ–¹æ³•çš„å¸–å­ã€‚å†åŠ ä¸Šæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ˜ç¡®çš„å…³äºå¦‚ä½•ä½¿ç”¨å˜å‹å™¨è§£å†³å¤šå…ƒå›å½’é—®é¢˜çš„æ•™ç¨‹ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºåˆ†äº«ä¸€ä¸‹æˆ‘çš„å·¥ä½œä¼šå¾ˆæœ‰ç”¨ã€‚</p><p id="c1d8" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">æ‰€æœ‰è¿™äº›å·¥ä½œéƒ½åœ¨<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">æˆ‘çš„å¡æ ¼å°”ç¬”è®°æœ¬</a>ä¸­ç»§ç»­</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/526717509a83f7e1031b6c35ee3f2eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJFU2zLC42kdoV6lrLJ2uA.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated"><a class="ae lj" href="https://unsplash.com/photos/AVYo3X6XZYg" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/AVYo3X6XZYg</a></figcaption></figure><h1 id="86f1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ä»‹ç»</h1><p id="9c50" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">æˆ‘ä»¬å¹¶ä¸éƒ½æœ‰ç¦æ¥¼æ‹œğŸ§¡çš„æ‰åï¼ŒæŸæ ¼æ£®ğŸ¤çš„æ¸…æ™°ï¼Œæ™®é²æ–¯ç‰¹çš„å¤©æ‰ğŸ’™ï¼Œä¹Ÿæ²¡æœ‰<strong class="kn ir">èŒ¨å¨æ ¼</strong>çš„é£æ ¼å’Œæ‰‹è…•ğŸ’œï¼Œä¹Ÿä¸æ˜¯ä¼å°”æ³°<strong class="kn ir">çš„æœ¬äº‹</strong>ğŸ’šï¼Œä¹Ÿä¸æ˜¯å”æœ¬å<strong class="kn ir">çš„å…ˆçŸ¥å…ˆè§‰</strong>ğŸ’–â€¦</p><p id="5038" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">è¿™ä»½æ¸…å•è¿œéè¯¦å°½æ— é—ï¼Œæ„Ÿè°¢ä¸Šå¸ï¼Œæœ‰å¤©æ‰çš„ä½œå®¶å’Œå“²å­¦å®¶å…è®¸æˆ‘ä»¬æš‚æ—¶é€ƒç¦»è¿™ä¸ªå”¯ç‰©ä¸»ä¹‰çš„ä¸–ç•Œã€‚</p><p id="e9f7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä½†å°±æˆ‘ä»¬æ™®é€šäººè€Œè¨€ï¼Œä¸ç®¡æ˜¯ä¸æ˜¯æ–‡å­¦ï¼Œæˆ‘ä»¬è‡³å°‘å¯ä»¥å¸Œæœ›å°½åŠ›å°Šé‡è¯­è¨€çš„è§„åˆ™ï¼Œå†™å¾—â€œæ­£ç¡®â€ã€‚è€å¸ˆä»¬å¸®åŠ©æˆ‘ä»¬å­¦ä¹ è¯­è¨€çš„åŸºç¡€çŸ¥è¯†ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿåè¿‡æ¥ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå¸®åŠ©ä»–ä»¬ï¼Œåˆ©ç”¨æˆ‘ä»¬çš„çŸ¥è¯†ï¼ŒèŠ‚çœæ—¶é—´æ¥ä¿®æ”¹å­¦ç”Ÿçš„ä½œæ–‡ï¼Œå¸®åŠ©ä»–ä»¬åŒºåˆ†æ¯ä¸ªå­¦ç”Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æ›´å¥½åœ°è°ƒæ•´ä»–ä»¬çš„æ•™å­¦æ–¹æ³•ï¼Œä»¥é€‚åº”æ¯ä¸ªå­¦ç”Ÿçš„æ°´å¹³ã€‚</p><p id="867a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨è¿™åœºæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬è¢«è¦æ±‚ä½¿ç”¨8-12å¹´çº§è‹±è¯­å­¦ä¹ è€…å†™çš„é¢„å…ˆè¯„åˆ†çš„è®®è®ºæ–‡ï¼Œæ ¹æ®å…­ä¸ªåˆ†ææŒ‡æ ‡åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹:<strong class="kn ir">è¡”æ¥ã€å¥æ³•ã€è¯æ±‡ã€çŸ­è¯­ã€è¯­æ³•</strong>å’Œ<strong class="kn ir">æƒ¯ä¾‹</strong>ã€‚åˆ†æ•°èŒƒå›´ä»1.0åˆ°5.0ï¼Œå¢é‡ä¸º0.5ã€‚</p><p id="650e" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">æˆ‘ä»¬å°†æè¿°å¦‚ä½•ä½¿ç”¨æ‹¥æŠ±è„¸æ¨¡å‹æ¥è§£å†³è¿™ç§ç±»å‹çš„é—®é¢˜ï¼Œæˆ‘é€‰æ‹©äº†<code class="fe mf mg mh mi b">deberta-v3-base</code>æ¨¡å‹(è¿™é‡Œçš„<a class="ae lj" href="https://huggingface.co/microsoft/deberta-v3-base?text=The+goal+of+life+is+%5BMASK%5D." rel="noopener ugc nofollow" target="_blank">æ˜¯ç›¸åº”çš„æ¨¡å‹å¡)ï¼Œæˆ‘å°†å±•ç¤ºæˆ‘ä»¬å¦‚ä½•ä»¥ä¸¤ç§æœ‰æ•ˆçš„æ–¹å¼ä½¿ç”¨å®ƒ:</a></p><p id="7040" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">ç‰¹å¾æå–</strong>:æˆ‘ä»¬ä½¿ç”¨éšè—çŠ¶æ€ä½œä¸ºç‰¹å¾ï¼Œåªåœ¨å…¶ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè€Œä¸ä¿®æ”¹é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å¯¹äºæœ¬èŠ‚<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577" rel="noopener ugc nofollow" target="_blank">,@ cdeotte</a>æå‡ºäº†è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç»å¦™ç”¨æ³•ï¼Œå³ä½¿ç”¨å¤šä¸ªéå¾®è°ƒçš„å˜å‹å™¨åµŒå…¥ï¼Œç„¶åå°†å®ƒä»¬è¿æ¥èµ·æ¥å¹¶è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„åˆ†ç±»å™¨:æˆ‘å¼ºçƒˆé‚€è¯·æ‚¨æŸ¥çœ‹ç›¸å…³çš„<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577" rel="noopener ugc nofollow" target="_blank">è®¨è®º</a>å’Œ<a class="ae lj" href="https://www.kaggle.com/code/cdeotte/rapids-svr-cv-0-450-lb-0-44x" rel="noopener ugc nofollow" target="_blank">ç¬”è®°æœ¬</a></p><p id="5d91" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">å¾®è°ƒ</strong>:æˆ‘ä»¬å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¿™ä¹Ÿæ„å‘³ç€å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚è¿™ç§æ–¹æ³•å°†åœ¨<a class="ae lj" href="https://zghrib.medium.com/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c" rel="noopener">åé¢çš„å¸–å­</a>ä¸­è®¨è®º</p><p id="0f53" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸€æ­¥ä¸€æ­¥åœ°ä»‹ç»ç¬¬ä¸€ç§æ–¹æ³•:ğŸ‘åŸºäºç¼–ç å™¨çš„å˜å‹å™¨ç®€ä»‹</p><p id="1151" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">è¿™ä¸ªæƒ³æ³•æ˜¯ä½¿ç”¨åŸºäºBERTçš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç»è¿‡é¢„å…ˆè®­ç»ƒï¼Œå¯ä»¥é¢„æµ‹æ–‡æœ¬çš„å±è”½å…ƒç´ ï¼Œä»¥åŠä¸€ä¸ªè‡ªå®šä¹‰çš„åˆ†ç±»å™¨:å·¥ä½œæµç¨‹å¦‚ä¸‹:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/7e9228951e01da8e430e43524ab9a16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pACupZc_5f12t7LqFvPuQ.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">å¸¦æœ‰åŸºäºç¼–ç å™¨çš„<br/>å˜å‹å™¨çš„åˆ†ç±»å™¨/å›å½’å™¨å¤´</figcaption></figure><p id="6c63" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir"> 1ã€‚ç”Ÿæˆä»¤ç‰Œç¼–ç :</strong> <br/>é¦–å…ˆï¼Œä»¤ç‰ŒåŒ–å™¨ç”Ÿæˆä¸€ä¸ªåä¸º<strong class="kn ir">ä»¤ç‰Œç¼–ç </strong>çš„çƒ­ç¼–ç :æ¯ä¸ªå‘é‡çš„ç»´æ•°ç­‰äºä»¤ç‰ŒåŒ–å™¨è¯æ±‡è¡¨<code class="fe mf mg mh mi b">[batch_size, vocab_size]</code>ã€‚HuggingFaceçš„<code class="fe mf mg mh mi b">AutoTokenizer</code>ç±»å°†è‡ªåŠ¨åŠ è½½å¯¹åº”äºæ£€æŸ¥ç‚¹åç§°çš„è®°å·èµ‹äºˆå™¨(å¯¹äºè¿™ä¸ªç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe mf mg mh mi b">deberta-v3-base</code>)ï¼Œè®°å·èµ‹äºˆå™¨å°†ç”Ÿæˆä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬:</p><ul class=""><li id="df0a" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">input_ids</code>:å¥å­ä¸­æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•</li><li id="d823" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">token_type_ids</code>:å½“æœ‰å¤šä¸ªåºåˆ—æ—¶ï¼Œæ ‡è¯†ä¸€ä¸ªä»¤ç‰Œå±äºå“ªä¸ªåºåˆ—</li><li id="c7ef" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">attention_mask</code>:ä»çœŸå®è®°å·ä¸­è¯†åˆ«å¡«å……å…ƒç´ </li></ul><p id="656e" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç„¶åï¼Œæ¨¡å‹å°†é‡‡ç”¨ä»¤ç‰Œç¼–ç ï¼Œå¹¶å¦‚ä¸‹è¿›è¡Œ:<br/> <br/> <strong class="kn ir"> 2 .ç”ŸæˆåµŒå…¥:</strong> <br/>è¯¥æ¨¡å‹å°†ä»¤ç‰Œç¼–ç è½¬æ¢ä¸º<strong class="kn ir">å¯†é›†åµŒå…¥</strong>ã€‚ä¸ä»¤ç‰Œç¼–ç ä¸åŒï¼ŒåµŒå…¥æ˜¯<strong class="kn ir">å¯†é›†çš„</strong> =éé›¶å€¼ã€‚ä»¤ç‰Œç¼–ç æ˜¯ç”±ä»¤ç‰ŒåŒ–å™¨ç”Ÿæˆçš„ã€‚<br/> - &gt;æˆ‘ä»¬ç”¨æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å°ºå¯¸å¾—åˆ°ä¸€ä¸ªç»´åº¦ä¸º<code class="fe mf mg mh mi b">[batch_size, max_len]</code>çš„å¼ é‡</p><p id="4f3a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir"> 3ã€‚ç”Ÿæˆéšè—çŠ¶æ€:</strong> <br/>æ¨¡å‹é€šè¿‡ç¼–ç å™¨å †æ ˆæä¾›åµŒå…¥ï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œè¾“å…¥è¿”å›éšè—çŠ¶æ€ã€‚æˆ‘ä»¬è·å¾—ä¸€ä¸ªæœ€ç»ˆå¼ é‡<code class="fe mf mg mh mi b">[batch_size, max_len, hidden_states_dim]</code> <br/>æˆ‘ä»¬åŠ è½½ä¸€ä¸ª<code class="fe mf mg mh mi b">AutoModel</code>å¯¹è±¡æ¥åˆå§‹åŒ–ä¸€ä¸ªå…·æœ‰æ‰€æœ‰æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code></p><p id="100f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å‡†å¤‡æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä»¥ä¾¿ç”±è½¬æ¢å™¨è¿›è¡Œå¤„ç†:</p><h1 id="eab9" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸªµå‡†å¤‡æ•°æ®é›†</h1><p id="0099" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">æˆ‘ä»¬å°†é€šè¿‡å°æ‰¹é‡è¿›è¡Œï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe mf mg mh mi b">torch.utils.data.DataLoader</code>å’Œ<code class="fe mf mg mh mi b">torch.utils.data.Dataset</code>(æ‚¨å¯ä»¥åœ¨æ­¤å¤„æŸ¥çœ‹å‚è€ƒ<a class="ae lj" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank"/>)ã€‚<code class="fe mf mg mh mi b">Dataset</code>å…è®¸è¿”å›å¸¦æœ‰ç›¸åº”æ ‡ç­¾çš„æ•°æ®é›†æ ·æœ¬ã€‚<code class="fe mf mg mh mi b">DataLoader</code>åœ¨æ•°æ®é›†å‘¨å›´åŒ…è£…äº†ä¸€ä¸ªiterableä»¥æ–¹ä¾¿è®¿é—®æ ·æœ¬ï¼Œæä¾›äº†è®¸å¤šå®ç”¨ç¨‹åºï¼Œä¾‹å¦‚åœ¨æ¯ä¸ªæ—¶æœŸé‡æ–°æ’åˆ—æ•°æ®ä»¥å‡å°‘æ¨¡å‹è¿‡åº¦æ‹Ÿåˆï¼Œæˆ–è€…å…è®¸ä½¿ç”¨å¤šé‡å¤„ç†æ¥åŠ é€Ÿæ•°æ®æ£€ç´¢ã€‚</p><p id="c6a3" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä¸ºäº†å¼€å‘æˆ‘ä»¬çš„å®šåˆ¶<code class="fe mf mg mh mi b">Dataset</code>ï¼Œæˆ‘ä»¬å¿…é¡»è¦†ç›–<code class="fe mf mg mh mi b">__init__</code>ã€<code class="fe mf mg mh mi b">__len__</code>å’Œ<code class="fe mf mg mh mi b">__getitem__</code>å‡½æ•°ã€‚<br/>æœ€é‡è¦çš„å‡½æ•°æ˜¯<code class="fe mf mg mh mi b">__getitem__</code>:å®ƒä»æ•°æ®é›†ä¸­ç»™å®šç´¢å¼•idxå¤„è¿”å›ä¸€ä¸ªæ ·æœ¬ã€‚å‡½æ•°çš„è¾“å‡ºæ ¼å¼å¿…é¡»ç¬¦åˆ<strong class="kn ir">æ¨¡å‹çš„é¢„æœŸæ ¼å¼</strong>:</p><ul class=""><li id="869c" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">input_ids</code>:æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œidåˆ—è¡¨ã€‚</li><li id="cba5" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">attention_mask</code>:æŒ‡å®šæ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›ä»¤ç‰Œçš„ç´¢å¼•åˆ—è¡¨</li><li id="8d57" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">labels</code>:åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¤„ç†ä¸€ä¸ª<strong class="kn ir">å¤šç±»å›å½’</strong>é—®é¢˜ï¼Œæ ‡ç­¾æ˜¯<em class="my">å…­ä¸ªåˆ†æåˆ†æ•°</em>çš„å‘é‡ã€‚</li></ul><p id="992a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">å¯¹äºè®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬å°†å‚è€ƒä¸å¯å¦è®¤çš„ç»å…¸äº¤å‰éªŒè¯æ–¹æ¡ˆ</p><p id="bfea" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ•<strong class="kn ir">å¤šæ ‡ç­¾æ•°æ®åˆ†å±‚:</strong></p><p id="8034" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨æˆ‘ä»¬çš„æ•°æ®ç§‘å­¦å®¶ç¤¾åŒºä¸­ï¼Œæœ‰è¯æ®è¡¨æ˜å¦‚ä½•åˆ†å‰²äº¤å‰éªŒè¯æŠ˜å å¯¹æ¨¡å‹æ€§èƒ½æœ‰ç›´æ¥å½±å“ã€‚<br/>é€šå¸¸ï¼Œå¯¹äºå•ç±»é—®é¢˜ï¼Œè¤¶çš±ä¸å•ä¸ªç›®æ ‡ä¸€èµ·åˆ†å±‚(ç¦»æ•£ç›®æ ‡çš„<em class="my">ç±»åˆ†å¸ƒæˆ–è¿ç»­ç›®æ ‡çš„é¢å…ƒåˆ†å¸ƒ</em>)ã€‚</p><blockquote class="mz na nb"><p id="ee58" class="kl km my kn b ko lk kq kr ks ll ku kv nc lm ky kz nd ln lc ld ne lo lg lh li ij bi translated">ä½†æ˜¯ï¼Œåœ¨å¤šç±»é—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ğŸ¤”ï¼Ÿ</p></blockquote><p id="0014" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">å—¯ï¼Œå·²ç»å¼€å±•äº†è®¸å¤šå·¥ä½œæ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚:</p><ul class=""><li id="ff85" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated">2011 : <a class="ae lj" href="http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf" rel="noopener ugc nofollow" target="_blank"> Sechidis â€”å…³äºå¤šæ ‡ç­¾æ•°æ®çš„åˆ†å±‚</a></li><li id="b51e" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">2017:<a class="ae lj" href="http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html" rel="noopener ugc nofollow" target="_blank">szymÅ„skiâ€”â€”ç¬¬ä¸€å±Šä¸å¹³è¡¡é¢†åŸŸå­¦ä¹ å›½é™…ç ”è®¨ä¼šè®ºæ–‡é›†</a></li></ul><p id="aa39" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><a class="ae lj" href="https://www.slideshare.net/tsoumakas/on-the-stratification-of-multilabel-data" rel="noopener ugc nofollow" target="_blank">è¿™é‡Œ</a>æ˜¯ä¸€ä¸ªæ¸©å’Œçš„æ¼”ç¤ºï¼Œè§£é‡Šç®—æ³•ã€‚<br/>æˆ‘ä»¬å°†ä½¿ç”¨é‡‡ç”¨<a class="ae lj" href="https://github.com/trent-b/iterative-stratification" rel="noopener ugc nofollow" target="_blank">è¿­ä»£åˆ†å±‚</a>å®æ–½çš„è¿­ä»£æ–¹æ³•ç®—æ³•:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="06d2" class="nj jo iq mi b be nk nl l nm nn">import pandas as pd<br/>from iterstrat.ml_stratifiers import MultilabelStratifiedKFoldtrain = pd.read_csv(PATH_TO_TRAIN)<br/>print("TRAIN SHAPE", train.shape)<br/>test = pd.read_csv(PATH_TO_TEST)<br/>print("TEST SHAPE", test.shape)<br/>label_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']<br/>cv = MultilabelStratifiedKFold(<br/>          n_splits=N_FOLDS, <br/>          shuffle=True, <br/>          random_state=SEED<br/>          )<br/>train = train.reset_index(drop=True)<br/>for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[label_cols])):<br/>    train.loc[val_idx , "fold"] = int(fold)<br/>    <br/>train["fold"] = train["fold"].astype(int)</span></pre><p id="76df" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç°åœ¨è®©æˆ‘ä»¬åƒå‰é¢æè¿°çš„é‚£æ ·å®ç°æ•°æ®é›†è¿­ä»£å™¨:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="23d9" class="nj jo iq mi b be nk nl l nm nn"># lets define the batch genetator<br/>class CustomIterator(torch.utils.data.Dataset):<br/>    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):<br/>        self.df = df<br/>        self.tokenizer = tokenizer<br/>        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length<br/>        self.labels = labels<br/>        self.is_train = is_train<br/>        <br/>    def __getitem__(self,idx):<br/>        tokens = self.tokenizer(<br/>                    self.df.loc[idx, 'full_text'],#.to_list(),<br/>                    add_special_tokens=True,<br/>                    padding='max_length',<br/>                    max_length=self.max_seq_length,<br/>                    truncation=True,<br/>                    return_tensors='pt',<br/>                    return_attention_mask=True<br/>                )     <br/>        res = {<br/>            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),<br/>            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()<br/>        }<br/>        <br/>        if self.is_train:<br/>            res["labels"] = torch.tensor(<br/>                self.df.loc[idx, self.labels].to_list(), <br/>            ).to(CONFIG.get('device')) <br/>            <br/>        return res<br/>    <br/>    def __len__(self):<br/>        return len(self.df)</span></pre><p id="9609" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">è¿™ä¸ªè‡ªå®šä¹‰çš„<code class="fe mf mg mh mi b">Dataset</code>å°†åœ¨ä»¥åè¢«ç”¨äºå¾®è°ƒè½¬æ¢å™¨æˆ–è€…ä»…ä»…ä½œä¸ºä¸€ä¸ªç‰¹å¾æå–å™¨ã€‚</p><p id="c514" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:æˆ‘æ·»åŠ äº†<code class="fe mf mg mh mi b">is_train</code>å‚æ•°æ¥å†³å®šæ˜¯å¦è¿”å›â€œæ ‡ç­¾â€å­—æ®µ(åªæœ‰è®­ç»ƒæ•°æ®é›†åŒ…å«æ ‡ç­¾å­—æ®µ)</p><h1 id="91a5" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">â›Transformersæ˜¯Extractorsâ›çš„ç‰¹è‰²</h1><p id="ed14" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç¼–ç å™¨æƒé‡è¢«<strong class="kn ir">å†»ç»“</strong>ï¼Œéšè—çŠ¶æ€è¢«å¤šå…ƒå›å½’å™¨ç”¨ä½œ<strong class="kn ir">ç‹¬ç«‹ç‰¹å¾</strong>ã€‚<br/>ç”±äºéšè—çŠ¶æ€åªè®¡ç®—ä¸€æ¬¡ï¼Œå¦‚æœæˆ‘ä»¬<strong class="kn ir">ä¸å¤„ç†GPU</strong>ï¼Œè¿™ç§æ–¹æ³•æ˜¯æœ€å¥½çš„é€‰æ‹©:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/2c107b051a73f5501363a3daef0e9022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvFWjL7q6_eet1-ReGVgiw.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">ç¼–ç å™¨å˜å‹å™¨ä½œä¸ºç‰¹å¾æå–å™¨:åªæœ‰å¤´éƒ¨æ˜¯å¯è®­ç»ƒçš„ï¼Œæ‰€æœ‰çš„å˜å‹å™¨å±‚è¢«å†»ç»“</figcaption></figure><p id="5ad4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç•™ç»™æˆ‘ä»¬çš„å”¯ä¸€ä¸€æŠŠè‡ªç”±ä¹‹æ–§æ˜¯å¦‚ä½•å°†éšè—çŠ¶æ€å¼ é‡<code class="fe mf mg mh mi b">[batch_size, max_len, hidden_states_dim]</code>ç®€åŒ–ä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡è¡¨ç¤º:æˆ‘å¼ºçƒˆå»ºè®®ä½ å‚è€ƒæ— ä»·çš„<a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> @rhtsingh notebook </a>ï¼Œå®ƒâ€œè¯¦å°½åœ°â€åˆ—ä¸¾äº†éšè—çŠ¶æ€ç¼–ç çš„ä¸åŒæ–¹å¼<strong class="kn ir">ã€</strong>ã€‚æˆ‘æµ‹è¯•äº†ä»¥ä¸‹æŠ€å·§:</p><p id="575d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir"> CLSåµŒå…¥:</strong> BERTå¼•å…¥ä¸€ä¸ª<strong class="kn ir">ã€CLSã€‘</strong>tokenæ ‡ç­¾ï¼Œç«™åœ¨æ¯å¥è¯çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œæ•æ‰æ•´ä¸ªå¥å­çš„ä¸Šä¸‹æ–‡ã€‚clsåµŒå…¥ç®€å•åœ°åŒ…æ‹¬é€‰æ‹©æ¯ä¸ªéšè—çŠ¶æ€å‘é‡çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œä»è€Œå¾—åˆ°<code class="fe mf mg mh mi b">[batch_size, 1, hidden_states_dim]</code>å‘é‡</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="dfb2" class="nj jo iq mi b be nk nl l nm nn">import torch<br/>import torch.nn as nn<br/>import transformers<br/>from transformers import (<br/>    AutoModel, AutoConfig, <br/>    AutoTokenizer, logging,<br/>    AdamW, get_linear_schedule_with_warmup,<br/>    DataCollatorWithPadding,<br/>    Trainer, TrainingArguments<br/>)<br/>from transformers.modeling_outputs import SequenceClassifierOutput# https://github.com/UKPLab/sentence-transformers/blob/0422a5e07a5a998948721dea435235b342a9f610/sentence_transformers/models/Pooling.py<br/># https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently<br/>def cls_embedding(outputs):<br/>    """Since Transformers are contextual model, <br/>    the idea is [CLS] token would have captured the entire context <br/>    and would be sufficient for simple downstream tasks such as classification<br/>    Select the first token for each hidden state<br/>    <br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    """<br/>    return outputs.last_hidden_state[:, 0, :].to(CONFIG.get('device'))</span></pre><p id="e4b7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">æ„å‘³ç€æ±‡é›†</strong>:æˆ‘ä»¬å°†è€ƒè™‘æ¯ä¸ªéšè—çŠ¶æ€ç»´åº¦çš„<code class="fe mf mg mh mi b">max_len </code>ç»´åº¦åµŒå…¥çš„å¹³å‡å€¼ï¼Œè€Œä¸æ˜¯é€‰æ‹©ç¬¬ä¸€ä¸ªå…ƒç´ :æˆ‘ä»¬è·å¾—ä¸€ä¸ª<code class="fe mf mg mh mi b">[batch_size, 1, hidden_states_dim]</code>çš„å¼ é‡ï¼Œæˆ–è€…ä»…ä»…æ˜¯æœªæ’åºçš„å½¢å¼:<code class="fe mf mg mh mi b">[batch_size, hidden_states_dim]</code></p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="2caf" class="nj jo iq mi b be nk nl l nm nn">def mean_pooling(inputs, outputs):<br/>    """<br/>    For each hidden_state, average along with max_len embeddings, <br/>    but we will condider only the highlighted tokens by the attention mask<br/>    <br/>    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field<br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    """<br/>    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()<br/>    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)<br/>    sum_mask = input_mask_expanded.sum(1)<br/>    sum_mask = torch.clamp(sum_mask, min=1e-9)<br/>    mean_embeddings = sum_embeddings / sum_mask<br/>    return mean_embeddings</span></pre><p id="1118" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">æœ€å¤§æ±‡é›†</strong>:ä¸ºäº†å¾—åˆ°æœ€å¤§æ±‡é›†ï¼Œæˆ‘ä»¬å°†åœ¨æ¯ä¸ªéšè—çŠ¶æ€ç»´åº¦ä¸Šçš„<code class="fe mf mg mh mi b">max_len</code>åµŒå…¥ä¸­å–æœ€å¤§å€¼ï¼Œç»“æœæ˜¯ä¸€ä¸ª<code class="fe mf mg mh mi b">[batch_size, hidden_states_dim]</code>ç»´åº¦çš„å¼ é‡</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="25c1" class="nj jo iq mi b be nk nl l nm nn">def max_pooling(inputs, outputs):<br/>    """<br/>    For each hidden_state, get the max element along with max_len embeddings,<br/>    considering only the non masked element difined by the attention mask computed by the tokenizer<br/>    <br/>    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field<br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    <br/>    """<br/>    last_hidden_state = outputs.last_hidden_state<br/>    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()<br/>    last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value<br/>    max_embeddings = torch.max(last_hidden_state, 1).values<br/>    return max_embeddings</span></pre><p id="9ffc" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">å¹³å‡æœ€å¤§æ± åŒ–:</strong>æˆ‘ä»¬åº”ç”¨å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ï¼Œç„¶åè¿æ¥ä¸¤è€…ä»¥è·å¾—<code class="fe mf mg mh mi b">[batch_size, 2*hidden_states_dim]</code>ç»´åº¦å¼ é‡</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="269b" class="nj jo iq mi b be nk nl l nm nn">def mean_max_pooling(inputs, outputs):<br/>    """<br/>    Apply mean and max-pooling embeddings, then we concatenate the two onto a single final representation<br/>    <br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, 2*hidden_states_dim]<br/>    """<br/>    mean_pooling_embeddings = mean_pooling(inputs, outputs)<br/>    max_pooling_embeddings = max_pooling(inputs, outputs)<br/>    mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)<br/>    return mean_max_embeddings</span></pre><p id="8d0d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä¸‹é¢æ˜¯è·å–æ‰€æœ‰åµŒå…¥çš„ä»£ç ç¤ºä¾‹:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="8b06" class="nj jo iq mi b be nk nl l nm nn"><br/><br/>def get_embedding(dataloader, model, n):<br/>    """<br/>    Run the model to predict hidden states then apply all the transformations implemented above<br/>    <br/>    @param dataloader: a torch.utils.data.DataLoader the iterator along with the custom torch.utils.data.Dataset<br/>    @param model : the huggingface AutoModel that generates the hidden states<br/>    """<br/>    embeddings = {}<br/>    model = model.to(CONFIG.get('device'))<br/>    for batch in tqdm_notebook(dataloader):<br/>        with torch.no_grad():<br/>            # please note here that the labels fileds is not necessary <br/>            # since we are not going to fine tune the model but just get the vectors output<br/>            outputs = model(<br/>                input_ids=batch['input_ids'].squeeze(),<br/>                attention_mask=batch['attention_mask'].squeeze()<br/>            )<br/>        for embed_name, embed_func in zip(['cls_embeddings', "mean_pooling", "max_pooling", "mean_max_pooling"], <br/>                                          [cls_embedding, mean_pooling, max_pooling, mean_max_pooling]):<br/>            if embed_name == 'cls_embeddings':<br/>                embed = embed_func(outputs)<br/>            else:<br/>                embed = embed_func(batch, outputs)<br/>            embeddings[embed_name] = torch.cat(<br/>                (<br/>                    embeddings.get(embed_name, torch.empty(embed.size()).to(CONFIG.get('device'))), <br/>                    embed<br/>                ),<br/>                0<br/>            )<br/>    threshold = min(n,CONFIG.get('train_batch_size'))<br/>    for key in embeddings:<br/>        embeddings[key] = embeddings[key][threshold:,:]<br/>    return embeddings</span></pre><p id="6a4b" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ç”ŸæˆåµŒå…¥:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="969d" class="nj jo iq mi b be nk nl l nm nn">model = AutoModel.from_pretrained(CONFIG["model_name"], config=config)<br/># TRAIN #<br/>df_iter = CustomIterator(train, tokenizer)<br/>train_dataloader = torch.utils.data.DataLoader(<br/>    df_iter, <br/>    batch_size=CONFIG["train_batch_size"],<br/>    shuffle=False<br/>)<br/>embeddings = get_embedding(train_dataloader, model, n=len(train))<br/># TEST #<br/>df_iter = CustomIterator(test, tokenizer, is_train=False)<br/>test_dataloader = torch.utils.data.DataLoader(<br/>    df_iter, <br/>    batch_size=CONFIG["train_batch_size"],<br/>    shuffle=False<br/>)<br/>test_embeddings = get_embedding(test_dataloader, model, n=len(test))</span></pre><h1 id="edc3" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ”éšè—çŠ¶æ€å¯è§†åŒ–:</h1><p id="959e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">åœ¨è®­ç»ƒåˆ†ç±»å™¨ä»¥è·å¾—2Då¯è§†åŒ–ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹åµŒå…¥ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åªå¯¹<code class="fe mf mg mh mi b">cls_embeddings</code>åº”ç”¨å®ƒï¼Œå¯¹äºå…¶ä»–ç±»å‹çš„åµŒå…¥ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚</p><p id="49a1" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">æˆ‘ä»¬å¿…é¡»å°†éšè—çŠ¶æ€å‡å°‘åˆ°2Dï¼Œè®¸å¤šæœ‰æ•ˆçš„æ¨¡å‹å¯ä»¥ç”¨æ¥å‡å°‘åµŒå…¥çš„ç»´æ•°:<a class="ae lj" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> UMAP </a>ï¼Œ<a class="ae lj" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">ä¸»æˆåˆ†åˆ†æ</a>ï¼Œ<a class="ae lj" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> T-SNE </a></p><p id="4f80" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">æˆ‘ä»¬å°†ä½¿ç”¨PCAç®—æ³•:<br/> 1ã€‚<strong class="kn ir">æ•°æ®æ ‡å‡†åŒ–</strong>:ä½¿ç”¨scikit learn <br/> 2çš„<code class="fe mf mg mh mi b">StandardScaler</code>æ ‡å‡†åŒ–åµŒå…¥ã€‚<strong class="kn ir"> 2Dé™ç»´</strong>:æ‹ŸåˆåµŒå…¥çš„PCAæ¨¡å‹ï¼Œæå–å‰ä¸¤ä¸ªåˆ†é‡<br/> 3ã€‚<strong class="kn ir"> Hexbinå¯è§†åŒ–</strong>:å¯¹äºæ¯ä¸ªç›®æ ‡ç±»ï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–æ¯ä¸ªåˆ†æ•°çš„bin(ä»1åˆ°5ï¼Œæ­¥é•¿= 0.5)</p><p id="5736" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">æˆ‘ä»¬æ¥çœ‹çœ‹è¯æ±‡è¯¾å‰§æƒ…:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi no"><img src="../Images/a1aa560d2f3a93459232c0de2a595dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPabZvtKdMa6l42L1lz3kA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">è¯æ±‡å¾—åˆ†åˆ†å¸ƒçš„Hexbinå›¾ï¼Œä»¥åŠcls _ embeddingsçš„å‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†</figcaption></figure><p id="c730" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:è¿™ä¸ªå¯è§†åŒ–éƒ¨åˆ†æŠ€æœ¯å¾ˆå—t <a class="ae lj" href="https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb" rel="noopener ugc nofollow" target="_blank">çš„HuggingFace NLP GitHubä¾‹å­</a>çš„å¯å‘</p><p id="8729" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä»è¿™ä¸ªå›¾ä¸­å¯ä»¥çœ‹å‡ºä¸€äº›æ¨¡å¼:å¯¹å¤§å¤šæ•°äººæ¥è¯´ï¼Œæç«¯åˆ†æ•°æ˜¯åˆ†å¼€çš„ï¼Œ2.5åˆ†ä¼¼ä¹åˆ†æ•£åœ¨æ‰€æœ‰åœ°æ–¹ï¼Œè€Œå¯¹å…¶ä»–äººæ¥è¯´ï¼Œæœ‰æ˜æ˜¾çš„é‡å ã€‚</p><p id="7082" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:ä¸è¦å¿˜è®°ï¼Œè¿™äº›åµŒå…¥æ˜¯ç”±ä¸€ä¸ªæ¨¡å‹<strong class="kn ir">ç”Ÿæˆçš„ï¼Œè¯¥æ¨¡å‹é¢„å…ˆè®­ç»ƒç”¨äºé¢„æµ‹å¥å­ä¸­çš„å±è”½è¯</strong>ï¼Œè€Œä¸æ˜¯å¯¹åˆ†æ•°è¿›è¡Œåˆ†ç±»ã€‚</p><h1 id="73c4" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">âš™å¤šå…ƒå›å½’å¤´åŸ¹è®­</h1><p id="dd8a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„åµŒå…¥ä¸Šè®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’æ¨¡å‹:æˆ‘é€‰æ‹©äº†ä¸€ä¸ªåŸºäºæ¢¯åº¦æ¨è¿›çš„æ¨¡å‹:<strong class="kn ir"> Xgboost </strong></p><p id="9553" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­:å¤šç±»å›å½’ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨scikit-learnçš„<code class="fe mf mg mh mi b">MultiOutputRegressor</code>ä¼°è®¡å™¨ã€‚æˆ‘ä¼šè®©ä½ ä»<a class="ae lj" href="https://www.kaggle.com/code/swimmy/stacking-xgboost-lgbm-ridge-catboost" rel="noopener ugc nofollow" target="_blank"> @SWIMMYä¼˜ç§€ç¬”è®°æœ¬</a>ä¸­æŸ¥çœ‹ä¸åŒçš„åŸºäºæ ‘çš„æ¨¡å‹+ç”¨ä¸€ä¸ªå…ƒæ¨¡å‹å †å çš„åŸå§‹å’Œè¿›ä¸€æ­¥çš„å®ç°ã€‚</p><p id="7af0" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä¸ºäº†æŸ¥çœ‹å“ªä¸ªæ± å…·æœ‰æœ€ä½³çš„åˆ†ç¦»è¡¨ç¤ºï¼Œæˆ‘ä»¬å°†å¯¹æ¯ä¸ªæ± åµŒå…¥ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°ã€‚å…¨å±€åº¦é‡åŒ…æ‹¬å¹³å‡6ä¸ªç›®æ ‡åˆ—çš„RMSE:è¿™ä¸ªåº¦é‡ç§°ä¸º<strong class="kn ir"> MCRMSE </strong>(å¹³å‡åˆ—å‡æ–¹æ ¹è¯¯å·®)</p><p id="1415" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">è®©æˆ‘ä»¬å®šä¹‰è¯„ä¼°æŒ‡æ ‡:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="2dda" class="nj jo iq mi b be nk nl l nm nn">def comp_score(y_true,y_pred):<br/>    rmse_scores = []<br/>    for i in range(len(CONFIG['label_cols'])):<br/>        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))<br/>    return np.mean(rmse_scores)</span></pre><p id="cc64" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç°åœ¨å¼€å§‹ç®€å†åŸ¹è®­:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="0fba" class="nj jo iq mi b be nk nl l nm nn">import joblib<br/>y_true = train[CONFIG['label_cols']].values<br/>cv_rmse = pd.DataFrame(0, index=range(N_FOLDS), columns=embeddings.keys())<br/><br/>oof_pred = {<br/>        emb_type : np.zeros((len(train), len(label_cols)))<br/>        for emb_type in embeddings<br/>    }<br/><br/>for emb_type, emb in embeddings.items(): <br/>    print(f"CV for {emb_type}")<br/>    emb = normalize(<br/>        emb, <br/>        p=1.0, <br/>        dim = 1<br/>    ).cpu()<br/><br/>    for fold, val_fold in train.groupby('fold'):<br/>        print(f"*** FOLD == {fold} **")<br/>        x_train, x_val = np.delete(emb, val_fold.index, axis=0), emb[val_fold.index]<br/>        y_train, y_val = np.delete(y_true, val_fold.index, axis=0), y_true[val_fold.index]<br/>        xgb_estimator = xgb.XGBRegressor(<br/>                n_estimators=500, random_state=0, <br/>                objective='reg:squarederror')<br/>        # create MultiOutputClassifier instance with XGBoost model inside<br/>        xgb_model = MultiOutputRegressor(xgb_estimator, n_jobs=2)<br/>        # model4 = XGBClassifier(early_stopping_rounds=10)<br/>        xgb_model.fit(x_train, y_train)<br/>        oof_pred[emb_type][val_fold.index] = xgb_model.predict(x_val)<br/>        for i, col in enumerate(CONFIG['label_cols']):<br/>            rmse_fold = np.sqrt(mean_squared_error(y_val[:,i], oof_pred[emb_type][val_fold.index,i]))<br/>            print(f'{col} RMSE = {rmse_fold:.3f}')<br/>            <br/>        cv_rmse.loc[fold, emb_type] = comp_score(y_val, oof_pred[emb_type][val_fold.index])<br/>        print(f'COMP METRIC = {cv_rmse.loc[fold, emb_type]:.3f}')        <br/>        joblib.dump(xgb_model, f'xgb_{emb_type}_{fold}.pkl')</span></pre><p id="1d8d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨CVè®­ç»ƒæœŸé—´ï¼Œå¯¹äºæ¯ç§åµŒå…¥ç±»å‹ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¿æŒä¸€ä¸ªæŠ˜å åˆ†å¼€ï¼Œå¹¶ä¸”åœ¨å‰©ä½™çš„æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬é¢„æµ‹çœ‹ä¸è§çš„è¤¶çš±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è·å¾—äº†éæŠ˜å é¢„æµ‹(OOFé¢„æµ‹)ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªé¢„æµ‹éƒ½æ˜¯åœ¨çœ‹ä¸è§çš„æ•°æ®ä¸Šå®Œæˆçš„ã€‚</p><p id="ee2f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°æ¯ä¸ªç±»ä¸Šçš„RMSEå’Œæ¯ä¸ªåµŒå…¥ç±»å‹çš„OOFé¢„æµ‹ä¸Šçš„å…¨å±€MCRMSE:æˆ‘ä»¬è·å¾—ä»¥ä¸‹æ€§èƒ½:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5763a3b61da5c26e39161d4cdfe2984c.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*FsapaGs_qN3p7WhVpdW26g.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">æŒ‰åµŒå…¥ç±»å‹åˆ’åˆ†çš„OOF XGBoostæ€§èƒ½</figcaption></figure><ul class=""><li id="a9ef" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated">çœ‹èµ·æ¥<strong class="kn ir">å¹³å‡æœ€å¤§æ± </strong>æä¾›äº†æœ€å¥½çš„æ€§èƒ½ï¼Œä½†æ˜¯<strong class="kn ir">å¹³å‡æ± </strong>éå¸¸æ¥è¿‘ã€‚ç”±äºå¹³å‡æœ€å¤§æ± çš„å®¹é‡æ˜¯å¹³å‡æ± çš„ä¸¤å€ï¼Œæˆ‘ä»¬å°†é€‰æ‹©å¹³å‡ç¼–ç æ¥å¾®è°ƒè½¬æ¢å™¨</li><li id="340a" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">æˆ‘ä»¬è¯­æ–™åº“ä¸­æ•ˆç‡æœ€ä½çš„æ± æ–¹æ³•æ˜¯<strong class="kn ir">æœ€å¤§æ± </strong></li><li id="77a7" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">å¯¹äºåˆ†æåº¦é‡çš„æ‰€æœ‰è¡¨ç¤ºï¼Œè¯æ±‡æ˜¯æœ€å®¹æ˜“ä¼°è®¡çš„ç›®æ ‡(æœ€ä½çš„RMSE)ï¼Œè€Œå†…èšæ˜¯æœ€éš¾ä¼°è®¡çš„ç›®æ ‡(æœ€é«˜çš„RMSE)</li></ul><p id="1f5f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">ä¸ºäº†æ¨æ–­æ–°çš„é¢„æµ‹ï¼Œæˆ‘çš„ä¸€ä¸ªæœ‹å‹<a class="ae lj" href="https://www.kaggle.com/mathurinach" rel="noopener ugc nofollow" target="_blank"> Mathurin AchÃ© </a>ï¼Œä»–ä¹Ÿæ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„æ•°æ®ç§‘å­¦å®¶å’ŒKaggleå¤§å¸ˆï¼Œæ•™äº†æˆ‘ä¸¤ä¸ªæ–¹æ³•:</p><ol class=""><li id="996a" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li nq mq mr ms bi translated">ä½¿ç”¨CVæ¨¡å‹æ¥é¢„æµ‹æ–°çš„æ•°æ®:å¯¹é¢„æµ‹è¿›è¡Œå¹³å‡å¯ä»¥<strong class="kn ir">å‡å°‘æ–¹å·®</strong>ä½†æ˜¯ä¼šå¢åŠ æ¯ä¸ªæ ·æœ¬æ—¶é—´çš„é¢„æµ‹ï¼Œå¦‚æœæˆ‘ä»¬å¿…é¡»éƒ¨ç½²è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»ä¿å­˜æ‰€æœ‰çš„CVæ¨¡å‹</li><li id="dc00" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li nq mq mr ms bi translated">å¯¹æ•´ä¸ªè®­ç»ƒæ•°æ®è®­ç»ƒä¸€æ¬¡ä¿ç•™çš„æ¨¡å‹ï¼Œç„¶åé¢„æµ‹æ–°çš„é¢„æµ‹ï¼Œè¿™å¯èƒ½ä¼šç¨å¾®å¢åŠ æ³›åŒ–è¯¯å·®aï¼Œä½†åœ¨éƒ¨ç½²çš„æƒ…å†µä¸‹(é¢„æµ‹æ—¶é—´å’Œè¦ç›‘è§†çš„å•ä¸ªæ¨¡å‹)å»ºè®®è¿™æ ·åš</li></ol><p id="945b" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åªè¦æˆ‘ä»¬å¤„äºKaggleç«äº‰ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å°±é€‰æ‹©ç¬¬ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨å‡å€¼-æœ€å¤§å€¼æ± :</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="c7b0" class="nj jo iq mi b be nk nl l nm nn">import glob<br/># init output with zeros<br/>xgb_infer = np.zeros((len(test), len(label_cols)))<br/>for model_path in glob.glob("./xgb_mean_max_pooling_*.pkl"):<br/>    print(f"load {model_path} model")<br/>    xgb_model = joblib.load(model_path)<br/>    emb = normalize(<br/>        test_embeddings["mean_max_pooling"], <br/>        p=1.0, <br/>        dim = 1<br/>    ).cpu()<br/>    # add fold model prediction<br/>    xgb_infer = np.add(xgb_infer, xgb_model.predict(emb))<br/># devide by the number of folds<br/>xgb_infer = xgb_infer*(1/N_FOLDS)</span></pre><h1 id="4ac2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ™å­¦åˆ†:</h1><p id="a4ba" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">æˆ‘åœ¨è¿™ä¸€éƒ¨åˆ†çš„å·¥ä½œå—åˆ°äº†è¿™äº›ä¼˜ç§€èµ„æºçš„å¯å‘ï¼Œè¯·ä¸è¦çŠ¹è±«å»æŸ¥é˜…å®ƒä»¬:</p><ul class=""><li id="cb5f" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/swimmy/stacking-xgboost-lgbm-ridge-catboost" rel="noopener ugc nofollow" target="_blank"/></li><li id="5952" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @rhtsingh </strong>ç¬”è®°æœ¬</a>:æ¢ç´¢å˜å‹å™¨è¡¨è±¡çš„ä¸åŒæ–¹å¼</li><li id="1105" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @Y.NAKAMA </strong>ç¬”è®°æœ¬</a>:æŸå¤±å‡½æ•°ä¸å¤šæ ‡ç­¾åˆ†å±‚äº¤å‰éªŒè¯</li><li id="d5ca" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> Huggingface NLP GitHub </a>:é™ç»´åŠå˜å‹å™¨åµŒå…¥å¯è§†åŒ–</li></ul><h1 id="1d5d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ç»“è®º:</h1><p id="2927" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">æ„Ÿè°¢æ‚¨é˜…è¯»æˆ‘çš„å¸–å­ğŸ¤—å¸Œæœ›æœ‰ç”¨ï¼æé†’ä¸€ä¸‹ï¼Œæˆ‘æ‰€æœ‰çš„ä½œå“éƒ½å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">ğŸã€‚</a></p><p id="4c56" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„è½¬æ¢å™¨æ¥æå–ä¸Šä¸‹æ–‡æ•è·åµŒå…¥ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’å™¨(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯Xgboost)æ¥å¯¹å­¦ç”Ÿè®ºæ–‡çš„åˆ†ææŒ‡æ ‡è¿›è¡Œå»ºæ¨¡ã€‚</p><p id="7c31" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†è§£å†³åŒæ ·çš„é—®é¢˜ï¼Œä½†è¿™æ¬¡æ˜¯é€šè¿‡<strong class="kn ir">å¾®è°ƒ</strong>è½¬æ¢å™¨ï¼Œå¹¶æ›´æ–°å…¶æ‰€æœ‰çš„ç¼–ç å™¨å †æ ˆã€‚æ­¤å¤–ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨<a class="ae lj" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">æƒé‡&amp;åå·®</strong> </a>å¤§å¹³å°æ¥è·Ÿè¸ªæ¨¡å‹æ€§èƒ½å¹¶åˆ›å»ºæ¨¡å‹å·¥ä»¶ã€‚å¤šå…ƒå›å½’ä»»åŠ¡çš„è½¬æ¢å™¨</p></div></div>    
</body>
</html>