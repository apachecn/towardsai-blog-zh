<html>
<head>
<title>Transformers for Multi-Regression â€” [PART2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¤šå…ƒå›å½’çš„å˜å½¢é‡‘åˆšâ€”[ç¬¬äºŒéƒ¨åˆ†]</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://pub.towardsai.net/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c?source=collection_archive---------4-----------------------#2022-11-18">https://pub.towardsai.net/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c?source=collection_archive---------4-----------------------#2022-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="ad93" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ¤–å¾®è°ƒğŸ¤–</h1><p id="1d8d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">åœ¨<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning" rel="noopener ugc nofollow" target="_blank"> FB3ç«èµ›</a>çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨8-12å¹´çº§è‹±è¯­å­¦ä¹ è€…å†™çš„é¢„å…ˆè¯„åˆ†çš„è®®è®ºæ–‡å»ºç«‹å…­ä¸ªåˆ†ææŒ‡æ ‡æ¨¡å‹ã€‚æˆ‘ä»¬å¿…é¡»æ¨¡æ‹Ÿçš„æŠ€èƒ½å¦‚ä¸‹:<strong class="kn ir">è¡”æ¥ã€å¥æ³•ã€è¯æ±‡ã€æªè¾ã€è¯­æ³•</strong>å’Œ<strong class="kn ir">çº¦å®š</strong>ã€‚åˆ†æ•°èŒƒå›´ä»1.0åˆ°5.0ï¼Œå¢é‡ä¸º0.5ã€‚</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/57ecb0c8dc8d8c9903e0758d96e20555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGODcZqHO9H7KjtKldraVw.jpeg"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">è‡´æ•¬é©¬èµ›å°”Â·æ™®é²æ–¯ç‰¹:@ <a class="ae lj" href="https://fr.dreamstime.com/marcel-proust-auteur-fran%C3%A7ais-illustration-vecteur-image131669085" rel="noopener ugc nofollow" target="_blank">é©¬é‡Œå¥¥Â·å¸ƒé›·è¾¾</a></figcaption></figure><p id="696e" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">åœ¨æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„è½¬æ¢å™¨æ¥æå–ä¸Šä¸‹æ–‡æ•è·åµŒå…¥ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’å™¨ã€‚</p><p id="bbd7" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">è¿™æ¬¡æˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å¯¹æ•´ä¸ªå˜å‹å™¨è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¿™ä¹Ÿæ„å‘³ç€æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚</p><p id="ef98" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æ­¤å¤–ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨weights and biaseså¹³å°:ä»ä½¿ç”¨<strong class="kn ir"> wandb </strong> APIç™»å½•ï¼Œåˆ°åˆ›å»ºå’Œä½¿ç”¨æ¨¡å‹å·¥ä»¶ä»¥åŠé€šè¿‡æ¨¡å‹è·Ÿè¸ªã€‚</p><p id="04cd" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æ‰€æœ‰ä»£ç æºéƒ½å¯ä»¥ä»<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">æˆ‘çš„Kaggleç¬”è®°æœ¬</a>ä¸­æ£€ç´¢åˆ°</p><p id="45a1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">ä¿¡ç”¨</strong>:è¿™éƒ¨åˆ†æˆ‘å€Ÿç”¨äº†<a class="ae lj" href="https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#notebook-container" rel="noopener ugc nofollow" target="_blank"> @debarshichanda </a>æ¨¡å‹çš„æ¶æ„ã€‚</p><h1 id="add2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ›Importså’Œé…ç½®</h1><p id="9a23" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®šä¹‰<code class="fe mf mg mh mi b">CONFIG</code>å­—å…¸å’Œä¸transformerç›¸å…³çš„å¯¼å…¥ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨å®ƒä»¬:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="5c5d" class="mn jo iq mi b be mo mp l mq mr">import torch<br/>import torch.nn as nn<br/>import transformers<br/>from transformers import (<br/>    AutoModel, AutoConfig, <br/>    AutoTokenizer, logging,<br/>    AdamW, get_linear_schedule_with_warmup,<br/>    DataCollatorWithPadding,<br/>    Trainer, TrainingArguments<br/>)<br/>from transformers.modeling_outputs import SequenceClassifierOutput<br/><br/>logging.set_verbosity_error()<br/>logging.set_verbosity_warning()<br/><br/>CONFIG = {<br/>    "model_name": "microsoft/deberta-v3-base",# "distilbert-base-uncased",<br/>    "device": 'cuda' if torch.cuda.is_available() else 'cpu',<br/>    "dropout": random.uniform(0.01, 0.60),<br/>    "max_length": 512,<br/>    "train_batch_size": 8,<br/>    "valid_batch_size": 16,<br/>    "epochs": 10,<br/>    "folds" : 3,<br/>    "max_grad_norm": 1000,<br/>    "weight_decay": 1e-6,<br/>    "learning_rate": 1e-5,<br/>     "loss_type": "rmse",<br/>    "n_accumulate" : 1,<br/>    "label_cols" : ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], <br/>    <br/>}</span></pre><h1 id="6f17" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ§®Customæ•°æ®é›†è¿­ä»£å™¨:</h1><p id="3697" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">å¦‚å‰ä¸€ç¯‡æ–‡ç« æ‰€è§£é‡Šçš„ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª<code class="fe mf mg mh mi b">torch.utils.data.Dataset</code>çš„å­ç±»ï¼Œå¹¶è¦†ç›–<code class="fe mf mg mh mi b">__init__</code>ã€<code class="fe mf mg mh mi b">__len__</code>å’Œ<code class="fe mf mg mh mi b">__getitem__</code>ç‰¹æ®Šæ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤º:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7e48" class="mn jo iq mi b be mo mp l mq mr">import pandas as pd<br/><br/>train = pd.read_csv(PATH_TO_TRAIN)<br/>test = pd.read_csv(PATH_TO_TEST)<br/><br/># lets define the batch genetator<br/>class CustomIterator(torch.utils.data.Dataset):<br/>    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):<br/>        self.df = df<br/>        self.tokenizer = tokenizer<br/>        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length<br/>        self.labels = labels<br/>        self.is_train = is_train<br/>        <br/>    def __getitem__(self,idx):<br/>        tokens = self.tokenizer(<br/>                    self.df.loc[idx, 'full_text'],#.to_list(),<br/>                    add_special_tokens=True,<br/>                    padding='max_length',<br/>                    max_length=self.max_seq_length,<br/>                    truncation=True,<br/>                    return_tensors='pt',<br/>                    return_attention_mask=True<br/>                )     <br/>        res = {<br/>            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),<br/>            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()<br/>        }<br/>        <br/>        if self.is_train:<br/>            res["labels"] = torch.tensor(<br/>                self.df.loc[idx, self.labels].to_list(), <br/>            ).to(CONFIG.get('device')) <br/>            <br/>        return res<br/>    <br/>    def __len__(self):<br/>        return len(self.df)</span></pre><h1 id="d264" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ¤–å¾®è°ƒå˜å‹å™¨</h1><p id="6572" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œéšè—çŠ¶æ€ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯å¯è®­ç»ƒçš„:ä¸ºæ­¤ï¼Œå®ƒè¦æ±‚åˆ†ç±»å¤´æ˜¯<strong class="kn ir">å¯å¾®åˆ†çš„ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œä½œä¸ºåˆ†ç±»å™¨ã€‚</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ms"><img src="../Images/876fd3f755637417b3a6c3f5f849c015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cHRLl9_JJ4yDCxNI"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">Zeineb Ghrib </figcaption></figure><p id="4e8f" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨HuggingFaceæä¾›çš„ç®€å•ä¸”åŠŸèƒ½å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°API:<code class="fe mf mg mh mi b">Trainer</code>åŸºäº<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code>é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒç¼–ç å™¨å˜å‹å™¨ã€‚</p><p id="2d45" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ï¼Œç”¨ä¸€ä¸ªå¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œå¤´æ¥æ‰©å±•<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code>ã€‚</p><p id="d9d0" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">å®šåˆ¶æ¨¡å‹å°†åŒ…æ‹¬:</p><ul class=""><li id="5788" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><strong class="kn ir">é¢„è®­ç»ƒåŸºçº¿æ¨¡å‹</strong>:åŠ è½½å¸¦æœ‰<code class="fe mf mg mh mi b">AutoModel.from_pretrained</code>åŠŸèƒ½çš„é¢„è®­ç»ƒ<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code></li><li id="3fd2" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">å¹³å‡æ± å±‚</strong>:æˆ‘ä»¬éœ€è¦å¯¹ä¹‹å‰çš„å¹³å‡æ± å‡½æ•°åšä¸€äº›ä¿®æ”¹(å‚è§<a class="ae lj" href="https://medium.com/@zghrib/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9" rel="noopener">ç¬¬ä¸€éƒ¨åˆ†</a>å¸–å­):ä»<code class="fe mf mg mh mi b">torch.nn.Module</code>ç»§æ‰¿æ± ç±»ï¼Œå¹¶åœ¨<code class="fe mf mg mh mi b">forward</code>æ–¹æ³•ä¸­å®šä¹‰å¹³å‡æ± å‡½æ•°(å‚è§ä¸‹é¢çš„ä»£ç )</li><li id="650c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">æ¼å¤±å±‚</strong>:æ·»åŠ ä¸€ä¸ªæ¼å¤±å±‚è¿›è¡Œæ­£åˆ™åŒ–</li><li id="b68c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">çº¿æ€§å›¾å±‚</strong>:è¾“å…¥å°ºå¯¸= <code class="fe mf mg mh mi b">hidden_state_dim</code>ï¼Œè¾“å‡ºå°ºå¯¸=ç›®æ ‡ç‰¹å¾æ•°é‡(6)</li></ul><p id="7943" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">çº¿æ€§å±‚çš„logitsè¾“å‡ºé€šè¿‡<code class="fe mf mg mh mi b">forward</code>æ–¹æ³•ä¸Šçš„<code class="fe mf mg mh mi b">SequenceClassifierOutput</code>-<code class="fe mf mg mh mi b">ModelOutput</code>ç±»çš„å­ç±»è¿”å›(æ‰€æœ‰æ¨¡å‹çš„è¾“å‡ºå¿…é¡»æ˜¯<code class="fe mf mg mh mi b">ModelOutput</code>å­ç±»çš„å®ä¾‹:<a class="ae lj" href="https://huggingface.co/docs/transformers/main_classes/output" rel="noopener ugc nofollow" target="_blank">å¼•ç”¨æ­¤å¤„ä¸º</a>)</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="757c" class="mn jo iq mi b be mo mp l mq mr">class MeanPooling(nn.Module):<br/>    def __init__(self):<br/>        super(MeanPooling, self).__init__()  <br/>    def forward(self, last_hidden_state, attention_mask):<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()<br/>        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)<br/>        sum_mask = input_mask_expanded.sum(1)<br/>        sum_mask = torch.clamp(sum_mask, min=1e-9)<br/>        mean_embeddings = sum_embeddings / sum_mask<br/>        return mean_embeddings<br/><br/>class FeedBackModel(nn.Module):<br/>    def __init__(self, model_name):<br/>        super(FeedBackModel, self).__init__()<br/>        self.config = AutoConfig.from_pretrained(model_name)<br/>        self.config.hidden_dropout_prob = 0<br/>        self.config.attention_probs_dropout_prob = 0<br/>        self.model = AutoModel.from_pretrained(model_name, config=self.config)<br/>        self.drop = nn.Dropout(p=0.2)<br/>        self.pooler = MeanPooling()<br/>        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))<br/>        <br/>    def forward(self, input_ids, attention_mask):<br/>        out = self.model(input_ids=input_ids,<br/>                         attention_mask=attention_mask, <br/>                         output_hidden_states=False)<br/>        out = self.pooler(out.last_hidden_state, attention_mask)<br/>        out = self.drop(out)<br/>        outputs = self.fc(out)<br/>        return SequenceClassifierOutput(logits=outputs)</span></pre><h2 id="9ce1" class="nj jo iq bd jp nk nl dn jt nm nn dp jx kw no np kb la nq nr kf le ns nt kj nu bi translated">æŸå¤±å’Œåº¦é‡</h2><p id="14b8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe mf mg mh mi b">Trainer</code>ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå¯¹åº”äºç›®æ ‡è¯„ä¼°æŒ‡æ ‡çš„æ–°æŸå¤±å‡½æ•°(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸ºMCRMSE)ã€‚è¯¥æŸå¤±å‡½æ•°å°†ç”¨äºè®­ç»ƒå˜å‹å™¨ã€‚å®ç°çš„æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ª<code class="fe mf mg mh mi b">subclassing Trainer</code>å¹¶è¦†ç›–<code class="fe mf mg mh mi b">compute_loss()</code>æ–¹æ³•ã€‚</p><p id="a313" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">åŒæ ·ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¯„ä¼°æ­¥éª¤ä¸­è·å¾—æ¯ä¸ªç›®æ ‡ç±»çš„å±€éƒ¨è¯„ä¼°ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸º<code class="fe mf mg mh mi b">Trainer</code>æä¾›ä¸€ä¸ªè‡ªå®šä¹‰çš„<code class="fe mf mg mh mi b">compute_metrics()</code>å‡½æ•°ï¼Œè¯¥å‡½æ•°å…è®¸è®¡ç®—å…­ä¸ªç›®æ ‡ä¸­æ¯ä¸ªç›®æ ‡çš„RMSE(å¦åˆ™ï¼Œè¯„ä¼°å°†åªè¿”å›æŸå¤±è¯„ä¼°MCRMSE)ã€‚</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="50c6" class="mn jo iq mi b be mo mp l mq mr">class RMSELoss(nn.Module):<br/>    """<br/>    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)<br/>    """<br/>    def __init__(self, reduction='mean', eps=1e-9):<br/>        super().__init__()<br/>        self.mse = nn.MSELoss(reduction='none')<br/>        self.reduction = reduction<br/>        self.eps = eps<br/><br/>    def forward(self, predictions, targets):<br/>        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)<br/>        if self.reduction == 'none':<br/>            loss = loss<br/>        elif self.reduction == 'sum':<br/>            loss = loss.sum()<br/>        elif self.reduction == 'mean':<br/>            loss = loss.mean()<br/>        return loss<br/><br/>class CustomTrainer(Trainer):<br/>    def compute_loss(self, model, inputs, return_outputs=False):<br/>        outputs = model(inputs['input_ids'], inputs['attention_mask'])<br/>        loss_func = RMSELoss(reduction='mean')<br/>        loss = loss_func(outputs.logits.float(), inputs['labels'].float())<br/>        return (loss, outputs) if return_outputs else loss<br/><br/>def compute_metrics(eval_pred):<br/>    predictions, labels = eval_pred<br/>    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))<br/>    res = {<br/>        f"{analytic.upper()}_RMSE" : colwise_rmse[i]<br/>        for i, analytic in enumerate(CONFIG["label_cols"])<br/>    }<br/>    res["MCRMSE"] = np.mean(colwise_rmse)<br/>    return res</span></pre><h1 id="9297" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ§šWeightså’ŒBiasesğŸ§š</h1><p id="4d7b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">å°½ç®¡HuggingFace Transformersæä¾›äº†å¹¿æ³›çš„è®­ç»ƒæ£€æŸ¥ç‚¹åŠŸèƒ½ã€‚W &amp; B æä¾›å¼ºå¤§çš„å®éªŒè·Ÿè¸ªå’Œæ¨¡å‹ç‰ˆæœ¬åŒ–å·¥å…·ï¼Œå¸¦æœ‰å‹å¥½çš„äº¤äº’å¼ä»ªè¡¨ç›˜ã€‚æ¯ä¸ªå®éªŒé¡¹ç›®éƒ½æ˜¯ç‹¬ç«‹åˆ’åˆ†çš„ã€‚</p><p id="e567" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æŸ¥çœ‹<a class="ae lj" href="https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases#%F0%9F%96%A5-Dashboard-(experiment-tracking" rel="noopener ugc nofollow" target="_blank">è¿™æ¬¾å‡ºè‰²çš„ç¬”è®°æœ¬</a>ï¼Œå®ƒè¯¦ç»†æè¿°äº†å¦‚ä½•åœ¨kaggleä¸­ä½¿ç”¨W &amp; B:</p><blockquote class="nv nw nx"><p id="8553" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><em class="iq">W&amp;Bæä¾›äº†ä¸¤ä¸ªä¸»è¦çš„å®ç”¨ç¨‹åº:</em></p><p id="0370" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">ä»ªè¡¨æ¿</strong>(å®éªŒè·Ÿè¸ª):å®æ—¶è®°å½•å’Œå¯è§†åŒ–å®éªŒ=å°†æ•°æ®å’Œç»“æœä¿å­˜åœ¨ä¸€ä¸ªæ–¹ä¾¿çš„åœ°æ–¹ã€‚æŠŠè¿™çœ‹ä½œæ˜¯ä¸€ä¸ªå®éªŒçš„å®åº“ã€‚</p><p id="7cc0" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">ğŸ¤™<strong class="kn ir">å·¥ä»¶</strong>(æ•°æ®é›†+æ¨¡å‹ç‰ˆæœ¬åŒ–):å­˜å‚¨å’Œç‰ˆæœ¬åŒ–æ•°æ®é›†ã€æ¨¡å‹å’Œç»“æœ=ç¡®åˆ‡åœ°çŸ¥é“æ¨¡å‹è¢«è®­ç»ƒçš„æ•°æ®ã€‚</p></blockquote><p id="f1b9" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ä¸ºäº†è¿æ¥åˆ°Weights &amp; Biasesï¼Œæˆ‘ä»¬éœ€è¦ä»<a class="ae lj" href="https://wandb.ai/authorize" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/authorize</a>è®¿é—®æ‚¨çš„APIå¯†é’¥ã€‚<br/>æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä½¿ç”¨Kaggleå†…æ ¸ç™»å½•:</p><ul class=""><li id="7783" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated">è¿è¡Œ<code class="fe mf mg mh mi b">wandb.login(key=your-api-key)</code> cmd:å®ƒä¼šè¦æ±‚API key:ä½ å¯ä»¥ä»<a class="ae lj" href="https://wandb.ai/authorize" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/authorize</a>å¤åˆ¶/ç²˜è´´ã€‚</li></ul><p id="f8ad" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><em class="ny">å¦‚æœä½ ä¸ä½¿ç”¨Kaggle </em>ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€éƒ¨åˆ†</p><ul class=""><li id="b83c" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated">ä½¿ç”¨Kaggle secretså­˜å‚¨æ‚¨çš„APIå¯†é’¥:å¹¶ä½¿ç”¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µç™»å½•ã€‚</li></ul><ol class=""><li id="7453" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">ç‚¹å‡»ç¬”è®°æœ¬ç¼–è¾‘å™¨ä¸­çš„<code class="fe mf mg mh mi b">Add-ons</code>èœå•ï¼Œç„¶åç‚¹å‡»<code class="fe mf mg mh mi b">Secrets</code>:</li></ol><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi od"><img src="../Images/10c8db78e76639fdd664e0949ea47fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*XibclEUc-hrZaCIw"/></div></figure><p id="e7da" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">2.å°†api-keyå­˜å‚¨ä¸ºå°†é™„åŠ åˆ°å½“å‰ç¬”è®°æœ¬çš„é”®å€¼å¯¹:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oe"><img src="../Images/257fe84457dc82921c33891157d60fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1J-jerDNO6RSpzus"/></div></div></figure><p id="a255" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">3.å¤åˆ¶å¹¶ç²˜è´´ä»£ç ç‰‡æ®µä»¥è®¿é—®api-keyï¼Œç„¶åä½¿ç”¨<code class="fe mf mg mh mi b">wandb.login()</code>è¿æ¥åˆ°W &amp; B:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="f43c" class="mn jo iq mi b be mo mp l mq mr">from kaggle_secrets import UserSecretsClient<br/>import wandb<br/><br/>user_secrets = UserSecretsClient()<br/>api_key = user_secrets.get_secret("wandb_api")<br/>wandb.login(key=api_key)</span></pre><h1 id="6a0c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ› Wandbè®ºç‚¹</h1><p id="0ad9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">å¯¹äºæ¯ä¸ªCVè¿­ä»£â€˜I â€™,æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º<strong class="kn ir"> FB3-fold-i </strong>çš„æ–°è¿è¡Œï¼Œå…¶ä¸­â€˜I â€™=è¿­ä»£çš„ç¼–å·ï¼Œåœ¨ä¸€ä¸ªåä¸º<strong class="kn ir"> Feedback3-deberta </strong>çš„é¡¹ç›®ä¸­ã€‚</p><p id="c9ca" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ä¸€äº›å…¶ä»–å‚æ•°:</p><ul class=""><li id="da7d" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">group</code>:ç¾¤ç»„å‚æ•°ç‰¹åˆ«ç”¨äºå°†å•ä¸ªå®éªŒç»„ç»‡æˆä¸€ä¸ªæ›´å¤§çš„å®éªŒï¼Œ<a class="ae lj" href="https://docs.wandb.ai/guides/track/advanced/grouping" rel="noopener ugc nofollow" target="_blank">ä¸‹é¢æ˜¯ä¸€äº›ç”¨ä¾‹ç¤ºä¾‹</a></li><li id="3d9f" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">tags</code>:æˆ‘ä»¬å°†æ·»åŠ å‹å·åç§°å’Œå…¬åˆ¶æ ‡ç­¾ã€‚æ­£å¦‚W &amp; Bä¸­æ‰€è§£é‡Šçš„ï¼Œdocæ ‡ç­¾å¯¹äºç»„ç»‡ä¸€èµ·è¿è¡Œæˆ–åº”ç”¨ä¸´æ—¶æ ‡ç­¾å¦‚â€œåŸºçº¿â€æˆ–â€œç”Ÿäº§â€æ˜¯æœ‰ç”¨çš„ã€‚å¾ˆå®¹æ˜“åœ¨UIä¸­æ·»åŠ å’Œåˆ é™¤æ ‡ç­¾ï¼Œæˆ–è€…è¿‡æ»¤åˆ°åªè¿è¡Œç‰¹å®šçš„æ ‡ç­¾ã€‚</li><li id="a077" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated">é€šå¸¸ä¸æ˜¯â€œè®­ç»ƒâ€å°±æ˜¯â€œè¯„ä¼°â€ã€‚ç¨åï¼Œå®ƒå°†å…è®¸å¯¹ç›¸ä¼¼çš„è¿è¡Œè¿›è¡Œè¿‡æ»¤å’Œåˆ†ç»„ã€‚æˆ‘ä»¬å°†æŠŠjob_typeè®¾ç½®ä¸º<strong class="kn ir">â€œtrainâ€</strong></li><li id="7b87" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">anonymous</code>:è¯¥å‚æ•°å…è®¸æ§åˆ¶åŒ¿åè®°å½•ã€‚æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸º<strong class="kn ir">â€œmustâ€</strong>ï¼Œè¿™å°†æŠŠè·‘æ­¥å‘é€åˆ°ä¸€ä¸ªåŒ¿åå¸æˆ·ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ³¨å†Œç”¨æˆ·å¸æˆ·ã€‚å¯¹äºå…¶ä»–é€‰é¡¹ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹<a class="ae lj" href="https://docs.wandb.ai/ref/python/init" rel="noopener ugc nofollow" target="_blank">æ–‡æ¡£</a></li></ul><p id="aa74" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">å¯¹äºæ¯ä¸ªCVè¿­ä»£ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®ä¾‹åŒ–ä¸€ä¸ªwandbè¿è¡Œ:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="2e94" class="mn jo iq mi b be mo mp l mq mr">run = wandb.init(project="FB3-deberta-v3", <br/>                   config=CONFIG,<br/>                   job_type='train',<br/>                   group="FB3-BASELINE-MODEL",<br/>                   tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                   name=f'FB3-fold-{fold}',<br/>                   anonymous='must')</span></pre><p id="fdff" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰æ‹¥æŠ±è„¸<code class="fe mf mg mh mi b">Trainer</code>å°†ä½¿ç”¨çš„è®­ç»ƒå‚æ•°</p><h1 id="4f4c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ› Trainingè®ºç‚¹</h1><p id="dc95" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">åœ¨å®ä¾‹åŒ–æˆ‘ä»¬çš„å®šåˆ¶æ•™ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª<code class="fe mf mg mh mi b">TrainingArguments</code>æ¥å®šä¹‰è®­ç»ƒé…ç½®ã€‚<br/>æˆ‘ä»¬å°†è®¾ç½®ä»¥ä¸‹å‚æ•°:</p><ul class=""><li id="6f47" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">output_dir</code>:æ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹å°†è¢«å†™å…¥çš„è¾“å‡ºç›®å½•:æ¯ä¸ªCVè¿­ä»£å°†æœ‰å®ƒè‡ªå·±çš„ç›®å½•ï¼Œå…¶åç§°ç­‰äºä»¥<strong class="kn ir">â€œè¾“å‡º-â€</strong>ä¸ºå‰ç¼€çš„è¿­ä»£æ¬¡æ•°</li><li id="1250" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">evaluation_strategy</code>:è®¾ç½®ä¸º<strong class="kn ir">â€œepochâ€</strong>ï¼Œè¡¨ç¤ºåœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚</li><li id="76af" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">per_device_train_batch_size</code>:è®­ç»ƒçš„æ‰¹é‡ã€‚æˆ‘ä»¬å°†å®ƒè®¾ç½®ä¸º8</li><li id="c817" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">per_device_eval_batch_size</code>:è¯„ä¼°çš„æ‰¹é‡ã€‚æˆ‘ä»¬å°†å®ƒè®¾ç½®ä¸º16(ä»¥åŠ å¿«æ—¶é—´æ‰§è¡Œ)</li><li id="1a25" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">num_train_epochs</code>:è®­ç»ƒæ—¶æœŸæ•°ã€‚æé†’ä¸€ä¸‹ï¼Œåœ¨ä¸€ä¸ªæ—¶æœŸå†…ï¼Œæ¨¡å‹å·²ç»çœ‹åˆ°äº†è®­ç»ƒæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬</li><li id="835a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">group_by_length</code>:åªè¦æˆ‘ä»¬å°†ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œæˆ‘ä»¬å°±å°†è¯¥å‚æ•°è®¾ç½®ä¸º<code class="fe mf mg mh mi b">True</code>ï¼Œä»¥å°†è®­ç»ƒæ•°æ®é›†ä¸­é•¿åº¦å¤§è‡´ç›¸åŒçš„æ ·æœ¬åˆ†ç»„åœ¨ä¸€èµ·(ä»¥æœ€å°åŒ–æ‰€åº”ç”¨çš„å¡«å……å¹¶æé«˜æ•ˆç‡)</li><li id="93b6" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">max_grad_norm</code>:æœ€å¤§æ¢¯åº¦èŒƒæ•°(ç”¨äºæ¢¯åº¦è£å‰ª)ã€‚</li><li id="ec46" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">learning_rate</code>:AdamWä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ã€‚æé†’ä¸€ä¸‹ï¼ŒAdamWä¼˜åŒ–å™¨</li><li id="af3d" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">weight_decay</code>:åº”ç”¨äºAdamWä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡:åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡è¡°å‡åº”ç”¨äºé™¤åå·®å’Œæ ‡å‡†åŒ–å±‚ä¹‹å¤–çš„æ‰€æœ‰å±‚<strong class="kn ir"/></li></ul><p id="8d1c" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">æ³¨:</strong> <br/>æƒé‡è¡°å‡æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ƒç»™æŸå¤±å‡½æ•°(é€šå¸¸æ˜¯æƒé‡çš„L2èŒƒæ•°)å¢åŠ äº†ä¸€ä¸ªå°çš„æƒ©ç½šã€‚<br/>æŸè€—=æŸè€—+æƒé‡_è¡°å‡_å‚æ•°* L2 _èŒƒæ•°_æƒé‡</p><p id="adc3" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ä¸€äº›å®ç°ä»…å¯¹æƒé‡è€Œä¸æ˜¯åå·®åº”ç”¨æƒé‡è¡°å‡ã€‚å¦ä¸€æ–¹é¢ï¼ŒPyTorchå°†æƒé‡è¡°å‡åº”ç”¨äºæƒé‡å’Œåç§»ã€‚</p><blockquote class="nv nw nx"><p id="06dd" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><strong class="kn ir">ä¸ºä»€ä¹ˆä½“é‡ä¼šè¡°å‡ï¼Ÿ</strong></p><p id="fa10" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">1.ä»¥é˜²æ­¢è¿‡åº¦æ‹Ÿåˆã€‚<br/> 2ã€‚ä¸ºäº†é¿å…çˆ†ç‚¸æ¢¯åº¦:ç”±äºé¢å¤–çš„L2èŒƒæ•°ï¼Œé™¤äº†æŸå¤±ä¹‹å¤–ï¼Œç½‘ç»œçš„æ¯æ¬¡è¿­ä»£å°†è¯•å›¾ä¼˜åŒ–æ¨¡å‹æƒé‡ã€‚è¿™å°†æœ‰åŠ©äºä¿æŒæƒé‡å°½å¯èƒ½å°ï¼Œé˜²æ­¢æƒé‡å¢é•¿å¤±æ§ï¼Œä»è€Œé¿å…çˆ†ç‚¸æ¢¯åº¦</p></blockquote><ul class=""><li id="b07d" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">gradient_accumulation_steps</code>:åœ¨æ‰§è¡Œå‘åä¼ é€’ä¹‹å‰ï¼Œæ¢¯åº¦åº”è¯¥ç´¯ç§¯çš„æ­¥æ•°:å½“ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œæ¢¯åº¦è®¡ç®—æ˜¯åœ¨è¾ƒå°çš„æ­¥ä¸­å®Œæˆçš„ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¸ºä¸€æ‰¹å®Œæˆï¼›1(è¡¨ç¤ºæ²¡æœ‰æ¢¯åº¦ç´¯ç§¯))</li></ul><blockquote class="nv nw nx"><p id="4b93" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><strong class="kn ir">æ³¨</strong> : <br/>åœ¨æœ¬<a class="ae lj" href="https://stackoverflow.com/questions/74065165/getting-cuda-error-when-trying-to-train-mbart-model" rel="noopener ugc nofollow" target="_blank"> Stackoverflowè®¨è®º</a>ä¸­ï¼Œå·²ç»è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨set <code class="fe mf mg mh mi b">gradient_accumulation_steps</code>å‚æ•°æ¥é¿å…OOMé”™è¯¯:å°†<code class="fe mf mg mh mi b">gradient_accumulation_steps</code>å‚æ•°è®¾ç½®ä¸ºä¸€ä¸ªé€‚åˆå†…å­˜çš„æ•°å­—ï¼Œå¹¶å°†<code class="fe mf mg mh mi b">per_device_train_batch_size</code>ä¿®æ”¹ä¸º<code class="fe mf mg mh mi b">original_batch_size/gradient_accumulation_steps</code>:è¿™æ ·æ¢¯åº¦å°†åœ¨<code class="fe mf mg mh mi b">gradient_accumulation_steps</code>ä¸Šç´¯ç§¯ï¼Œå¹¶é€šè¿‡<code class="fe mf mg mh mi b">gradient_accumulation_steps</code> * <code class="fe mf mg mh mi b">original_batch_size/gradient_accumulation_steps</code> = <code class="fe mf mg mh mi b">original_batch_size</code>æ ·æœ¬æ‰§è¡Œå‘åä¼ é€’ã€‚è®­ç»ƒæ­¥éª¤çš„æ€»æ•°æ˜¯:</p></blockquote><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2903204a08c267677da85ead89619bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*P35rCfoKGywIUS9M07o4vA.png"/></div></figure><ul class=""><li id="ac45" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">load_best_model_at_end</code>:æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸ºTrueï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹:åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ<code class="fe mf mg mh mi b">save_strategy</code>å¿…é¡»ä¸<code class="fe mf mg mh mi b">evaluation_strategy</code> : <strong class="kn ir"> epoch </strong>ç›¸åŒ</li><li id="e97b" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">metric_for_best_model</code>:æˆ‘ä»¬å°†è®¾ç½®ç«äº‰æŒ‡æ ‡<strong class="kn ir"> MCRMSE </strong>æˆ–<strong class="kn ir"> eval_MCRMSE </strong>(å¸¦æœ‰eval_ prefix)</li><li id="b76f" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">greater_is_better</code>:è®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬æƒ³å¾—åˆ°MCRMSEè¾ƒä½çš„æ¨¡å‹</li><li id="d5ba" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">save_total_limit</code>:æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸º1ï¼Œä»¥ä¾¿æ¯æ¬¡æ€»æ˜¯ä¿ç•™ä¸€ä¸ªæ£€æŸ¥ç‚¹(output_dirä¸­è¾ƒæ—§çš„æ£€æŸ¥ç‚¹å°†è¢«åˆ é™¤)ã€‚</li><li id="dc65" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">report_to</code>:ç”±äºæˆ‘ä»¬è¿æ¥åˆ°W &amp; Bï¼Œæˆ‘ä»¬å°†æŠŠreport_to logsè®¾ç½®ä¸º<strong class="kn ir">â€œwandbâ€</strong></li><li id="f562" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">label_name</code>:å°†label_nameå‚æ•°åˆ—è¡¨è®¾ç½®ä¸º<strong class="kn ir"> ["labels"]ï¼Œ</strong>ï¼Œå¯¹åº”ç›®æ ‡ç±»å¯¹åº”çš„è‡ªå®šä¹‰<code class="fe mf mg mh mi b">Dataloader</code>ç”Ÿæˆçš„é¢„å®šä¹‰å­—æ®µ</li></ul><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="474a" class="mn jo iq mi b be mo mp l mq mr">training_args = TrainingArguments(<br/>        output_dir=f"outputs-{fold}/",<br/>        evaluation_strategy="epoch",<br/>        per_device_train_batch_size=CONFIG['train_batch_size'],<br/>        per_device_eval_batch_size=CONFIG['valid_batch_size'],<br/>        num_train_epochs=CONFIG['epochs'],<br/>        learning_rate=CONFIG['learning_rate'],<br/>        weight_decay=CONFIG['weight_decay'],<br/>        gradient_accumulation_steps=CONFIG['n_accumulate'],<br/>        seed=SEED,<br/>        group_by_length=True,<br/>        max_grad_norm=CONFIG['max_grad_norm'],<br/>        metric_for_best_model='eval_MCRMSE',<br/>        load_best_model_at_end=True,<br/>        greater_is_better=False,<br/>        save_strategy="epoch",<br/>        save_total_limit=1,<br/>        report_to="wandb",<br/>        label_names=["labels"]<br/>    )</span></pre><p id="5805" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä¸º<code class="fe mf mg mh mi b">Trainer</code>å®šä¹‰ä¸€äº›å…¶ä»–å‚æ•°:</p><ul class=""><li id="0b47" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><strong class="kn ir">æ•°æ®æ•´ç†å™¨</strong>:æˆ‘ä»¬éœ€è¦å®šä¹‰å¦‚ä½•ä»<code class="fe mf mg mh mi b">Dataloader<br/></code>è¿”å›çš„æ•°æ®è¾“å…¥åˆ—è¡¨ä¸­åˆ›å»ºä¸€ä¸ªæ‰¹å¤„ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe mf mg mh mi b">DataCollatorWithPadding</code>æ¥åŠ¨æ€å¡«å……æ¥æ”¶åˆ°çš„è¾“å…¥ã€‚</li><li id="2149" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">ä¼˜åŒ–å™¨</strong>:æˆ‘ä»¬å°†åœ¨æ‰€æœ‰å±‚ä½¿ç”¨<code class="fe mf mg mh mi b">AdamW</code>ï¼Œé™¤äº†åç½®å’Œæ ‡å‡†åŒ–å±‚</li><li id="1290" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">è°ƒåº¦å™¨</strong>:æˆ‘ä»¬å°†ä½¿ç”¨<code class="fe mf mg mh mi b">get_linear_schedule_with_warmup</code>åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é¢„çƒ­æœŸçš„è°ƒåº¦ï¼Œåœ¨æ­¤æœŸé—´ï¼Œå­¦ä¹ ç‡ä»0çº¿æ€§å¢åŠ åˆ°åˆå§‹lr(åœ¨ä¼˜åŒ–å™¨ä¸­è®¾ç½®)ï¼Œç„¶åä»ä¼˜åŒ–å™¨ä¸­è®¾ç½®çš„åˆå§‹lrçº¿æ€§å‡å°‘åˆ°0ã€‚<br/>è°ƒåº¦å™¨å…è®¸æˆ‘ä»¬ä¿æŒå¯¹å­¦ä¹ ç‡çš„æ§åˆ¶ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ç¡®ä¿å­¦ä¹ ç‡çš„æ¯æ¬¡æ›´æ–°ä¸è¶…è¿‡Î»å€¼(æŸ¥çœ‹è¿™ä¸ª<a class="ae lj" href="https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer" rel="noopener ugc nofollow" target="_blank"> Stackoverflowè®¨è®º</a>å…³äºä¼˜åŒ–å™¨è°ƒåº¦å™¨çš„æ•ˆç”¨)</li></ul><p id="ecca" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">è¦å¯åŠ¨äº¤å‰éªŒè¯åŸ¹è®­ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»æŒ‰ç…§<a class="ae lj" href="https://zghrib.medium.com/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9" rel="noopener">ç¬¬ä¸€éƒ¨åˆ†å¸–å­</a>ä¸­çš„è¯´æ˜åˆ›å»ºç®€å†æŠ˜å æ :</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="332e" class="mn jo iq mi b be mo mp l mq mr"># set seed to produce similar folds<br/>cv = MultilabelStratifiedKFold(n_splits=CONFIG.get("folds", 3), shuffle=True, random_state=SEED)<br/><br/>train = train.reset_index(drop=True)<br/>for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[CONFIG['label_cols']])):<br/>    train.loc[val_idx , "fold"] = int(fold)<br/>    <br/>train["fold"] = train["fold"].astype(int)</span></pre><p id="0e3d" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">CVåŸ¹è®­å·¥ä½œæµç¨‹å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å®æ–½:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7630" class="mn jo iq mi b be mo mp l mq mr"># Data Collator for Dynamic Padding<br/>collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)<br/># init predictions by fold<br/>predictions = {}<br/>for fold in range(0, CONFIG['folds']):<br/>    print(f" ---- Fold: {fold} ----")<br/>    run = wandb.init(project="FB3-deberta-v3", <br/>                     config=CONFIG,<br/>                     job_type='train',<br/>                     group="FB3-BASELINE-MODEL",<br/>                     tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                     name=f'FB3-fold-{fold}',<br/>                     anonymous='must')<br/>    # the reset index is VERY IMPORTANT for the Dataset iterator<br/>    df_train = train[train.fold != fold].reset_index(drop=True)<br/>    df_valid = train[train.fold == fold].reset_index(drop=True)<br/>    # create iterators<br/>    train_dataset = CustomIterator(df_train, tokenizer)<br/>    valid_dataset = CustomIterator(df_valid, tokenizer)<br/>    # init model<br/>    model = FeedBackModel(CONFIG['model_name'])<br/>    model.to(CONFIG['device'])<br/>    <br/>    # SET THE OPITMIZER AND THE SCHEDULER<br/>    # no decay for bias and normalization layers<br/>    param_optimizer = list(model.named_parameters())<br/>    no_decay = ["bias", "LayerNorm.weight"]<br/>    optimizer_parameters = [<br/>        {<br/>            "params": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],<br/>            "weight_decay": CONFIG['weight_decay'],<br/>        },<br/>        {<br/>            "params": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],<br/>            "weight_decay": 0.0,<br/>        },<br/>    ]<br/>    optimizer = AdamW(optimizer_parameters, lr=CONFIG['learning_rate'])<br/>    num_training_steps = (len(train_dataset) * CONFIG['epochs']) // (CONFIG['train_batch_size'] * CONFIG['n_accumulate'])<br/>    scheduler = get_linear_schedule_with_warmup(<br/>        optimizer,<br/>        num_warmup_steps=0.1*num_training_steps,<br/>        num_training_steps=num_training_steps<br/>    )<br/>    # CREATE THE TRAINER<br/>    trainer = CustomTrainer(<br/>        model=model,<br/>        args=training_args,<br/>        train_dataset=train_dataset,<br/>        eval_dataset=valid_dataset,<br/>        data_collator=collate_fn,<br/>        optimizers=(optimizer, scheduler),<br/>        compute_metrics=compute_metrics<br/>    )<br/>    # LAUNCH THE TRAINER<br/>    trainer.train()</span></pre><p id="eebe" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ä½ å¯ä»¥è®¿é—®æˆ‘ä¸ºè¿™ä¸ªé¡¹ç›®åˆ›å»ºçš„å…¬å…±W&amp;Bä»ªè¡¨æ¿:<a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/athena75/FB3-deberta-v3?å·¥ä½œç©ºé—´=ç”¨æˆ·-é›…å…¸å¨œ75 </a></p><h1 id="301d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ—¿åˆ›å»ºW&amp;Bå·¥ä»¶</h1><p id="988f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">å¯¹äºä»¥åçš„ä½¿ç”¨ï¼Œä¸€æ—¦æ¨¡å‹è¢«å¾®è°ƒï¼ŒW&amp;Béå¸¸æ–¹ä¾¿åœ°åˆ›å»ºæ¨¡å‹å·¥ä»¶ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä»¥åä½¿ç”¨å®ƒä»¬ï¼Œå¹¶åˆ›å»ºæˆ‘ä»¬æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚</p><p id="38c1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">è¦åˆ›å»ºä¸€ä¸ªæ¨¡å‹å·¥ä»¶ï¼Œæ‚¨æ‰€è¦åšçš„å°±æ˜¯:</p><ol class=""><li id="8137" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">åˆ›å»ºä¸€ä¸ªåå­—æ¸…æ™°ä¸€è‡´çš„<code class="fe mf mg mh mi b">wandb.Artifact</code>å¯¹è±¡ï¼Œä½ å¿…é¡»æŒ‡å®š<code class="fe mf mg mh mi b">type</code>å‚æ•°ï¼Œå®ƒå¯ä»¥æ˜¯<code class="fe mf mg mh mi b">dataset</code>æˆ–<code class="fe mf mg mh mi b">model</code>ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯<code class="fe mf mg mh mi b">model</code></li><li id="24a3" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">å°†æœ¬åœ°ç›®å½•æ·»åŠ åˆ°å·¥ä»¶ä¸­:äº‹å®ä¸Šï¼Œä¸€æ—¦æ‚¨å®ä¾‹åŒ–äº†æ¨¡å‹å¹¶å¼€å§‹å¾®è°ƒå®ƒï¼Œå®ƒå°±ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«æ¨¡å‹<code class="fe mf mg mh mi b">bin</code>ä»¥åŠæ¨¡å‹çŠ¶æ€å’Œé…ç½®çš„æœ¬åœ°æ£€æŸ¥ç‚¹ã€‚ä½ å¾—æŠŠå®ƒåŠ åˆ°è‰ºæœ¯å“ä¸Šã€‚</li><li id="b29a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">ä¸€æ—¦å·¥ä»¶æ‹¥æœ‰äº†æ‰€æœ‰éœ€è¦çš„æ–‡ä»¶ï¼Œæ‚¨å°±å¯ä»¥è°ƒç”¨<code class="fe mf mg mh mi b">wandb.log_artifact()</code>æ¥è®°å½•å®ƒã€‚</li></ol><p id="ab37" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ä¸‹é¢æ˜¯ä¸€ä¸ªä¸ºæ¯ä¸ªCVæ¨¡å‹åˆ›å»ºå·¥ä»¶çš„ä»£ç ç‰‡æ®µç¤ºä¾‹:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="b156" class="mn jo iq mi b be mo mp l mq mr">for fold in range(0, CONFIG['folds']):<br/>    run = wandb.init(project="FB3-deberta-v3", <br/>                         config=CONFIG,<br/>                         job_type='train',<br/>                         group="FB3-BASELINE-MODEL",<br/>                         tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                         name=f'FB3-fold-{fold}',<br/>                         anonymous='must')<br/><br/>    trainer = CustomTrainer(<br/>              .....<br/>    )<br/>    ##### TRAIN / FINE-TUNE ####<br/>    # create model artifact<br/>    model_artifact = wandb.Artifact(f'FB3-fold-{fold}', type="model",<br/>                                   description=f"MultilabelStratified - fold--{fold}")<br/>    # save locally the model - it would create a local dir<br/>    trainer.save_model(f'fold-{fold}')<br/>    # add the local dir to the artifact<br/>    model_artifact.add_dir(f'fold-{fold}')<br/>    # log artifact<br/>    # it would save the artifact version and declare the artifact as an output of the run<br/>    run.log_artifact(model_artifact)<br/>    <br/>    run.finish()<br/>  <br/></span></pre><h1 id="e4fd" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">inferenceâœ¨:çš„âœ¨Use W&amp;Bè‰ºæœ¯å“</h1><p id="d920" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥<strong class="kn ir">ä½¿ç”¨Weights&amp;biasæœåŠ¡å™¨ä¸Šå­˜å‚¨çš„å·¥ä»¶</strong>ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”Ÿæˆæ¨¡å‹é¢„æµ‹å¹¶ç”Ÿæˆèšåˆè¾“å‡ºé¢„æµ‹ã€‚</p><p id="38e1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">PS:å¯ä»¥ç›´æ¥ä»W&amp;Bæ¥å£æå–ä½¿ç”¨ä»£ç <a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta/artifacts/model/FB3-fold-0/93c08783e5b7c696451a/usage" rel="noopener ugc nofollow" target="_blank">https://wandb . ai/Athena 75/feedback 3-deberta/artifacts/model/FB3-fold-0/93c 08783 e5b 7c 696451 a/usage</a></p><ol class=""><li id="0917" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">ç™»å½•åˆ°æ‚¨çš„wandbå¸æˆ·ï¼Œç”¨<code class="fe mf mg mh mi b">wandb.init()</code>å®ä¾‹åŒ–ä¸€ä¸ªé»˜è®¤è¿è¡Œ</li><li id="5c9c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">å‘<code class="fe mf mg mh mi b">use_artifact()</code>æ–¹æ³•æŒ‡å‡ºå·¥ä»¶çš„è·¯å¾„ä»¥åŠæ£€ç´¢å·¥ä»¶çš„ç±»å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯<code class="fe mf mg mh mi b">model</code>)</li><li id="b690" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">ä½¿ç”¨<code class="fe mf mg mh mi b">download()</code>æ–¹æ³•åœ¨æœ¬åœ°ä¸‹è½½å·¥ä»¶ç›®å½•</li><li id="a452" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">åŠ è½½æœ¬åœ°æ¨¡å‹å¹¶ä½¿ç”¨å®ƒè¿›è¡Œé¢„æµ‹</li></ol><p id="a99c" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">å®æ–½ç¤ºä¾‹:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7920" class="mn jo iq mi b be mo mp l mq mr">predictions = torch.zeros((len(test), len(CONFIG['label_cols']))<br/><br/>for fold in range(CONFIG["folds"]):<br/>    print(f"---- FOLD {fold} -------")<br/>    # instantiate deafault run<br/>    run = wandb.init()<br/>    # Indicate the artifact we want to use with the use_artifact method.<br/>    artifact = run.use_artifact(f'athena75/FB3-deberta-10/FB3-fold-{fold}:v0', type='model')<br/>    # download locally the model<br/>    artifact_dir = artifact.download()<br/>    # load the loacal model<br/>    # it is a pytorch moeal: loaded as follows<br/>    # https://pytorch.org/tutorials/beginner/saving_loading_models.html<br/>    model = FeedBackModel(CONFIG['model_name'])<br/>    model.load_state_dict(torch.load(f'artifacts/FB3-fold-{fold}:v0/pytorch_model.bin'))<br/>    # generate test embediings<br/>    test_dataset = CustomIterator(test, tokenizer, is_train=False)<br/>    test_dataloader = torch.utils.data.DataLoader(<br/>            test_dataset, <br/>            batch_size=CONFIG["train_batch_size"],<br/>            shuffle=False<br/>        )<br/>    input_ids, attention_mask = tuple(next(iter(test_dataloader)).values())<br/>    input_ids = input_ids.to('cpu')<br/>    attention_mask = attention_mask.to('cpu')<br/>    # genreate predictions<br/>    fold_preds = model(input_ids, attention_mask)<br/>    predictions = fold_preds.logits.add(predictions)<br/>    # remove local dir to save space<br/>    shutil.rmtree('artifacts')</span></pre><h1 id="53d8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ğŸ™å­¦åˆ†:</h1><p id="15b4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">è¿™é¡¹å·¥ä½œæ˜¯è¿™äº›ä¼˜ç§€èµ„æºçš„ç‹¬ç‰¹èµ·æºï¼Œè¯·ä¸è¦çŠ¹è±«æŠ•ç¥¨æ”¯æŒKaggleèµ„æº:</p><ul class=""><li id="d2ec" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @rhtsingh </strong>ç¬”è®°æœ¬</a>:æ¢ç´¢å˜å‹å™¨è¡¨è±¡çš„ä¸åŒæ–¹å¼</li><li id="f44a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#Loss-Function" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @debarshichanda </strong>ä½œå“</a>:å€Ÿç”¨äº†è®­ç»ƒå™¨è¶…å‚æ•°å’Œæ¨¡å‹æ¶æ„</li><li id="87e7" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @Y.NAKAMA </strong>ç¬”è®°æœ¬</a>:æŸå¤±å‡½æ•°å’Œå¤šæ ‡ç­¾â€”åˆ†å±‚äº¤å‰éªŒè¯</li><li id="832a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/shreydan/using-transformers-for-the-first-time-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @shreydan </strong>ç¬”è®°æœ¬</a>:ç¬”è®°æœ¬çš„å…¨çƒé£æ ¼</li></ul><h1 id="f70c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">ç»“è®º:</h1><p id="5693" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">éå¸¸æ„Ÿè°¢ä½ é˜…è¯»æˆ‘çš„å¸–å­ï¼ğŸ¥°ï¼Œæˆ‘è·Ÿä½ åˆ†äº«æˆ‘æ‰€æœ‰çš„ä½œå“:è¿™ä¸ª<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank"> Kaggleç¬”è®°æœ¬</a>ï¼Œè¿˜æœ‰æˆ‘çš„å…¬<a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75" rel="noopener ugc nofollow" target="_blank"> W &amp; Bä»ªè¡¨ç›˜</a>ã€‚</p><p id="0706" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">æˆ‘å¸Œæœ›è¿™æ˜¯æ˜ç¡®çš„ï¼Œå¹¶éšæ—¶é—®æˆ‘é—®é¢˜ã€‚</p><p id="107e" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">ğŸ“¬æˆ‘çš„é‚®ä»¶åœ°å€æ˜¯:<strong class="kn ir">schopenhacker75@gmail.com</strong>ğŸ“¬</p><p id="ff92" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">åœ¨åé¢çš„ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ‰“ç®—è®¨è®ºå¦‚ä½•åœ¨ç”Ÿäº§ä¸­éƒ¨ç½²ä¸€ä¸ªè½¬æ¢å™¨ã€‚æˆ–è€…å¦‚ä½•ä¸ºNLPå˜å‹å™¨å»ºç«‹ä¸€ä¸ª<strong class="kn ir"> MLOpsç®¡é“ï¼Œ</strong>æˆ‘è¿˜æ²¡æœ‰å†³å®šâ€¦</p></div></div>    
</body>
</html>