<html>
<head>
<title>How To Build Your Own K-Means Algorithm Implementation in Python From Scratch With K-Means++ Initialization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¦‚ä½•ç”¨K-Means++åˆå§‹åŒ–ä»é›¶å¼€å§‹ç”¨Pythonæ„å»ºè‡ªå·±çš„K-Meansç®—æ³•å®ç°</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://pub.towardsai.net/how-to-build-your-own-k-means-algorithm-implementation-in-python-from-scratch-with-k-means-f652ed08ea31?source=collection_archive---------5-----------------------#2022-11-29">https://pub.towardsai.net/how-to-build-your-own-k-means-algorithm-implementation-in-python-from-scratch-with-k-means-f652ed08ea31?source=collection_archive---------5-----------------------#2022-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7cbf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">æœ‰ä»€ä¹ˆæ¯”è‡ªå·±ä»0å¼€å§‹å®ç°æ›´å¥½çš„åŠ æ·±å¯¹ç®—æ³•åŸç†çŸ¥è¯†çš„æ–¹æ³•ï¼Ÿ</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2755fb9ccfa0d8a6edf853ebf7a28337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQ8rwmJy7wtyOZl8m-toSA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">æ¢…å°”Â·æ™®å°”åœ¨<a class="ae kv" href="https://unsplash.com/s/photos/clusters?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šçš„ç…§ç‰‡</figcaption></figure><h2 id="3aa0" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">ä»€ä¹ˆæ˜¯K-Meansï¼Ÿ</strong></h2><p id="b744" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">K-Meansæ˜¯ä¸€ç§<strong class="lu ir">æ— ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯</strong> <em class="ml"> </em>ï¼Œç”¨äºå°†å¤šä¸ª<em class="ml">ã€nã€‘ä¸ªè§‚å¯Ÿå€¼</em>æ‹†åˆ†æˆ<em class="ml">ã€kã€‘ä¸ªä¸åŒçš„èšç±»</em>ï¼Œå…¶ä¸­æ¯ä¸ªè§‚å¯Ÿå€¼å±äºè´¨å¿ƒæœ€è¿‘çš„èšç±»ã€‚ç»“æœå°†æ˜¯æ•°æ®é›†è¢«åˆ†å‰²æˆ<em class="ml"> Voronoiå•å…ƒ</em>ã€‚</p><p id="5a2e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªè¦ç´ çš„æ•°æ®é›†ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b06254ec4106e0d4d4eacd8fa7a52859.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*L8qEHPIlEOMjvYA1JxI1iw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">å›¾ä¸€ã€‚éšæœºç”Ÿæˆçš„äºŒç»´æ— æ ‡ç­¾æ•°æ®é›†ã€‚ä½œè€…æ’å›¾ã€‚</figcaption></figure><p id="c23f" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">æ­£å¦‚æ‚¨åœ¨<em class="ml">å›¾1 </em>ä¸­æ‰€æ³¨æ„åˆ°çš„ï¼Œè‚‰çœ¼å¾ˆå®¹æ˜“å°±å¯ä»¥çœ‹å‡ºè¿™ä¸ªæ•°æ®é›†å¯ä»¥è¢«åˆ’åˆ†æˆå…­ä¸ªä¸åŒçš„é›†ç¾¤<em class="ml">(æˆ–ç»„)</em>ã€‚ä½†æ˜¯ç®—æ³•å¦‚ä½•ç¡®å®šå“ªä¸ªè§‚æµ‹å€¼å±äºå“ªä¸ªèšç±»å‘¢ï¼Ÿ</p><p id="90bc" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">ä¸ºäº†ç»™æ¯ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªç‰¹å®šçš„ç»„ï¼ŒK-Meansç®—æ³•éµå¾ªä»¥ä¸‹æ­¥éª¤:</p><ol class=""><li id="1103" class="ms mt iq lu b lv mm ly mn lf mu lj mv ln mw mk mx my mz na bi translated">åˆå§‹åŒ–â€œkâ€ä¸ªè´¨å¿ƒï¼Œæ¯ä¸ªèšç±»ä¸€ä¸ªã€‚</li><li id="9436" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">æ ¹æ®æœ€è¿‘çš„è´¨å¿ƒä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªèšç±»ã€‚</li><li id="2da0" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">åŸºäºæŒ‡å®šçš„ç‚¹é‡æ–°è®¡ç®—èšç±»ä¸­å¿ƒã€‚</li><li id="3ea9" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">é‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°è´¨å¿ƒä¸å†æ”¹å˜ã€‚</li></ol><p id="37c5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">ç§ï¼ŒK-Meanså°±æ˜¯è¿™æ ·å¾—å‡ºæœ€ç»ˆç»“æœçš„ã€‚</p><h2 id="001d" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">k-æ„å‘³ç€ç”¨ä¾‹</h2><p id="5888" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">è¿™ç§èšç±»ç®—æ³•çš„ä¸€äº›æœ€å¸¸è§çš„ç”¨ä¾‹åŒ…æ‹¬è¯¸å¦‚<strong class="lu ir"> <em class="ml">æœç´¢å¼•æ“ã€å¼‚å¸¸æ£€æµ‹</em> </strong>å’Œ<strong class="lu ir"> <em class="ml">åŸºäºå…ˆå‰è¡Œä¸º(å…´è¶£ã€è´­ä¹°ç­‰)çš„å®¢æˆ·åˆ†å‰²</em> </strong>ç­‰ä¸»é¢˜ã€‚)</p><h2 id="c3a0" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">è´¨å¿ƒçš„åˆå§‹åŒ–æ–¹æ³•</strong></h2><p id="078f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">æˆ‘ä»¬åº”è¯¥è€ƒè™‘çš„ä¸€ä»¶é‡è¦çš„äº‹æƒ…æ˜¯<strong class="lu ir">æœ€ç»ˆçš„ç»“æœå°†å–å†³äº</strong>å¯¹<strong class="lu ir">èšç±»ä¸­å¿ƒçš„åˆå§‹åŒ–</strong>æ–¹æ³•<strong class="lu ir">ã€‚æœ€å¸¸ç”¨çš„ä¸¤ç§åˆå§‹åŒ–æ–¹æ³•æ˜¯<em class="ml">â€˜randomâ€™</em>å’Œ<em class="ml">â€˜k-means++â€™</em>ã€‚è¿™ä¸¤è€…ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯â€œk-means++â€è¯•å›¾æ¨åŠ¨è´¨å¿ƒå°½å¯èƒ½è¿œç¦»å½¼æ­¤ï¼Œè¿™æ„å‘³ç€<strong class="lu ir">å®ƒä¼šæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆ</strong>ã€‚</strong></p><p id="5744" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¯¹äºè¿™ä¸ªå®ç°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<em class="ml">â€˜k-means++â€™</em>ä½œä¸ºåˆå§‹åŒ–æ–¹æ³•ã€‚</p><h2 id="d061" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">å®ç°</strong></h2><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="e823" class="nl kx iq nh b be nm nn l no np">class myKMeans:<br/>    def __init__(self, n_clusters, iters):<br/>        """<br/>        KMeans Class constructor.<br/><br/>        Args:<br/>          n_clusters (int) : Number of clusters used for partitioning.<br/>          iters (int) : Number of iterations until the algorithm stops.<br/><br/>        """<br/>        self.n_clusters = n_clusters<br/>        self.iters = iters<br/>        <br/>    def kmeans_plus_plus(self, X, n_clusters):<br/>        pass<br/><br/>    def find_closest_centroids(self, X, centroids):<br/>        pass<br/><br/>    def compute_centroids(self, X, idx, K):<br/>        pass<br/><br/>    def fit_predict(self, X):<br/>        pass</span></pre><p id="ea46" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">è¿™æ˜¯æˆ‘ä»¬å°†è¦æ„å»ºçš„ç±»çš„ç»“æ„ã€‚<em class="ml"> 'kmeans_plus_plus()'ã€' find _ closed _ Centros()'</em>å’Œ<em class="ml">' compute _ Centros()'</em>æ–¹æ³•å°†åˆ†åˆ«æ‰§è¡Œç®—æ³•çš„ç¬¬ä¸€ã€ç¬¬äºŒå’Œç¬¬ä¸‰æ­¥ã€‚<em class="ml">â€˜æ‹Ÿåˆ_é¢„æµ‹()â€™</em>æ–¹æ³•å°†è·å–æ•°æ®é›†å¹¶åœ¨æ ‡ç­¾ä¸Šè¿›è¡Œé¢„æµ‹ã€‚</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="fc02" class="nl kx iq nh b be nm nn l no np">def kmeans_plus_plus(self, X, n_clusters):<br/>    """<br/>    My implementation of the KMeans++ initialization method for computing the centroids.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples<br/>        n_clusters (int): Number of clusters<br/><br/>    Returns:<br/>        centroids (ndarray): Initial position of centroids<br/>    """<br/>    # Assign the first centroid to a random sample from the dataset.<br/>    idx = random.randrange(len(X))<br/>    centroids = [X[idx]]<br/><br/>    # For each cluster<br/>    for _ in range(1, n_clusters):<br/><br/>        # Get the squared distance between that centroid and each sample in the dataset<br/>        squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])<br/><br/>        # Convert the distances into probabilities that a specific sample could be the center of a new centroid<br/>        proba = squared_distances / squared_distances.sum()<br/><br/>        for point, probability in enumerate(proba):<br/>            # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.<br/>            if probability == proba.max():<br/>                centroid = point<br/>                break<br/><br/>        centroids.append(X[centroid])<br/><br/>    return np.array(centroids)</span></pre><p id="b2d5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><em class="ml">ä»¥ä¸ŠåŠŸèƒ½å®ç°äº†K-Meansç®—æ³•ç¬¬ä¸€æ­¥<strong class="lu ir">(åˆå§‹åŒ–æ–¹æ³•)</strong>ã€‚</em></p><p id="b547" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">æˆ‘ä»¬ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸€ä¸ªæ ·æœ¬ï¼Œå¹¶å°†å…¶æŒ‡å®šä¸º<strong class="lu ir"> <em class="ml">ç¬¬ä¸€ä¸ªè´¨å¿ƒ</em> </strong>ã€‚ç„¶å<em class="ml">é‡å¤è®¡ç®—</em>æ¯ä¸ªæ ·æœ¬<em class="ml">ä¸æ‰€æœ‰è´¨å¿ƒ</em>ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶å°†å‰©ä½™çš„æ‰€æœ‰è´¨å¿ƒåˆ†é…åˆ°è·ç¦»å…ˆå‰è®¡ç®—çš„ä¸­å¿ƒæœ€è¿œçš„<em class="ml">ç‚¹</em>ã€‚</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="eb33" class="nl kx iq nh b be nm nn l no np">def find_closest_centroids(self, X, centroids):<br/>    """<br/>    Computes the distance to the centroids and assigns the new label to each sample in the dataset.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples  <br/>        centroids (ndarray): Number of clusters<br/><br/>    Returns:<br/>        idx (ndarray): Closest centroids for each observation<br/><br/>    """<br/><br/>    # Set K as number of centroids<br/>    K = centroids.shape[0]<br/><br/>    # Initialize the labels array to 0<br/>    label = np.zeros(X.shape[0], dtype=int)<br/><br/>    # For each sample in the dataset<br/>    for sample in range(len(X)):<br/>        distance = []<br/>        # Take every centroid<br/>        for centroid in range(len(centroids)):<br/>            # Compute Euclidean norm between a specific sample and a centroid<br/>            norm = np.linalg.norm(X[sample] - centroids[centroid])<br/>            distance.append(norm)<br/><br/>        # Assign the closest centroid as it's label<br/>        label[sample] = distance.index(min(distance))<br/><br/>    return label</span></pre><p id="3187" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><em class="ml">ä¸Šçš„å‡½æ•°å®ç°äº†K-Meansç®—æ³•çš„ç¬¬äºŒæ­¥</em>(æ±‚æœ€è¿‘è´¨å¿ƒ)ã€‚</p><p id="03d9" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¯¹äºæ•°æ®é›†ä¸­çš„<em class="ml">æ¯ä¸ªæ ·æœ¬</em>ï¼Œæˆ‘ä»¬å–<em class="ml">æ¯ä¸ªè´¨å¿ƒ</em>å’Œ<strong class="lu ir">è®¡ç®—å®ƒä»¬ä¹‹é—´çš„æ¬§å‡ é‡Œå¾·èŒƒæ•°</strong>ã€‚æˆ‘ä»¬å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†è§‚å¯Ÿç»“æœåˆ†é…ç»™<em class="ml">æœ€æ¥è¿‘çš„è´¨å¿ƒ</em>ã€‚</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="f7e2" class="nl kx iq nh b be nm nn l no np">def compute_centroids(self, X, idx, K):<br/>    """<br/>    Returns the new centroids by computing the mean of the data points assigned to each centroid.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples <br/>        idx (ndarray): Closest centroids for each observation <br/>        K (int): Number of clusters<br/><br/>    Returns:<br/>        centroids (ndarray): New centroids computed<br/>    """<br/><br/>    # Number of samples and features<br/>    m, n = X.shape<br/><br/>    # Initialize centroids to 0<br/>    centroids = np.zeros((K, n))<br/><br/>    # For each centroid<br/>    for k in range(K):   <br/>        # Take all samples assigned to that specific centroid<br/>        points = X[idx == k]<br/>        # Compute their mean<br/>        centroids[k] = np.mean(points, axis=0)<br/><br/>    return centroids</span></pre><p id="82fe" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">ä¸Šé¢çš„<em class="ml">åŠŸèƒ½å®ç°äº†K-Meansç®—æ³•çš„</em>ç¬¬ä¸‰æ­¥<strong class="lu ir">(é‡æ–°è®¡ç®—æ–°çš„èšç±»ä¸­å¿ƒ)</strong>ã€‚</p><p id="352c" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¯¹äº<em class="ml">çš„æ¯ä¸ªè´¨å¿ƒ</em>ï¼Œæˆ‘ä»¬å–<em class="ml">æ‰€æœ‰åˆ†é…ç»™è¯¥<em class="ml">ç‰¹å®šç»„çš„ç‚¹</em>å’Œ<strong class="lu ir">è®¡ç®—å…¶å¹³å‡å€¼</strong>ã€‚ç»“æœä¼šç»™æˆ‘ä»¬<em class="ml">æ–°çš„èšç±»ä¸­å¿ƒ</em>ã€‚</em></p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="9303" class="nl kx iq nh b be nm nn l no np">def fit_predict(self, X):<br/>    """<br/>    My implementation of the KMeans algorithm.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples<br/><br/>    Returns:<br/>        centroids (ndarray):  Computed centroids<br/>        labels (ndarray):     Predicts for each sample in the dataset.<br/>    """<br/>    # Number of samples and features<br/>    m, n = X.shape<br/><br/>    # Compute initial position of the centroids<br/>    initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)<br/><br/>    centroids = initial_centroids   <br/>    labels = np.zeros(m)<br/><br/>    prev_centroids = centroids<br/><br/>    # Run K-Means<br/>    for i in range(self.iters):<br/>        # For each example in X, assign it to the closest centroid<br/>        labels = self.find_closest_centroids(X, centroids)<br/><br/>        # Given the memberships, compute new centroids<br/>        centroids = self.compute_centroids(X, labels, self.n_clusters)<br/>        <br/>        # Check if centroids stopped changing positions<br/>        if centroids.tolist() == prev_centroids.tolist():<br/>            print(f'K-Means converged at {i+1} iterations')<br/>            break<br/>        else:<br/>            prev_centroids = centroids<br/><br/>    return centroids, labels</span></pre><p id="1629" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œå°†è°ƒç”¨<em class="ml">' fit _ project()'</em>å‡½æ•°å¯¹æ•°æ®é›†ä¸­çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ã€‚</p><p id="ebb0" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">æœ€å<em class="ml">ï¼Œ</em>æ‚¨çš„<em class="ml"> K-Meansç±»</em>åº”è¯¥æ˜¯è¿™æ ·çš„<strong class="lu ir"/>:</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="3fdd" class="nl kx iq nh b be nm nn l no np">class myKMeans:<br/>    def __init__(self, n_clusters, iters):<br/>        """<br/>        KMeans Class constructor.<br/>  <br/>        Args:<br/>          n_clusters (int) : Number of clusters used for partitioning.<br/>          iters (int) : Number of iterations until the algorithm stops.<br/>  <br/>        """<br/>        self.n_clusters = n_clusters<br/>        self.iters = iters<br/>        <br/>    def kmeans_plus_plus(self, X, n_clusters):<br/>        """<br/>        My implementation of the KMeans++ initialization method for computing the centroids.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples<br/>            n_clusters (int): Number of clusters<br/>  <br/>        Returns:<br/>            centroids (ndarray): Initial position of centroids<br/>        """<br/>        # Assign the first centroid to a random sample from the dataset.<br/>        idx = random.randrange(len(X))<br/>        centroids = [X[idx]]<br/>  <br/>        # For each cluster<br/>        for _ in range(1, n_clusters):<br/>  <br/>            # Get the squared distance between that centroid and each sample in the dataset<br/>            squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])<br/>  <br/>            # Convert the distances into probabilities that a specific sample could be the center of a new centroid<br/>            proba = squared_distances / squared_distances.sum()<br/>  <br/>            for point, probability in enumerate(proba):<br/>                # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.<br/>                if probability == proba.max():<br/>                    centroid = point<br/>                    break<br/>  <br/>            centroids.append(X[centroid])<br/>  <br/>        return np.array(centroids)<br/>  <br/>    def find_closest_centroids(self, X, centroids):<br/>        """<br/>        Computes the distance to the centroids and assigns the new label to each sample in the dataset.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples  <br/>            centroids (ndarray): Number of clusters<br/>  <br/>        Returns:<br/>            idx (ndarray): Closest centroids for each observation<br/>  <br/>        """<br/>  <br/>        # Set K as number of centroids<br/>        K = centroids.shape[0]<br/>  <br/>        # Initialize the labels array to 0<br/>        label = np.zeros(X.shape[0], dtype=int)<br/>  <br/>        # For each sample in the dataset<br/>        for sample in range(len(X)):<br/>            distance = []<br/>            # Take every centroid<br/>            for centroid in range(len(centroids)):<br/>                # Compute Euclidean norm between a specific sample and a centroid<br/>                norm = np.linalg.norm(X[sample] - centroids[centroid])<br/>                distance.append(norm)<br/>  <br/>            # Assign the closest centroid as it's label<br/>            label[sample] = distance.index(min(distance))<br/>  <br/>        return label<br/>  <br/>    def compute_centroids(self, X, idx, K):<br/>        """<br/>        Returns the new centroids by computing the mean of the data points assigned to each centroid.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples <br/>            idx (ndarray): Closest centroids for each observation <br/>            K (int): Number of clusters<br/>  <br/>        Returns:<br/>            centroids (ndarray): New centroids computed<br/>        """<br/>  <br/>        # Number of samples and features<br/>        m, n = X.shape<br/>  <br/>        # Initialize centroids to 0<br/>        centroids = np.zeros((K, n))<br/>  <br/>        # For each centroid<br/>        for k in range(K):   <br/>            # Take all samples assigned to that specific centroid<br/>            points = X[idx == k]<br/>            # Compute their mean<br/>            centroids[k] = np.mean(points, axis=0)<br/>  <br/>        return centroids<br/>  <br/>    def fit_predict(self, X):<br/>        """<br/>        My implementation of the KMeans algorithm.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples<br/>  <br/>        Returns:<br/>            centroids (ndarray):  Computed centroids<br/>            labels (ndarray):     Predicts for each sample in the dataset.<br/>        """<br/>        # Number of samples and features<br/>        m, n = X.shape<br/>  <br/>        # Compute initial position of the centroids<br/>        initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)<br/>  <br/>        centroids = initial_centroids   <br/>        labels = np.zeros(m)<br/>        <br/>        prev_centroids = centroids<br/>  <br/>        # Run K-Means<br/>        for i in range(self.iters):<br/>            # For each example in X, assign it to the closest centroid<br/>            labels = self.find_closest_centroids(X, centroids)<br/>  <br/>            # Given the memberships, compute new centroids<br/>            centroids = self.compute_centroids(X, labels, self.n_clusters)<br/>            <br/>            # Check if centroids stopped changing positions<br/>            if centroids.tolist() == prev_centroids.tolist():<br/>                print(f'K-Means converged at {i+1} iterations')<br/>                break<br/>            else:<br/>                prev_centroids = centroids<br/>  <br/>        return labels, centroids</span></pre><p id="2cee" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">é…·ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹<strong class="lu ir">æˆ‘ä»¬çš„å®ç°çš„ç»“æœ</strong>ä¸ç›¸æ¯”çœ‹èµ·æ¥æ˜¯æ€æ ·çš„(ç›¸æ¯”äºK-Meansçš„<em class="ml"> sklearn </em>ç‰ˆæœ¬):</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="a7d4" class="nl kx iq nh b be nm nn l no np">import random<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans</span></pre><p id="2f58" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å¯¹å…¶æ‰§è¡Œèšç±»ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†ä½¿ç”¨<em class="ml"> 'make_blobs()' </em>å‡½æ•°ä»<em class="ml">sklearn . dataset .</em>ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="a587" class="nl kx iq nh b be nm nn l no np">from sklearn.datasets import make_blobs<br/><br/># Generate 2D classification dataset<br/>X, y = make_blobs(n_samples=1500, centers=6, n_features=2, random_state=67)</span></pre><p id="a2f9" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">ä¸Šé¢çš„ä»£ç ç‰‡æ®µå°†ä¸ºæˆ‘ä»¬ç”Ÿæˆä»¥ä¸‹æ•°æ®é›†:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b64b594c23ac5f797ad6497d027f0799.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*fyLT1fCoA_pqZWsa0VLjuQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">å›¾2ã€‚éšæœºç”Ÿæˆçš„äºŒç»´æ ‡è®°æ•°æ®é›†ã€‚ä½œè€…çš„æ’å›¾ã€‚</figcaption></figure><p id="7fd0" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿è¡ŒK-Meansçš„ä¸¤ä¸ªç‰ˆæœ¬(<em class="ml">æ‹¥æœ‰</em>å’Œ<em class="ml"> sklearn </em>å®ç°)å¹¶çœ‹çœ‹å®ƒä»¬æ˜¯å¦‚ä½•è¿è¡Œçš„ã€‚</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="14de" class="nl kx iq nh b be nm nn l no np"># sklearn version of KMeans<br/>kmeans = KMeans(n_clusters=5)<br/>sklearn_labels = kmeans.fit_predict(X)<br/>sklearn_centers = kmeans.cluster_centers_<br/><br/># own implementation of KMeans<br/>my_kmeans = myKMeans(5, 50)<br/>mykmeans_labels, mykmeans_centers = my_kmeans.fit_predict(X)</span></pre><p id="3eb1" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¾ˆå¥½ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨è®ºï¼Œè®©æˆ‘ä»¬æŠŠå®ƒä»¬å’ŒVoronoiç»†èƒä¸€èµ·å¯è§†åŒ–ã€‚ğŸ˜</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="b95d" class="nl kx iq nh b be nm nn l no np">plt.figure(figsize=(12,4)) <br/>vor = Voronoi(sklearn_centers)<br/>fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 1))<br/>plt.subplot(1, 2, 1)<br/>plt.title("sklearn KMeans Predicts")<br/>plt.xlabel("Feature 1")<br/>plt.ylabel("Feature 2")<br/>plt.xlim([-13, 11])<br/>plt.ylim([-14, 13])<br/>plt.scatter(X[:, 0], X[:, 1], 4, c=sklearn_labels) <br/>plt.scatter(sklearn_centers[:, 0], sklearn_centers[:, 1], marker='x', c='red', s=50)<br/>vor = Voronoi(mykmeans_centers)<br/>fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 2))<br/>plt.subplot(1, 2, 2)<br/>plt.title("My KMeans Predicts")<br/>plt.xlabel("Feature 1")<br/>plt.ylabel("Feature 2")<br/>plt.xlim([-13, 11])<br/>plt.ylim([-14, 13])<br/>plt.scatter(X[:, 0], X[:, 1], 4, c=mykmeans_labels) <br/>plt.scatter(mykmeans_centers[:, 0], mykmeans_centers[:, 1], marker='x', c='red', s=50)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ee45e902cb2278b1d52220aaa346c282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CT9zwS0m5I67RO0v2blRFg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">å›¾3ã€‚æˆ‘ä»¬ä»å¤´å¼€å§‹å®ç°K-Meanså’Œsklearnç‰ˆæœ¬çš„æ¯”è¾ƒã€‚ä½œè€…æ’å›¾ã€‚</figcaption></figure><p id="3b5e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å“‡å“¦ã€‚å¦‚æœä½ é—®æˆ‘ï¼Œé‚£çœ‹èµ·æ¥çœŸçš„ä»¤äººå°è±¡æ·±åˆ» ã€‚ç»“æœåŸºæœ¬ç›¸åŒã€‚</p><h2 id="db78" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">ç»“è®º</h2><p id="dd09" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">æ€»ä¹‹ï¼Œè¿™æˆ–å¤šæˆ–å°‘æ˜¯æ‚¨éœ€è¦äº†è§£çš„å…³äºè¿™ä¸ªå¼ºå¤§çš„èšç±»ç®—æ³•çš„ä¸€åˆ‡ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ äº†è§£K-MeansåŸåˆ™ã€‚æ„Ÿè°¢é˜…è¯»ï¼</p><p id="0c23" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¦‚æœä½ å¯¹è¿™ç¯‡æ–‡ç« æœ‰ä»»ä½•æ„è§ï¼Œè¯·å†™åœ¨è¯„è®ºé‡Œï¼æˆ‘å¾ˆæƒ³è¯»è¯»å®ƒä»¬ğŸ˜‹</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="f1ab" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">å…³äºæˆ‘</h2><p id="08f5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">å¤§å®¶å¥½ï¼Œæˆ‘å«Alexï¼Œæ˜¯ä¸€åå¹´è½»çƒ­æƒ…çš„æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦å­¦ç”Ÿã€‚</p><p id="4a51" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">å¦‚æœä½ å–œæ¬¢çš„å†…å®¹ï¼Œè¯·è€ƒè™‘ä¸‹é™å…³æ³¨å’Œé¼“æŒï¼Œå› ä¸ºä»–ä»¬çœŸçš„å¾ˆæ„Ÿæ¿€ã€‚æ­¤å¤–ï¼Œè¯·éšæ—¶åœ¨<a class="ae kv" href="https://www.linkedin.com/in/alexandru-florin-belengeanu-74b1a3128/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>ä¸Šä¸æˆ‘è”ç³»ï¼Œä»¥ä¾¿è·å¾—ä¸€äº›å…³äºæœºå™¨å­¦ä¹ ç›¸å…³ä¸»é¢˜çš„æ¯å‘¨è§è§£ã€‚</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="8801" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">å‚è€ƒ</h2><p id="30be" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">[1]å¤§å«Â·äºšç‘Ÿå’Œè°¢å°”ç›–Â·ç“¦è¥¿é‡Œç»´èŒ¨åŸºï¼Œ<a class="ae kv" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank"> k-means++ç²¾å¿ƒæ’­ç§çš„ä¼˜åŠ¿</a> (2007)ï¼Œ<a class="ae kv" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a></p><p id="5558" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">[2]<a class="ae kv" href="https://en.wikipedia.org/wiki/K-means_clustering#Applications" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/K-means _ clustering #åº”ç”¨</a></p><p id="7d03" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">[3]<a class="ae kv" href="https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnuggets . com/2020/06/centroid-initial ization-k-means-clustering . html</a></p></div></div>    
</body>
</html>