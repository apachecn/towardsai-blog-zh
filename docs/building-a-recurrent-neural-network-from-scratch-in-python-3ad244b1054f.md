# ç”¨ Python ä»å¤´å¼€å§‹æ„å»ºé€’å½’ç¥ç»ç½‘ç»œ

> åŸæ–‡ï¼š<https://pub.towardsai.net/building-a-recurrent-neural-network-from-scratch-in-python-3ad244b1054f?source=collection_archive---------1----------------------->

## å¦‚ä½•ä½¿ç”¨åŸºæœ¬çš„ Python åº“æ„å»ºåŸºæœ¬çš„ RNN

é€šç”¨ç¥ç»ç½‘ç»œ(RNN)æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶ä»–åºåˆ—å»ºæ¨¡ä»»åŠ¡çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬æœ‰ä¸€ä¸ªæ‰€è°“çš„è®°å¿†ç»†èƒã€‚ä»–ä»¬å¯ä»¥ä¸€æ¬¡è¯»å–ä¸€ä¸ªè¾“å…¥ğ‘¥âŸ¨ğ‘¡âŸ©(å¦‚å•è¯)ï¼Œå¹¶é€šè¿‡ä»ä¸€ä¸ªæ­¥éª¤ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ­¥éª¤çš„éšè—å±‚æ¿€æ´»æ¥è®°ä½ä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™å…è®¸å•å‘(å•å‘)RNN ä»è¿‡å»è·å–ä¿¡æ¯æ¥å¤„ç†åæ¥çš„è¾“å…¥ã€‚åŒå‘(åŒå‘)RNN å¯ä»¥ä»è¿‡å»å’Œæœªæ¥è·å–ä¸Šä¸‹æ–‡ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Python ç¼–ç¨‹è¯­è¨€çš„åŸºæœ¬å‡½æ•°å’Œåº“ä»å¤´å¼€å§‹å®ç° RNN æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆå°†å¼€å§‹æ„å»º RNN çš„å‰å‘ä¼ æ’­ï¼Œç„¶åæ˜¯ RNN çš„åå‘ä¼ æ’­ï¼Œå¹¶å°†å®ƒä»¬ç»“åˆåœ¨ä¸€èµ·ä»¥å¾—åˆ°ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„ RNN æ¨¡å‹ã€‚

![](img/35108f6bcb1eb70d6d05e32108cdfce1.png)

ç…§ç‰‡ç”± [DeepMind](https://unsplash.com/@deepmind?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

## ç›®å½•:

1.  **é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­
    1.1ã€‚RNN ç»†èƒ
    1.2ã€‚RNN å‘å‰ä¼ çƒ**
2.  **é€’å½’ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­
    2.1ã€‚RNN å‘åå•å…ƒæ ¼
    2.2ã€‚RNN å‘åä¼ çƒ**
3.  **å‚è€ƒ**

**å¦‚æœä½ æƒ³å…è´¹å­¦ä¹ æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ï¼Œçœ‹çœ‹è¿™äº›èµ„æº:**

*   å…è´¹äº’åŠ¨è·¯çº¿å›¾ï¼Œè‡ªå­¦æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚ä»è¿™é‡Œå¼€å§‹:[https://aigents.co/learn/roadmaps/intro](https://aigents.co/learn/roadmaps/intro)
*   æ•°æ®ç§‘å­¦å­¦ä¹ èµ„æºæœç´¢å¼•æ“(å…è´¹)ã€‚å°†ä½ æœ€å–œæ¬¢çš„èµ„æºåŠ å…¥ä¹¦ç­¾ï¼Œå°†æ–‡ç« æ ‡è®°ä¸ºå®Œæ•´ï¼Œå¹¶æ·»åŠ å­¦ä¹ ç¬”è®°ã€‚[https://aigents.co/learn](https://aigents.co/learn)
*   æƒ³è¦åœ¨å¯¼å¸ˆå’Œå­¦ä¹ ç¤¾åŒºçš„æ”¯æŒä¸‹ä»å¤´å¼€å§‹å­¦ä¹ æ•°æ®ç§‘å­¦å—ï¼Ÿå…è´¹åŠ å…¥è¿™ä¸ªå­¦ä¹ åœˆ:[https://community.aigents.co/spaces/9010170/](https://community.aigents.co/spaces/9010170/)

å¦‚æœä½ æƒ³åœ¨æ•°æ®ç§‘å­¦&äººå·¥æ™ºèƒ½é¢†åŸŸå¼€å§‹èŒä¸šç”Ÿæ¶¯ï¼Œä½†ä¸çŸ¥é“å¦‚ä½•å¼€å§‹ã€‚æˆ‘æä¾›æ•°æ®ç§‘å­¦æŒ‡å¯¼è¯¾ç¨‹å’Œé•¿æœŸèŒä¸šæŒ‡å¯¼:

*   é•¿æœŸæŒ‡å¯¼:[https://lnkd.in/dtdUYBrM](https://lnkd.in/dtdUYBrM)
*   è¾…å¯¼ä¼šè®®:[https://lnkd.in/dXeg3KPW](https://lnkd.in/dXeg3KPW)

***åŠ å…¥*** [***ä¸­ç­‰ä¼šå‘˜***](https://youssefraafat57.medium.com/membership) ***è®¡åˆ’ç»§ç»­æ— é™åˆ¶å­¦ä¹ ã€‚å¦‚æœä½ ä½¿ç”¨ä¸‹é¢çš„é“¾æ¥ï¼Œæˆ‘ä¼šæ”¶åˆ°ä¸€å°éƒ¨åˆ†ä¼šå‘˜è´¹ï¼Œä¸éœ€è¦ä½ é¢å¤–ä»˜è´¹ã€‚***

[](https://youssefraafat57.medium.com/membership) [## åŠ å…¥æˆ‘çš„ä»‹ç»é“¾æ¥åª’ä½“-ä¼˜ç´ ç¦èƒ¡æ–¯å°¼

### é˜…è¯» Youssef Hosni(ä»¥åŠåª’ä½“ä¸Šæˆåƒä¸Šä¸‡çš„å…¶ä»–ä½œå®¶)çš„æ¯ä¸€ä¸ªæ•…äº‹ã€‚æ‚¨çš„ä¼šå‘˜è´¹ç›´æ¥æ”¯æŒâ€¦

youssefraafat57.medium.com](https://youssefraafat57.medium.com/membership) 

# 1.é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­

æˆ‘ä»¬å°†ä» RNN çš„æ­£å‘ä¼ æ’­å¼€å§‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°å¦‚å›¾ 1 æ‰€ç¤ºçš„åŸºæœ¬ RNN æ¨¡å‹ã€‚

**è¾“å…¥ğ‘¥:çš„å°ºå¯¸**

ç”¨ğ‘›ğ‘¥å•ä½æ•°è¾“å…¥

*   å¯¹äºå•ä¸ªè¾“å…¥ç¤ºä¾‹çš„å•ä¸ªæ—¶é—´æ­¥é•¿ï¼Œğ‘¥(ğ‘–)âŸ¨ğ‘¡âŸ©æ˜¯ä¸€ç»´è¾“å…¥å‘é‡ã€‚
*   ä»¥è¯­è¨€ä¸ºä¾‹ï¼Œä¸€ç§æœ‰ 5000 ä¸ªå•è¯çš„è¯­è¨€å¯ä»¥ä¸€æ¬¡æ€§ç¼–ç æˆä¸€ä¸ªæœ‰ 5000 ä¸ªå•ä½çš„å‘é‡ã€‚æ‰€ä»¥ğ‘¥(ğ‘–)âŸ¨ğ‘¡âŸ©would æœ‰å½¢çŠ¶(5000ï¼Œ)ã€‚
*   è¿™é‡Œä½¿ç”¨ç¬¦å·ğ‘›ğ‘¥æ¥è¡¨ç¤ºå•ä¸ªè®­ç»ƒç¤ºä¾‹çš„å•ä¸ªæ—¶é—´æ­¥é•¿ä¸­çš„å•ä½æ•°ã€‚

**ğ‘‡ğ‘¥å°ºå¯¸çš„æ—¶é—´æ­¥é•¿**

*   ä¸€ä¸ªé€’å½’ç¥ç»ç½‘ç»œæœ‰å¤šä¸ªæ—¶é—´æ­¥é•¿ï¼Œä½ å¯ä»¥ç”¨ğ‘¡.æ¥ç´¢å¼•

**æ‰¹é‡ğ‘š**

*   å‡è®¾æˆ‘ä»¬æœ‰å°æ‰¹é‡ï¼Œæ¯ä¸ªå°æ‰¹é‡æœ‰ 20 ä¸ªè®­ç»ƒæ ·æœ¬
*   ä¸ºäº†ä»çŸ¢é‡åŒ–ä¸­å—ç›Šï¼Œæ‚¨å°†å †å  20 åˆ—ğ‘¥(ğ‘–)ç¤ºä¾‹
*   ä¾‹å¦‚ï¼Œè¿™ä¸ªå¼ é‡çš„å½¢çŠ¶æ˜¯(5000ï¼Œ20ï¼Œ10)
*   æ‚¨å°†ä½¿ç”¨ğ‘šm æ¥è¡¨ç¤ºè®­ç»ƒæ ·æœ¬çš„æ•°é‡
*   S9oï¼Œå°æ‰¹é‡çš„å½¢çŠ¶æ˜¯(ğ‘›ğ‘¥ï¼Œğ‘šï¼Œğ‘‡ğ‘¥)

**ä¸‰ç»´å½¢çŠ¶å¼ é‡(ğ‘›ğ‘¥,ğ‘š,ğ‘‡ğ‘¥)**

*   å½¢çŠ¶çš„ä¸‰ç»´å¼ é‡ğ‘¥x(ğ‘›ğ‘¥,ğ‘š,ğ‘‡ğ‘¥)è¡¨ç¤ºè¢«é¦ˆå…¥ RNN çš„è¾“å…¥ğ‘¥ã€‚

ä¸ºæ¯ä¸ªæ—¶é—´æ­¥é•¿å–ä¸€ä¸ª 2D åˆ‡ç‰‡:ğ‘¥âŸ¨ğ‘¡âŸ©

*   åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ï¼Œæ‚¨å°†ä½¿ç”¨ä¸€ä¸ªå°æ‰¹é‡çš„è®­ç»ƒç¤ºä¾‹(ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç¤ºä¾‹)
*   å› æ­¤ï¼Œå¯¹äºæ¯ä¸€ä¸ªæ—¶é—´æ­¥ğ‘¡ï¼Œä½ å°†ä½¿ç”¨ä¸€ä¸ªäºŒç»´åˆ‡ç‰‡çš„å½¢çŠ¶(ğ‘›ğ‘¥,ğ‘š)
*   è¿™ä¸ª 2D åˆ‡ç‰‡è¢«ç§°ä¸ºğ‘¥âŸ¨ğ‘¡âŸ©.

**éšè—çŠ¶æ€ğ‘çš„å®šä¹‰**

*   ä»ä¸€ä¸ªæ—¶é—´æ­¥éª¤åˆ°å¦ä¸€ä¸ªæ—¶é—´æ­¥éª¤ä¼ é€’ç»™ RNN çš„æ¿€æ´»ğ‘âŸ¨ğ‘¡âŸ©aâŸ¨tâŸ©è¢«ç§°ä¸ºâ€œéšè—çŠ¶æ€â€

**éšè—çŠ¶æ€ğ‘çš„ç»´åº¦**

*   ç±»ä¼¼äºè¾“å…¥å¼ é‡ğ‘¥xï¼Œå•ä¸ªè®­ç»ƒç¤ºä¾‹çš„éšè—çŠ¶æ€æ˜¯é•¿åº¦ä¸ºğ‘›ğ‘çš„å‘é‡
*   å¦‚æœåŒ…æ‹¬å°æ‰¹é‡çš„ğ‘šm åŸ¹è®­ç¤ºä¾‹ï¼Œå°æ‰¹é‡çš„å½¢çŠ¶ä¸º(ğ‘›ğ‘,ğ‘š)
*   å½“åŒ…æ‹¬æ—¶é—´æ­¥é•¿ç»´åº¦æ—¶ï¼Œéšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º(ğ‘›ğ‘,ğ‘š,ğ‘‡ğ‘¥)
*   æ‚¨å°†ä½¿ç”¨ç´¢å¼•ğ‘¡t å¾ªç¯æ—¶é—´æ­¥é•¿ï¼Œå¹¶ä½¿ç”¨ 3D å¼ é‡çš„ 2D åˆ‡ç‰‡
*   è¿™ä¸ª 2D åˆ‡ç‰‡è¢«ç§°ä¸ºğ‘âŸ¨ğ‘¡âŸ©aâŸ¨tâŸ©
*   è¿™ä¸ªäºŒç»´åˆ‡ç‰‡çš„å½¢çŠ¶æ˜¯(ğ‘›ğ‘,ğ‘š)

**é¢„æµ‹ğ‘¦Ì‚çš„ç»´åº¦**

*   ç±»ä¼¼äºè¾“å…¥å’Œéšè—çŠ¶æ€ï¼Œğ‘¦Ì‚æ˜¯ä¸€ä¸ªä¸‰ç»´å½¢çŠ¶å¼ é‡(ğ‘›ğ‘¦,ğ‘š,ğ‘‡ğ‘¦)
*   ä»£è¡¨é¢„æµ‹çš„å‘é‡ä¸­çš„ğ‘›ğ‘¦:å•ä½æ•°
*   å°æ‰¹é‡æ ·å“çš„ğ‘š:æ•°é‡
*   é¢„æµ‹ä¸­çš„ğ‘‡ğ‘¦:æ—¶é—´æ­¥æ•°

å¯¹äºå•ä¸ªæ—¶é—´æ­¥é•¿ğ‘¡ï¼Œ2D åˆ‡ç‰‡ğ‘¦Ì‚ âŸ¨ğ‘¡âŸ©å·²ç»æˆå½¢(ğ‘›ğ‘¦,ğ‘š)

**ä¸‹é¢æ˜¯ä½ å¦‚ä½•å®ç°ä¸€ä¸ª RNN:**

*   å®ç° RNN çš„ä¸€æ¬¡æ€§æ­¥éª¤æ‰€éœ€çš„è®¡ç®—ã€‚
*   åœ¨ğ‘‡ğ‘¥Tx æ—¶é—´æ­¥é•¿ä¸Šæ‰§è¡Œå¾ªç¯ï¼Œä»¥ä¾¿ä¸€æ¬¡å¤„ç†ä¸€ä¸ªè¾“å…¥ã€‚

![](img/7740a4e6bf4c76c9b00c566c643ae3bf.png)

å›¾ä¸€ã€‚åŸºæœ¬ RNN æ¨¡å‹

## 1.1.é€’å½’ç¥ç»ç½‘ç»œç»†èƒ

æˆ‘ä»¬å¯ä»¥æŠŠå›¾ 1 æ‰€ç¤ºçš„ RNN æ¨¡å‹çœ‹ä½œæ˜¯å›¾ 2 æ‰€ç¤ºçš„å•ä¸ªç»†èƒçš„é‡å¤ä½¿ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå•ä¸ªå•å…ƒï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¾ªç¯éå†å®ƒï¼Œå°†å¤šä¸ªå•ä¸ªå•å…ƒç›¸äº’å †å èµ·æ¥ï¼Œå¹¶åˆ›å»º RNN æ¨¡å‹çš„æ­£å‘ä¼ é€’ã€‚

![](img/3ec430f4ed60a789e1d672171d6f0cb0.png)

å›¾äºŒã€‚åŸºæœ¬ RNN ç»†èƒã€‚

åŸºæœ¬ RNN å•å…ƒå°†ğ‘¥âŸ¨ğ‘¡âŸ©(å½“å‰è¾“å…¥)å’Œğ‘âŸ¨ğ‘¡âˆ’1âŸ©(åŒ…å«æ¥è‡ªè¿‡å»çš„ä¿¡æ¯çš„å…ˆå‰éšè—çŠ¶æ€)ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºğ‘âŸ¨ğ‘¡âŸ©aâŸ¨ï¼Œè¯¥è¢«æä¾›ç»™ä¸‹ä¸€ä¸ª RNN å•å…ƒï¼Œå¹¶ä¸”ä¹Ÿç”¨äºé¢„æµ‹ğ‘¦Ì‚ âŸ¨ğ‘¡âŸ©.

è®©æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹å››ä¸ªä¸»è¦æ­¥éª¤æ¥å®ç°å›¾ 2 æ‰€ç¤ºçš„ RNN å•å…ƒ:

1.  ç”¨ tanh æ¿€æ´»è®¡ç®—éšè—çŠ¶æ€:ğ‘âŸ¨ğ‘¡âŸ©=tanh(ğ‘ŠâŸ¨ğ‘ğ‘âŸ©*ğ‘âŸ¨ğ‘¡âˆ’1âŸ©+ğ‘ŠâŸ¨ğ‘ğ‘¥âŸ©*ğ‘¥âŸ¨ğ‘¡âŸ©+ğ‘âŸ¨ğ‘âŸ©)
2.  ä½¿ç”¨æ–°çš„éšè—çŠ¶æ€ğ‘âŸ¨ğ‘¡âŸ©ï¼Œè®¡ç®—é¢„æµ‹ğ‘¦Ì‚ âŸ¨ğ‘¡âŸ© = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ŠâŸ¨ğ‘¦ğ‘âŸ©*ğ‘âŸ¨ğ‘¡âŸ© +ğ‘âŸ¨ğ‘¦âŸ©).
3.  å•†åº—(ğ‘âŸ¨ğ‘¡âŸ©ã€ğ‘âŸ¨ğ‘¡âˆ’1âŸ©ã€ğ‘¥âŸ¨ğ‘¡âŸ©ã€ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ä¸­çš„ä¸€å®¶`cache`
4.  è¿”å›ğ‘âŸ¨ğ‘¡âŸ©ã€ğ‘¦Ì‚ âŸ¨ğ‘¡âŸ©å’Œ`cache`

è®©æˆ‘ä»¬é¦–å…ˆå®ç° softmax æ¿€æ´»å‡½æ•°:

```
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç å®ç°ä¸Šé¢æ˜¾ç¤ºçš„å››ä¸ªæ­¥éª¤:

```
def rnn_cell_forward(xt, a_prev, parameters):
    """
    Implements a single forward step of the RNN-cell as described in Figure (2)

    Arguments:
    xt -- your input data at timestep "t", numpy array of shape (n_x, m).
    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
    parameters -- python dictionary containing:
                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                        ba --  Bias, numpy array of shape (n_a, 1)
                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)
    """

    # Retrieve parameters from "parameters"
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    # compute next activation state using the formula given above
    a_next = np.tanh(np.dot(Wax,xt) + np.dot(Waa,a_prev) + ba)
    # compute output of the current cell using the formula given above
    yt_pred = softmax(np.dot(Wya,a_next) + by) 

    # store values you need for backward propagation in cache
    cache = (a_next, a_prev, xt, parameters)

    return a_next, yt_pred, cache
```

## 1.2.RNN å‘å‰ä¼ çƒ

é€’å½’ç¥ç»ç½‘ç»œ(RNN)æ˜¯æˆ‘ä»¬åˆšåˆšå»ºç«‹çš„ RNN ç»†èƒçš„é‡å¤ã€‚å¦‚æœä½ çš„æ•°æ®è¾“å…¥åºåˆ—æ˜¯ 10 ä¸ªæ—¶é—´æ­¥é•¿ï¼Œé‚£ä¹ˆä½ å°†é‡å¤ä½¿ç”¨ RNN å•å…ƒ 10 æ¬¡ã€‚è¿™å¦‚å›¾ 3 æ‰€ç¤ºã€‚

æ¯ä¸ªå•å…ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿æ¥å—ä¸¤ä¸ªè¾“å…¥:

*   ğ‘âŸ¨ğ‘¡âˆ’1âŸ©:æ¥è‡ªå‰ä¸€ä¸ªå•å…ƒæ ¼çš„éšè—çŠ¶æ€
*   ğ‘¥âŸ¨ğ‘¡âŸ©:å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥æ•°æ®

å®ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥æœ‰ä¸¤ä¸ªè¾“å‡º:

*   ä¸€ä¸ªéšè—çš„å›½å®¶(ğ‘âŸ¨ğ‘¡âŸ©)
*   ä¸€ä¸ªé¢„è¨€(ğ‘¦âŸ¨ğ‘¡âŸ©)

æƒé‡å’Œåå·®(ğ‘Šğ‘ğ‘ã€ğ‘ğ‘ã€ğ‘Šğ‘ğ‘¥ã€ğ‘ğ‘¥)åœ¨æ¯ä¸ªæ—¶é—´æ­¥é‡å¤ä½¿ç”¨

*   å®ƒä»¬åœ¨è°ƒç”¨â€œå‚æ•°â€å­—å…¸ä¸­çš„`rnn_cell_forward`ä¹‹é—´è¢«ç»´æŠ¤

![](img/c63bfa9e4fa46776d0b29a934d6e3e8e.png)

å›¾ 3ã€‚RNN æ¨¡å‹

**å®ç°å¦‚å›¾ 3** æ‰€ç¤ºçš„ RNN æ¨¡å‹:

1.  åˆ›å»ºä¸€ä¸ªç”± 0 å’Œå½¢çŠ¶(ğ‘›ğ‘ã€ğ‘šã€ğ‘‡ğ‘¥)çš„**å’Œ**ç»„æˆçš„ 3D æ•°ç»„ï¼Œè¯¥æ•°ç»„å°†å­˜å‚¨ RNN è®¡ç®—å‡ºçš„æ‰€æœ‰éšè—çŠ¶æ€ã€‚
2.  åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º(ğ‘›ğ‘¦ï¼Œğ‘šï¼Œğ‘‡ğ‘¥)çš„é›¶ï¼Œğ‘¦Ì‚çš„ä¸‰ç»´æ•°ç»„æ¥å­˜å‚¨é¢„æµ‹ã€‚è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œğ‘‡ğ‘¦=ğ‘‡ğ‘¥(é¢„æµ‹å’Œè¾“å…¥å…·æœ‰ç›¸åŒæ•°é‡çš„æ—¶é—´æ­¥é•¿)
3.  é€šè¿‡å°† 2D éšè—çŠ¶æ€`a_next`è®¾ç½®ä¸ºåˆå§‹éšè—çŠ¶æ€ğ‘0 æ¥åˆå§‹åŒ–å®ƒ
4.  åœ¨æ¯ä¸ªæ—¶é—´æ­¥ğ‘¡ :
    4.1ã€‚**å¾—åˆ°** ğ‘¥âŸ¨ğ‘¡âŸ©ï¼Œè¿™æ˜¯ä¸€ä¸ªå•ä¸€æ—¶é—´æ­¥é•¿ğ‘¡.çš„ğ‘¥çš„äºŒç»´åˆ‡ç‰‡ğ‘¥âŸ¨ğ‘¡âŸ©æœ‰å½¢çŠ¶(ğ‘›ğ‘¥ï¼Œğ‘š)ï¼Œx æœ‰å½¢çŠ¶(ğ‘›ğ‘¥ï¼Œğ‘šï¼Œğ‘‡ğ‘¥).
    4.1ã€‚**æ›´æ–°**2d éšè—çŠ¶æ€ğ‘âŸ¨ğ‘¡âŸ©(å˜é‡å`a_next`)ã€é¢„æµ‹ğ‘¦Ì‚âŸ¨ğ‘¡âŸ©å’Œç¼“å­˜é€šè¿‡è¿è¡Œ`rnn_cell_forward .`aâŸ¨tâŸ©å·²ç»æˆå½¢(ğ‘›âŸ¨ğ‘âŸ©ã€ğ‘š).
    4.3ã€‚**å­˜å‚¨**3d å¼ é‡ğ‘ä¸­çš„ 2D éšè—çŠ¶æ€ï¼Œåœ¨ğ‘¡-th ä½ç½® a æœ‰å½¢çŠ¶(ğ‘›ğ‘ï¼Œğ‘šï¼Œğ‘‡âŸ¨ğ‘¥âŸ©)
    4.4ã€‚**åœ¨ğ‘¡-th ä½ç½®çš„ 3D å¼ é‡ğ‘¦Ì‚ ğ‘ğ‘Ÿğ‘’ğ‘‘ä¸­å­˜å‚¨**2dğ‘¦Ì‚âŸ¨ğ‘¡âŸ©é¢„æµ‹(å˜é‡å`yt_pred`)ã€‚ğ‘¦Ì‚ âŸ¨ğ‘¡âŸ©æœ‰å½¢çŠ¶(ğ‘›ğ‘¦,ğ‘š)å’Œğ‘¦Ì‚æœ‰å½¢çŠ¶(ğ‘›ğ‘¦,ğ‘š,ğ‘‡ğ‘¥)
    4.6ã€‚**å°†ç¼“å­˜æ·»åŠ åˆ°ç¼“å­˜åˆ—è¡¨ä¸­**
5.  è¿”å› 3D å¼ é‡ğ‘a å’Œğ‘¦Ì‚ï¼Œä»¥åŠç¼“å­˜åˆ—è¡¨

```
def rnn_forward(x, a0, parameters):
    """
    Implement the forward propagation of the recurrent neural network described in Figure (3).

    Arguments:
    x -- Input data for every time-step, of shape (n_x, m, T_x).
    a0 -- Initial hidden state, of shape (n_a, m)
    parameters -- python dictionary containing:
                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                        ba --  Bias numpy array of shape (n_a, 1)
                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

    Returns:
    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
    caches -- tuple of values needed for the backward pass, contains (list of caches, x)
    """

    # Initialize "caches" which will contain the list of all caches
    caches = []

    # Retrieve dimensions from shapes of x and parameters["Wya"]
    n_x, m, T_x = x.shape
    n_y, n_a = parameters["Wya"].shape

    # initialize "a" and "y_pred" with zeros (â‰ˆ2 lines)
    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))

    # Initialize a_next (â‰ˆ1 line)
    a_next = a0

    # loop over all time-steps
    for t in range(T_x):
        # Update next hidden state, compute the prediction, get the cache (â‰ˆ1 line)
        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)
        # Save the value of the new "next" hidden state in a (â‰ˆ1 line)
        a[:,:,t] = a_next
        # Save the value of the prediction in y (â‰ˆ1 line)
        y_pred[:,:,t] = yt_pred
        # Append "cache" to "caches" (â‰ˆ1 line)
        caches.append(cache)

    # store values needed for backward propagation in cache
    caches = (caches, x)

    return a, y_pred, caches
```

# 2.é€’å½’ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­

åœ¨æ‰€æœ‰ç°ä»£çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä½ åªéœ€è¦å®ç°å‰å‘ä¼ é€’ï¼Œæ¡†æ¶è´Ÿè´£åå‘ä¼ é€’ï¼Œæ‰€ä»¥å¤§å¤šæ•°æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆä¸éœ€è¦ä¸ºåå‘ä¼ é€’çš„ç»†èŠ‚è´¹å¿ƒã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ­£å¤„äºå­¦ä¹ é˜¶æ®µï¼Œæˆ–è€…ä½ æœ‰è¶³å¤Ÿçš„å¥½å¥‡å¿ƒå»çœ‹çœ‹ RNN æ¨¡å‹æ˜¯å¦‚ä½•çœŸæ­£å·¥ä½œçš„ï¼Œé‚£ä¹ˆçœ‹ä¸€çœ‹æˆ–è€…ç”šè‡³ä»å¤´å¼€å§‹å®ç°å®ƒå°†æ˜¯å¾ˆé‡è¦çš„ã€‚

## 2.1. **RNN è½åçš„ç‰¢æˆ¿**

RNN çš„å‘åä¼ çƒå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬å°†é¦–å…ˆå®ç°è¯¥å•å…ƒï¼Œç„¶åä½¿ç”¨å®ƒæ¥å®ç° RNN çš„åå‘ä¼ æ’­:

![](img/b1ed6950354f88a4d398fb65693e5180.png)

å›¾ 4ã€‚RNN çš„åå‘ä¼ é€’ç»†èƒã€‚

è¦è®¡ç®— RNN å‘ååƒå…ƒï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼:

![](img/9a96dec32478627a807f499310970994.png)

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥å®ç°è¿™äº›ç­‰å¼:

```
def rnn_cell_backward(da_next, cache):
    """
    Implements the backward pass for the RNN-cell (single time-step).

    Arguments:
    da_next -- Gradient of loss with respect to next hidden state
    cache -- python dictionary containing useful values (output of rnn_cell_forward())

    Returns:
    gradients -- python dictionary containing:
                        dx -- Gradients of input data, of shape (n_x, m)
                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)
                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)
                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)
                        dba -- Gradients of bias vector, of shape (n_a, 1)
    """

    # Retrieve values from cache
    (a_next, a_prev, xt, parameters) = cache

    # Retrieve values from parameters
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    # compute the gradient of tanh with respect to a_next (â‰ˆ1 line)
    dtanh = (1-a_next*a_next)*da_next

    # compute the gradient of the loss with respect to Wax (â‰ˆ2 lines)
    dxt = np.dot(Wax.T,  dtanh)
    dWax = np.dot(dtanh,xt.T)

    # compute the gradient with respect to Waa (â‰ˆ2 lines)
    da_prev = np.dot(Waa.T, dtanh)  
    dWaa = np.dot( dtanh,a_prev.T)

    # compute the gradient with respect to b (â‰ˆ1 line)
    dba = np.sum( dtanh,keepdims=True,axis=-1)

    # Store the gradients in a python dictionary
    gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}

    return gradients
```

## 2.2. **RNN å‘åä¼ çƒ**

åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿ t è®¡ç®—æˆæœ¬ç›¸å¯¹äºğ‘âŸ¨ğ‘¡âŸ©at çš„æ¢¯åº¦æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºè¿™æœ‰åŠ©äºæ¢¯åº¦åå‘ä¼ æ’­åˆ°å‰ä¸€ä¸ª RNN å°åŒºã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦è¿­ä»£æ‰€æœ‰çš„æ—¶é—´æ­¥éª¤ï¼Œä»æœ€åå¼€å§‹ï¼Œåœ¨æ¯ä¸€æ­¥ï¼Œä½ å¢åŠ æ•´ä¸ªğ‘‘ğ‘âŸ¨ğ‘âŸ©ï¼Œğ‘‘ğ‘ŠâŸ¨ğ‘ğ‘âŸ©ï¼Œğ‘‘ğ‘ŠâŸ¨ğ‘ğ‘¥âŸ©å’Œä½ å­˜å‚¨ğ‘‘ğ‘¥.

**æŒ‡ä»¤**:

æ‰§è¡Œ`rnn_backward`åŠŸèƒ½ã€‚é¦–å…ˆç”¨é›¶åˆå§‹åŒ–è¿”å›å˜é‡ï¼Œç„¶ååœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿è°ƒç”¨`rnn_cell_backward`æ—¶å¾ªç¯æ‰€æœ‰æ—¶é—´æ­¥é•¿ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°å…¶ä»–å˜é‡ã€‚

```
def rnn_backward(da, caches):
    """
    Implement the backward pass for a RNN over an entire sequence of input data.

    Arguments:
    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)
    caches -- tuple containing information from the forward pass (rnn_forward)

    Returns:
    gradients -- python dictionary containing:
                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)
                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)
                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)
                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy array of shape (n_a, n_a)
                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)
    """

    ### START CODE HERE ###

    # Retrieve values from the first cache (t=1) of caches (â‰ˆ2 lines)
    (caches, x) = caches
    (a1, a0, x1, parameters) = caches[0]

    # Retrieve dimensions from da's and x1's shapes (â‰ˆ2 lines)
    n_a, m, T_x = da.shape
    n_x, m = x1.shape 

    # initialize the gradients with the right sizes (â‰ˆ6 lines)
    dx = np.zeros((n_x, m, T_x)) 
    dWax = np.zeros((n_a, n_x))
    dWaa = np.zeros((n_a, n_a))
    dba = np.zeros((n_a, 1)) 
    da0 = np.zeros((n_a, m))
    da_prevt = np.zeros((n_a, m))  

    # Loop through all the time steps
    for t in reversed(range(T_x)):
        # Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (â‰ˆ1 line)
        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])
        # Retrieve derivatives from gradients (â‰ˆ 1 line)
        dxt, da_prevt, dWaxt, dWaat, dbat = gradients["dxt"], gradients["da_prev"], gradients["dWax"], gradients["dWaa"], gradients["dba"]
        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (â‰ˆ4 lines)
        dx[:, :, t] = dxt  
        dWax += dWaxt  
        dWaa += dWaat  
        dba += dbat  

    # Set da0 to the gradient of a which has been backpropagated through all time-steps (â‰ˆ1 line) 
    da0 = da_prevt
    ### END CODE HERE ###

    # Store the gradients in a python dictionary
    gradients = {"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba}

    return gradients
```

***å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« å¹¶æ„¿æ„æ”¯æŒæˆ‘ï¼Œè¯·åŠ¡å¿…:***

*   **ğŸ‘ä¸ºè¿™ä¸ªæ•…äº‹é¼“æŒ(50 æ¬¡)å¹¶è·Ÿæˆ‘æ¥ğŸ‘‰**
*   **ğŸ“°æŸ¥çœ‹æˆ‘çš„åª’ä½“æ¡£æ¡ˆä¸­çš„æ›´å¤šå†…å®¹**
*   **ğŸ””å…³æ³¨æˆ‘:**[**LinkedIn**](https://www.linkedin.com/in/youssef-hosni-b2960b135/)**|**[**Medium**](https://medium.com/@youssefraafat57)**|**[**GitHub**](https://github.com/youssefHosni)**|**[**Twitter**](https://twitter.com/Youssef70125494)
*   *ğŸš€ğŸ‘‰* ***åŠ å…¥*** [***ä¸­ç­‰ä¼šå‘˜***](https://youssefraafat57.medium.com/membership) ***è®¡åˆ’ç»§ç»­æ— é™åˆ¶å­¦ä¹ ã€‚å¦‚æœä½ ä½¿ç”¨ä¸‹é¢çš„é“¾æ¥ï¼Œæˆ‘ä¼šæ”¶åˆ°ä¸€å°éƒ¨åˆ†ä¼šå‘˜è´¹ï¼Œä¸éœ€è¦ä½ é¢å¤–ä»˜è´¹ã€‚***

[](https://youssefraafat57.medium.com/membership) [## åŠ å…¥æˆ‘çš„ä»‹ç»é“¾æ¥åª’ä½“-ä¼˜ç´ ç¦èƒ¡æ–¯å°¼

### é˜…è¯» Youssef Hosni(ä»¥åŠåª’ä½“ä¸Šæˆåƒä¸Šä¸‡çš„å…¶ä»–ä½œå®¶)çš„æ¯ä¸€ä¸ªæ•…äº‹ã€‚æ‚¨çš„ä¼šå‘˜è´¹ç›´æ¥æ”¯æŒâ€¦

youssefraafat57.medium.com](https://youssefraafat57.medium.com/membership)