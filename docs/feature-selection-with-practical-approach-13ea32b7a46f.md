# å®ç”¨çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•

> åŸæ–‡ï¼š<https://pub.towardsai.net/feature-selection-with-practical-approach-13ea32b7a46f?source=collection_archive---------2----------------------->

## [æ•°æ®ç§‘å­¦](https://towardsai.net/p/category/data-science)ï¼Œ[æœºå™¨å­¦ä¹ ](https://towardsai.net/p/category/machine-learning)

## äº†è§£ç”¨äºä»¥æœ€å®ç”¨çš„æ–¹å¼ä»æ•°æ®é›†ä¸­é€‰æ‹©é‡è¦è¦ç´ çš„è¦ç´ é€‰æ‹©æŠ€æœ¯ã€‚

![](img/c8886711e1cd020de4e2754a2e2f1ca4.png)

> åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€å®ç”¨çš„æ–¹æ³•ä¸­çš„å„ç§æŠ€æœ¯å¯¹æ•°æ®é›†è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚

**ç‰¹æ€§é€‰æ‹©**æ˜¯ä»»ä½•æ•°æ®ç§‘å­¦é¡¹ç›®ç”Ÿå‘½å‘¨æœŸçš„å…³é”®æ­¥éª¤ä¹‹ä¸€ã€‚æœ‰è®¸å¤šæ–‡ç« ä»ç†è®ºä¸Šæè¿°äº†å¯ç”¨äºç‰¹å¾é€‰æ‹©çš„æŠ€æœ¯ï¼Œä½†æ˜¯ç¼ºä¹å®ç”¨çš„æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ç›´æ¥å¤„ç†ä»£ç ï¼Œçœ‹çœ‹å®ƒå®é™…ä¸Šæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘å°†å°è¯•æä¾›åˆå­¦è€…å‹å¥½çš„ä»£ç ç‰‡æ®µçš„é«˜çº§æè¿°ã€‚

## ä»€ä¹ˆæ˜¯ç‰¹å¾é€‰æ‹©ï¼Ÿ

å½“æˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„ç‰¹å¾éƒ½æœ‰åŒç­‰çš„è´¡çŒ®ã€‚æœ‰äº›åŠŸèƒ½éå¸¸é‡è¦ï¼Œæœ‰äº›ç”šè‡³å¯¹è®­ç»ƒæ¯«æ— å¸®åŠ©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥ä»æ•°æ®é›†ä¸­ç§»é™¤è¿™äº›ç±»å‹çš„è¦ç´ ã€‚å› æ­¤ï¼Œ**ç‰¹å¾é€‰æ‹©**æ˜¯ç§»é™¤æˆ–å‡å°‘è¾“å…¥ç‰¹å¾/å˜é‡çš„è¿‡ç¨‹ï¼Œè¿™åè¿‡æ¥é™ä½äº†**å¤æ‚æ€§**ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹**æ›´å¿«**ï¼Œå¹¶ä¸”è¿˜å¢åŠ äº†æ¨¡å‹çš„**å‡†ç¡®æ€§**ã€‚

æœ‰è®¸å¤šæŠ€æœ¯å¯ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œä½†æ˜¯æˆ‘ä»¬å°†åªè®¨è®ºè‘—åçš„å’Œé‡è¦çš„æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯æ˜¯:-

*   æ–¹å·®é˜ˆå€¼
*   ç›¸äº’å…³ç³»
*   å¡æ–¹æ£€éªŒ
*   ä¿¡æ¯å¢ç›Š

**æ³¨æ„:-åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åªè®¨è®ºå‰ä¸¤ä¸ªï¼Œå…¶ä½™ä¸¤ä¸ªå°†åœ¨ç¬¬äºŒéƒ¨åˆ†è®¨è®ºã€‚**

## èµ„æ–™ç»„

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª **Kaggle** çš„"*æ¡‘å¦å¾·é“¶è¡Œå®¢æˆ·æ»¡æ„åº¦*"æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¹Ÿå¯åœ¨ [**GitHub**](https://github.com/PushkaraSharma/articles_codes/tree/master/Feature_Selection_with_Practical_Approach) ä¸Šè·å¾—ã€‚ä¸æ­¤åŒæ—¶ï¼Œä½¿ç”¨çš„å…¶ä»–æ•°æ®é›†å¯ä»¥é€šè¿‡æè¿°çš„ä»£ç ç‰‡æ®µç›´æ¥è·å¾—ã€‚

## å…ˆå†³æ¡ä»¶:

æˆ‘å‡è®¾ä½ ç†Ÿæ‚‰ ***python*** å¹¶ä¸”å·²ç»åœ¨ä½ çš„ç³»ç»Ÿä¸­å®‰è£…äº† ***python 3*** ã€‚è¿™ä¸ªæ•™ç¨‹æˆ‘ç”¨äº†ä¸€ä¸ª ***jupyter ç¬”è®°æœ¬*** ã€‚ä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„ **IDE** ã€‚æ‰€æœ‰éœ€è¦çš„åº“éƒ½å†…ç½®åœ¨ ***anaconda*** å¥—ä»¶ä¸­ã€‚

# è®©æˆ‘ä»¬ç¼–ç 

è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸¤ç§æŠ€æœ¯ï¼Œå³**æ–¹å·®é˜ˆå€¼**å’Œ**ç›¸å…³æ€§**ã€‚

# ä½¿ç”¨æ–¹å·®é˜ˆå€¼ç§»é™¤æ’å®šè¦ç´ 

æˆ‘ä»¬åº”è¯¥ä»æ•°æ®é›†ä¸­ç§»é™¤çš„ç¬¬ä¸€ä¸ªè¦ç´ æ˜¯å¸¸é‡è¦ç´ ã€‚æ‰‹åŠ¨æ‰§è¡Œä¼¼ä¹å¾ˆå®¹æ˜“ï¼Œä½†å‡è®¾æ‚¨æœ‰**200**â€“**300**ç‰¹å¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸€äº›æŠ€å·§æ˜¯æœ‰æ„ä¹‰çš„ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ã€‚`Pandas`ç”¨äºåˆ›å»ºå’Œæ“ä½œæ•°æ®é›†ã€‚`VarianceThreshold`ç”¨äºå»é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾ã€‚**æ–¹å·®**åªæ˜¯å¯¹**å¯å˜æ€§**çš„ä¸€ç§åº¦é‡ã€‚ **0** æ–¹å·®æ„å‘³ç€æ‰€æœ‰å€¼éƒ½ç›¸åŒ/ä¸å˜ã€‚`Train_test_split`ç”¨äºä¸ºè®­ç»ƒå’Œæµ‹è¯•ç›®çš„æ‹†åˆ†æ•°æ®ã€‚

```
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
```

è¿™é‡Œï¼Œæˆ‘ä»¬åˆšåˆšåˆ›å»ºäº†ä¸€ä¸ªè™šæ‹Ÿçš„**æ•°æ®æ¡†**ï¼Œä»¥ä¾¿äºç†è§£è¯¥ç‰¹å¾é€‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åˆ— **C** å’Œ **D** å…·æœ‰å¸¸é‡å€¼ï¼Œå› æ­¤å®ƒä»¬åº”è¯¥è¢«åˆ é™¤ã€‚

```
df = pd.DataFrame({"A":[2,3,5,2,6],
                  "B":[4,6,4,2,8],
                  "C":[5,5,5,5,5],
                  "D":[0,0,0,0,0]})
df
```

![](img/f53aafd98576860825948823eed6b5a9.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»åˆå§‹åŒ–äº† VarianceThreshold çš„å¯¹è±¡`var`ï¼Œé˜ˆå€¼ä¸º **0** (å¸¸é‡)ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦æ”¹å˜è¿™ä¸€ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬æ‰“å°äº†å¸ƒå°”æ•°ç»„ï¼Œå…¶ä¸­`true`å’Œ`false` åˆ†åˆ«ä»£è¡¨éå¸¸æ•°å’Œå¸¸æ•°ç‰¹æ€§ã€‚

```
var = VarianceThreshold(threshold=0.0)
var.fit(df)
var.get_support()
```

![](img/d8b85f6a7fc52bc91269c46b1894991e.png)

æˆ‘ä»¬ä½¿ç”¨äº†åˆ—è¡¨ç†è§£ï¼Œå¾ªç¯éå†æ‰€æœ‰åˆ—ï¼Œå¹¶å°†å¸¸é‡ç‰¹æ€§æ’å…¥åˆ°åä¸º`constant_features`çš„åˆ—è¡¨ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬ä»æ•°æ®é›†ä¸­åˆ é™¤äº†å¸¸é‡è¦ç´ ã€‚

```
constant_features = [i for i in df.columns
                    if i not in df.columns[var.get_support()]]
print(constant_features)
df.drop(constant_features,axis=1)
```

![](img/3b37b0d59aa43cd9ce6405a48878a2c9.png)

å‰é¢çš„ä¾‹å­å¯èƒ½çœ‹èµ·æ¥å¤ªç®€å•äº†ï¼Œå¾ˆå®¹æ˜“è¢«å‘ç°ã€‚ä½†æ˜¯åœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­ï¼Œå°†ä¼šæœ‰ **100** æˆ– **1000** çš„ç‰¹æ€§ã€‚è¿™ç§æŠ€æœ¯åœ¨è¿™äº›æƒ…å†µä¸‹ä¼šæœ‰æ‰€å¸®åŠ©ã€‚è®©æˆ‘ä»¬ç”¨æ›´å¤§æ›´çœŸå®çš„æ•°æ®é›†æ¥çœ‹çœ‹åŒæ ·çš„æƒ…å†µã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åŠ è½½äº† GitHub ä¸Šçš„æ•°æ®é›†ã€‚æ•°æ®é›†ç”± **371** ä¸ªç‰¹å¾ç»„æˆã€‚

```
df = pd.read_csv("train.csv")
print(df.shape)
```

è¿™é‡Œï¼Œ **X** ä»£è¡¨è¾“å…¥å˜é‡ï¼Œ **Y** ä»£è¡¨ç›®æ ‡å˜é‡ã€‚æˆ‘ä»¬åº”è¯¥å§‹ç»ˆå°†ä»»ä½•ç‰¹å¾é€‰æ‹©æŠ€æœ¯åº”ç”¨äºè®­ç»ƒé›†ï¼Œä»¥é¿å…ä»»ä½•ç±»å‹çš„**è¿‡åº¦æ‹Ÿåˆ**ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•çš„åŸå› ã€‚ä¹‹åçš„è¿‡ç¨‹å’Œä¸Šé¢è§£é‡Šçš„ä¸€æ ·ã€‚åœ¨è¿™é‡Œï¼Œ371â€“332 =**39**ç‰¹å¾æ˜¯ä¸å˜çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ é™¤äº†è¿™äº›ç‰¹æ€§ã€‚

```
X = df.drop(labels=['TARGET'],axis=1)
Y = df['TARGET']
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2)
var = VarianceThreshold(threshold=0.0)
var.fit(x_train)
sum(var.get_support())constant_features = [i for i in x_train
                    if i not in x_train.columns[var.get_support()]]
x_train.drop(constant_features,axis=1)
```

![](img/422dbaebd882770f8f5a4cfb73c7f4b0.png)

# ä½¿ç”¨çš®å°”é€Šç›¸å…³ç§»é™¤ç›¸ä¼¼ç‰¹å¾

ä¸€äº›ç‰¹å¾å½¼æ­¤ä¹‹é—´å‘ˆçº¿æ€§å…³ç³»ï¼Œå³å½“å˜é‡ 2 å¢åŠ  **3** å€æ—¶ï¼Œå˜é‡ 1 å¢åŠ  **2** å€ã€‚æ‰€ä»¥è¿™ä¸¤ä¸ªç‰¹å¾æ˜¯é«˜åº¦ç›¸å…³çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ é™¤å…¶ä¸­ä¸€ä¸ªï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶ä¸ä¼šä¸¢å¤±ä»»ä½•é‡è¦ä¿¡æ¯ã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºç¤ºä¾‹å¯¼å…¥äº†`boston`æ•°æ®é›†ã€‚`seaborn`å°†ç”¨äºç»˜åˆ¶äº¤äº’å›¾å½¢ã€‚å…¶ä»–åº“å·²ç»è®¨è®ºè¿‡äº†ã€‚

```
from sklearn.datasets import load_boston
import pandas as pd
from sklearn.model_selection import train_test_split
import seaborn as sns
```

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†åŠ è½½æ•°æ®é›†çš„`pandas`æ•°æ®å¸§ï¼Œå¹¶æ·»åŠ äº†ç›®æ ‡å˜é‡ä½œä¸º`TARGET`åˆ—ã€‚

```
dataset = load_boston()
df = pd.DataFrame(dataset.data,columns = dataset.feature_names)
df['TARGET'] = dataset.target
df.head()
```

![](img/946fdd1df9054189450f9a4d6c7f629c.png)

è¿™ä¸å‰é¢çš„æŠ€æœ¯æ­¥éª¤ç›¸ä¼¼ï¼Œæˆ‘ä»¬å°†è¾“å…¥å˜é‡å£°æ˜ä¸º **X** ï¼Œå°†ç›®æ ‡å˜é‡å£°æ˜ä¸º **Y** ã€‚éœ€è¦åˆ†å‰²æ•°æ®ï¼Œå› ä¸ºæˆ‘ä»¬åªå¯¹è®­ç»ƒé›†æ‰§è¡Œç‰¹å¾é€‰æ‹©éƒ¨åˆ†ï¼Œä»¥é¿å…ä»»ä½•ç±»å‹çš„è¿‡æ‹Ÿåˆã€‚`x_train`çš„å½¢çŠ¶æ˜¯(404ï¼Œ13)`x_test`çš„å½¢çŠ¶æ˜¯(102ï¼Œ13)ã€‚

```
X = df.drop('TARGET',axis=1)
Y = df['TARGET']
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)
x_train.shape , x_test.shape
```

è¿™é‡Œï¼Œæˆ‘ä»¬ç»™å‡ºäº†ä¸€ä¸ªå›ºå®šå¤§å°çš„å›¾ï¼Œç„¶ååœ¨`x_train`ä¸Šè°ƒç”¨`corr()`å‡½æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬åªæ‰“å°`correlation`å˜é‡ï¼Œé‚£ä¹ˆå°†å¾ˆéš¾è§£é‡Šã€‚å› æ­¤ï¼Œåœ¨`seaborn`çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†**çƒ­å›¾**ï¼Œæ¸…æ¥šåœ°æ˜¾ç¤ºäº†ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

```
plt.figure(figsize=(12,10))
correlation = x_train.corr()
sns.heatmap(correlation,annot=True,cmap=plt.cm.CMRmap)
```

![](img/34f998aa451ff3c288b3b88a596a3dbe.png)

ä¹‹åï¼Œæˆ‘ä»¬å£°æ˜äº†ä»¥**æ•°æ®é›†**å’Œ**é˜ˆå€¼**ä¸ºå‚æ•°çš„å‡½æ•°`find_correlated_features`ã€‚åœ¨å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬è¿­ä»£äº†ç›¸å…³çŸ©é˜µçš„æ¯ä¸ªå€¼ï¼Œå¦‚æœç›¸å…³å€¼é«˜äºç»™å®šçš„é˜ˆå€¼ï¼Œåˆ™å°†åˆ—åæ·»åŠ åˆ°åä¸º`col_corr`çš„é›†åˆä¸­ã€‚

```
def find_correlated_features(data,threshold):
    col_corr = set()
    corr_metrix = data.corr()
    for i in range(len(corr_metrix.columns)):
        for j in range(i):
            if((corr_metrix.iloc[i,j])>threshold):
                column_name = corr_metrix.columns[i]
                col_corr.add(column_name)
    return col_corr
```

ç°åœ¨ï¼Œæˆ‘ä»¬åˆšåˆšä½¿ç”¨`x_train`ä½œä¸ºæ•°æ®å’Œ threshold = **0.7** è°ƒç”¨äº†ä¸Šé¢çš„å‡½æ•°ï¼Œä»¥è·å¾—æˆ‘ä»¬å¯ä»¥ä¸¢å¼ƒçš„ç›¸å…³ç‰¹å¾ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™äº›ç‰¹å¾æ˜¯{ **å¹´é¾„**ï¼Œ**æ°®æ°§åŒ–ç‰©**ï¼Œ**ç¨æ”¶** }ã€‚åˆ é™¤è¿™äº›ç‰¹å¾åï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ç‰¹å¾ä» **13** å‡å°‘åˆ° **10** ã€‚

å½“è¦ç´ æ•°é‡è¿‡å¤šè€Œå¯¼è‡´çƒ­å›¾æ— åŠ©äºè§£é‡Šæ—¶ï¼Œå®šä¹‰ä¸Šè¿°å‡½æ•°å°†æœ‰åŠ©äºæˆ‘ä»¬ã€‚

```
corelated_features = find_correlated_features(x_train,0.7)
print(corelated_features)
x_train = x_train.drop(corelated_features,axis=1)
x_train.head()
```

![](img/8e70fee7cd225ba391ae08e69cdcbb92.png)

æˆ‘å¸Œæœ›ä½ ç†è§£è¿™ä¸¤ä¸ª**ç‰¹å¾é€‰æ‹©**çš„æŠ€å·§ã€‚å¦‚æœä½ æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·éšæ—¶è¯„è®ºï¼Œæˆ‘ä¼šå°½å¿«å›å¤ã€‚

å…¶ä»–ä¸¤ç§æŠ€æœ¯å°†å¾ˆå¿«ä»‹ç»ã€‚

æºä»£ç åœ¨ [**GitHub**](https://github.com/PushkaraSharma/articles_codes/tree/master/Feature_Selection_with_Practical_Approach) ä¸Šæœ‰ã€‚è¯·éšæ„æ”¹è¿›ã€‚

è°¢è°¢ä½ å®è´µçš„æ—¶é—´ã€‚ğŸ˜Šæˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚

å¦å¤–ï¼ŒæŸ¥çœ‹æˆ‘å…³äº[**åˆ©ç”¨æœºå™¨å­¦ä¹ **](https://medium.com/towards-artificial-intelligence/classify-plant-leaf-diseases-using-machine-learning-4747dc1eb43d) å¯¹æ¤ç‰©å¶éƒ¨ç—…å®³è¿›è¡Œåˆ†ç±»çš„æ–‡ç« 

[](https://medium.com/towards-artificial-intelligence/logistic-regression-from-scratch-with-only-python-code-9d3ae607e739) [## ä»…ä½¿ç”¨ Python ä»£ç ä»é›¶å¼€å§‹è¿›è¡Œé€»è¾‘å›å½’

### ä»…ä½¿ç”¨ Python å¯¹å¤šè¦ç´ æ•°æ®é›†åº”ç”¨é€»è¾‘å›å½’ã€‚åˆ†æ­¥å®ç°ç¼–ç ç¤ºä¾‹â€¦

medium.com](https://medium.com/towards-artificial-intelligence/logistic-regression-from-scratch-with-only-python-code-9d3ae607e739) [](https://medium.com/towards-artificial-intelligence/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f) [## å›å½’é—®é¢˜çš„æ¢¯åº¦ä¸‹é™ v/s æ­£è§„æ–¹ç¨‹

### é€‰æ‹©æ­£ç¡®çš„ç®—æ³•æ¥æ‰¾åˆ°ä½¿æˆæœ¬å‡½æ•°æœ€å°çš„å‚æ•°ã€‚

medium.com](https://medium.com/towards-artificial-intelligence/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f)