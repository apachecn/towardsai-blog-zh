# ä½¿ç”¨ T5 å˜å‹å™¨ç”Ÿæˆé…·ç‚«çš„æ•…äº‹æƒ…èŠ‚ï¼Œäº«å—ä¹è¶£

> åŸæ–‡ï¼š<https://pub.towardsai.net/generating-cool-storylines-using-a-t5-transformer-and-having-fun-4a79f6ab8adb?source=collection_archive---------0----------------------->

## [è‡ªç„¶è¯­è¨€å¤„ç†](https://towardsai.net/p/category/nlp)

## t5â€”â€”ä¸€ä¸ªç©·äººçš„ GPT 3

![](img/be447db577aea8c75da2477617463199.png)

[ç§‘æŠ€æ—¥æŠ¥](https://unsplash.com/@techdailyca?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/s/photos/streaming?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

# T5 å˜å‹å™¨ç®€ä»‹

Google AI çš„äººå‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡â€œæ¢ç´¢ä½¿ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨è¿›è¡Œè¿ç§»å­¦ä¹ çš„é™åˆ¶â€ï¼Œå¹¶ä»‹ç»äº†ä¸€é¡¹å…³äºä»€ä¹ˆç±»å‹çš„é¢„è®­ç»ƒæ–¹æ³•æˆ–è¿ç§»å­¦ä¹ æŠ€æœ¯æœ€æœ‰æ•ˆçš„å®è¯ç ”ç©¶ï¼Œç„¶åä½¿ç”¨è¯¥ç ”ç©¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œå³æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨(T5)ã€‚è¿™ä¸ª transformer æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªæ›´å¹²å‡€çš„æ™®é€šçˆ¬è¡Œè¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè°·æ­Œå°†å…¶å‘½åä¸ºå·¨å¤§çš„å¹²å‡€çˆ¬è¡Œè¯­æ–™åº“(C4)ã€‚å¬èµ·æ¥å¾ˆé…·å§ğŸ˜ã€‚å½“æ‚¨æ£€æŸ¥è¯¥æ¨¡å‹çš„èƒ½åŠ›å’Œçµæ´»æ€§ï¼Œä»¥ä¾¿ç”¨å¾ˆå°‘åˆ°ä¸­ç­‰çš„æ•°æ®å¯¹å¤§é‡çš„ä¸‹æ¸¸ NLP é—®é¢˜è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒçš„æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚

# ç°åœ¨ä½ å¯èƒ½ä¼šæƒ³ï¼ŒT5 å’Œå…¶ä»–å‹å·çš„å˜å½¢é‡‘åˆšæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦çœ‹çœ‹å…¶ä»–è½¬æ¢å™¨ï¼Œå¦‚ä¼¯ç‰¹ã€GPT ç­‰ã€‚æ‰€æœ‰è¿™äº›è½¬æ¢å™¨éƒ½é’ˆå¯¹å¤§é‡æ•°æ®è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä½†ä¸ºäº†å¯¹åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä¼šæ·»åŠ ä¸€ä¸ªåˆ†ç±»å±‚å¹¶è¾“å‡ºä¸€ä¸ªåˆ†ç±»æ ‡ç­¾ï¼Œæˆ–è€…ä¸º NER è¾“å‡ºä¸€ä¸ªè¾“å…¥èŒƒå›´ã€‚ä½†æ˜¯åœ¨ T5ï¼Œä¸€åˆ‡éƒ½æ˜¯æœ‰åºçš„ï¼Œå°±åƒä»–ä»¬è¯´çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€ã€‚å› æ­¤ï¼Œå¯¹äºåˆ†ç±»ï¼Œå®ƒå°†æ˜¯å­—ç¬¦ä¸²è¾“å‡ºï¼Œå¯¹äº NERï¼Œå®ƒå°†æ˜¯å­—ç¬¦ä¸²ï¼Œç”šè‡³å¯¹äºå›å½’ï¼Œå®ƒä¹Ÿæ˜¯å­—ç¬¦ä¸²ã€‚ä¸ä¿¡æˆ‘çœ‹æŠ¥çº¸ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œè¯¥æ¨¡å‹ä¸å—ä¸€å®šæ•°é‡çš„ç±»çš„é™åˆ¶ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å¤„ç†å¤šä¸ªæ–‡æœ¬åˆ†ç±»æ•°æ®å¹¶è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼Œè€Œæ— éœ€æ‹…å¿ƒæ ‡ç­¾çš„å¤„ç†ã€‚å› ä¸ºä½ ç»™å®ƒçš„ä¸€åˆ‡éƒ½åªæ˜¯ä¸€ä¸ªåºåˆ—ã€‚è¿™ç¡®å®ä¸ºæ¨¡å‹æä¾›äº†å¾ˆå¤šè¡¨è¾¾èƒ½åŠ›ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒå¯èƒ½ä¼šé¢„æµ‹æ ‡ç­¾ç©ºé—´ä¹‹å¤–çš„ä¸€äº›ä¸œè¥¿ã€‚æˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•äººæŠ¥å‘Šè¿™ä¸€ç‚¹æˆ–æœ‰è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥å®ƒåº”è¯¥å·¥ä½œè‰¯å¥½ã€‚

![](img/783a9f227591d7fa91226e177255914f.png)

æ¥æº:è°·æ­Œäººå·¥æ™ºèƒ½åšå®¢

è¿™æ˜¯å¾ˆå¤šå…³äºå˜å½¢é‡‘åˆšåŠå…¶èƒŒåçš„æƒ³æ³•çš„è®¨è®ºï¼Œå¦‚æœä½ æƒ³é˜…è¯»æ›´å¤šï¼Œè¯·æŸ¥çœ‹è®ºæ–‡å’Œè°·æ­Œå®˜æ–¹äººå·¥æ™ºèƒ½åšå®¢ã€‚

*   [ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™](https://arxiv.org/pdf/1910.10683.pdf)
*   [è°·æ­Œäººå·¥æ™ºèƒ½åšå®¢](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ä»£ç ï¼Œæ„å»ºä¸€äº›éå¸¸æœ‰è¶£çš„ä¸œè¥¿ã€‚

# æˆ‘ä»¬ä»Šå¤©åœ¨å»ºé€ ä»€ä¹ˆï¼Ÿ

æ­£å¦‚åšå®¢çš„æ ‡é¢˜æ‰€æš—ç¤ºçš„ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªæ•…äº‹æƒ…èŠ‚ç”Ÿæˆå™¨ã€‚ä½†æ˜¯å¯¹äºæˆ‘ä»¬å°†ç”¨æ¥è¾“å‡ºæ•…äº‹æƒ…èŠ‚/æƒ…èŠ‚çš„è¾“å…¥æœ‰ä¸€ç‚¹å°å°çš„å˜åŒ–ã€‚ç”±äº transformer æ˜¯ Sequence2Sequence æˆ–è°·æ­Œæ‰€è¯´çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€,æˆ‘ä»¬å°†é€šè¿‡è¾“å…¥ç±»å‹ã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç§æ—å’Œæ•…äº‹çº¿/æƒ…èŠ‚çš„è®­ç»ƒä½œä¸ºè¾“å‡ºæ¥å¾®è°ƒè¿™ä¸ªæ¨¡å‹ã€‚è¿™æ ·åšæœ‰åŠ©äºæˆ‘ä»¬çœ‹åˆ°å»ºè®®çš„ Sequence2Sequence(â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€)è½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚

è¦äº†è§£æœ¬æ•™ç¨‹çš„æ•ˆæœå¦‚ä½•ï¼Œä½ å¯ä»¥ç‚¹å‡»è¿™ä¸ª[é“¾æ¥](https://movie-plot-generator.vercel.app/)å»æœªæ¥çœ‹çœ‹è¿™ä¸ªæ¨¡å‹å¦‚ä½•è¿è¡Œï¼Œç„¶åå›åˆ°è¿™é‡Œä¸ºä½ è‡ªå·±å»ºé€ å®ƒğŸ˜‚ã€‚

 [## ç”µå½±æƒ…èŠ‚ç”Ÿæˆå™¨

### æˆ‘åœ¨ç½‘ä¸Šç”Ÿæˆæ¨¡ç³Šçš„ç”µå½±æƒ…èŠ‚(ä½†æœ‰æ—¶å¾ˆå¥½)ã€‚ä½†æˆ‘å¯ä»¥å‘ä½ ä¿è¯ï¼Œå®ƒå°†æ°¸è¿œæ˜¯â€¦

ç”µå½±-æƒ…èŠ‚-ç”Ÿæˆå™¨. vercel.app](https://movie-plot-generator.vercel.app/) 

# æˆ‘ä»¬å°†ä½¿ç”¨å“ªäº›æ•°æ®ï¼Ÿ

æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†æ˜¯[ç»´åŸºç™¾ç§‘ç”µå½±æƒ…èŠ‚](https://www.kaggle.com/jrobischon/wikipedia-movie-plots)æ•°æ®é›†ã€‚å®ƒæœ‰å…³äºå‘è¡Œå¹´ä»½ã€æ ‡é¢˜ã€ç§æ—ã€å¯¼æ¼”ã€æ¼”å‘˜ã€æƒ…èŠ‚ã€æµæ´¾ã€ç»´åŸºé¡µé¢å’Œæƒ…èŠ‚çš„æ•°æ®ã€‚æˆ‘ä»¬å°†åªä½¿ç”¨ç±»å‹ï¼Œå¯¼æ¼”ï¼Œæ¼”å‘˜ï¼Œç§æ—å’Œæƒ…èŠ‚æ¥å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‚¨å¯ä»¥éšæ„å°è¯•ä»»ä½•å…¶ä»–é¢å¤–çš„åˆ—æˆ–ä»»ä½•å…¶ä»–æ•°æ®é›†ã€‚

# å¦‚ä½•å‡†å¤‡èµ„æ–™ï¼Ÿ

è¿™æ˜¯å»ºæ¨¡æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ã€‚å› æ­¤ï¼Œè¯·ä»ä¸Šé¢çš„é“¾æ¥ä¸‹è½½æ•°æ®ï¼Œå¹¶æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤æ“ä½œã€‚

*   *ä¸€äº›æ­£è§„è¿›å£*

```
import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook, tnrange
from sklearn.utils import shuffle
import pickle
```

*   *ä¸€äº›ä¿å­˜æ•°æ®çš„åŠŸèƒ½*

```
def save_pickle(path, obj):
  with open(path, 'wb') as fp:
    pickle.dump(obj, fp)def load_pickle(path):
  with open(path, 'rb') as fp:
    return pickle.load(fp)
```

*   *è§£å‹æ•°æ®ï¼ŒåŠ è½½ç†ŠçŒ«*ğŸ¼ ğŸ¼ ğŸ¼

```
movieDf = pd.read_csv('drive/MyDrive/MoviePlotsModels/data/wiki_movie_plots_deduped.csv')
```

*   *æ”¶é›†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹æ‰€éœ€çš„ä»»ä½•ä¸œè¥¿*

```
outputs = []with tqdm_notebook(total = len(movieDf)) as pbar:
  for ix, row in movieDf.iterrows():
        genre_to_plot_input = f'generate plot for genre: {row.Genre}'
    genre_to_plot_output = row.Plot
    outputs.append({
        "source": genre_to_plot_input,
        "target": genre_to_plot_output
    }) genre_director_to_plot_input = f'generate plot for: genre {row.Genre} and director: {row.Director}'
    genre_director_to_plot_output = row.Plot
    outputs.append({
        "source": genre_director_to_plot_input,
        "target": genre_director_to_plot_output
    }) genre_director_ethinicity_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and ethinicity {row["Origin/Ethnicity"]}'
    genre_director_ethinicity_to_plot_output = row.Plot
    outputs.append({
        "source": genre_director_ethinicity_to_plot_input,
        "target": genre_director_ethinicity_to_plot_output
    }) if not pd.isna(row.Cast):

      genre_director_cast_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and cast: {row.Cast}'
      genre_director_cast_to_plot_output = row.Plot outputs.append({
          "source": genre_director_cast_to_plot_input,
          "target": genre_director_cast_to_plot_output
      }) pbar.update(1)
```

*   *è®©æˆ‘ä»¬ä¿å­˜æ•°æ®*

```
save_pickle('t5-source-target-data.pkl', outputs)
```

# è®©æˆ‘ä»¬è®­ç»ƒ(å¾®è°ƒğŸ˜…)æˆ‘ä»¬çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬â€è½¬æ¢å™¨

ä»ä¸Šé¢çš„æ ‡é¢˜ä½ å¯èƒ½å·²ç»æ˜ç™½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæ¥è‡ª **HuggingFace çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬ç¡®å®è¦ä½¿ç”¨`**transformers**`åº“ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹è®­ç»ƒä»£ç ã€‚åœ¨å¼€å§‹ç¼–ç ä¹‹å‰ï¼Œå®‰è£…ä¸‹é¢çš„åŒ…ï¼Œé‡å¯ä½ çš„ Colab å¹¶å¼€å§‹è®­ç»ƒã€‚å¦‚æœä½ ä¸é‡å¯ï¼Œä½ å¯èƒ½ä¼šåœ¨åˆå§‹åŒ–æ¨¡å‹å’Œä»¤ç‰ŒåŒ–å™¨çš„æ—¶å€™é‡åˆ°ä¸€äº›é”™è¯¯ï¼Œå½“ä½ åœ¨ Stackoverflow å’Œ Github ä¸Šæœç´¢äº†ä¸€ä¸ªå°æ—¶ä¹‹åï¼Œä½ å¯èƒ½ä¼šæ‰¾åˆ°è§£å†³æ–¹æ³•ï¼Œé‚£å°±æ˜¯åœ¨å®‰è£…å®ŒåŒ…ä¹‹åé‡å¯ Colab ç¬”è®°æœ¬ã€‚å› æ­¤ï¼Œåªéœ€æŒ‰ç…§å®‰è£…åé‡æ–°å¯åŠ¨ Colab ç¬”è®°æœ¬çš„æ­¥éª¤æ“ä½œï¼Œå®ƒå°†ä¸ºæ‚¨èŠ‚çœæ—¶é—´ã€‚**

*   *å®‰è£…è¿™äº›åŒ…(ä½ ä¼šçœ‹åˆ°å®ƒä»¬çš„é‡è¦æ€§)*

```
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install rich[jupyter]
```

***é¡ºä¾¿è¯´ä¸€å¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ PyTorchï¼Œæ‰€ä»¥å¦‚æœä½ ä¸€ç›´è®¤ä¸ºè¿™æ˜¯ TensorFlowï¼Œæˆ‘å¾ˆæŠ±æ­‰è®©ä½ å¤±æœ›äº†ã€‚æˆ‘çŒœ*** ä½ å¯èƒ½æƒ³æ”¹å˜ä½ çš„æ¡†æ¶ğŸ¤£

*   *å†æ¬¡å¸¸è§„è¿›å£*

```
import os
import re
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook, tnrange
from sklearn.utils import shuffle
import pickle
import math
## use if working on jupyter notebook or colab
from IPython.display import clear_output
```

*   *ä¸€äº›ä¿å­˜å’ŒåŠ è½½æ•°æ®çš„åŠŸèƒ½*

```
def save_pickle(path, obj):
  with open(path, 'wb') as fp:
    pickle.dump(obj, fp)def load_pickle(path):
  with open(path, 'rb') as fp:
    return pickle.load(fp)
```

D *å¦‚æœä½ ç”¨çš„æ˜¯åŒä¸€ä¸ª Colab ç¬”è®°æœ¬*å°±ä¸è¦å†å¤åˆ¶ç²˜è´´äº†

*   *å‹å·ç›¸å…³è¿›å£*

```
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import T5Tokenizer, T5ForConditionalGeneration
from rich.table import Column, Table
from rich import box
from rich.console import Consolefrom torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
```

*   *è®¾ç½®ä¸°å¯Œçš„æ§åˆ¶å°ï¼Œä»¥æ›´å¥½çš„æ–¹å¼æ˜¾ç¤ºåˆ†æ•°*

```
console = Console(record=True)training_logger = Table(
    Column("Random Selection", justify = "center"),
    Column("Epoch", justify="center"),
    Column("Loss", justify="center"),
    title="Training Status",
    pad_edge=False,
    box=box.ASCII,
)valid_loggger = Table(
    Column("Random Selection", justify = "center"),
    Column("Loss", justify = "center"),
    title="Validation Status",
    pad_edge=False,
    box=box.ASCII,
)
```

*   *åŠ è½½é¢„å¤„ç†æ•°æ®*

```
data = load_pickle('/content/drive/MyDrive/T5MovieWikiTraining2_0/t5-source-target-data-2.pkl')
```

*   *PyTorch* `*Dataset*` *ç‰©ä½“*

åˆå§‹åŒ–æ­¤å¯¹è±¡æ—¶ä¼ é€’æ ‡è®°åŒ–å™¨å®ä¾‹

```
class T5Dataset(Dataset): def __init__(self, tokenizer, data, source_len, target_len): super(T5Dataset, self).__init__()
    self.tokenizer = tokenizer
    self.source_len = source_len
    self.target_len = target_len
    self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): source_seq = self.data[index]['source']
    target_seq = self.data[index]['target'] source = self.tokenizer.batch_encode_plus(
        [source_seq],
        max_length = self.source_len,
        pad_to_max_length = True,
        truncation = True,
        padding = "max_length",
        return_tensors = "pt"
    ) target = self.tokenizer.batch_encode_plus(
        [target_seq],
        max_length = self.target_len,
        pad_to_max_length = True,
        truncation = True,
        padding = "max_length",
        return_tensors = "pt"
    ) source_ids = source["input_ids"].squeeze()
    source_mask = source["attention_mask"].squeeze()
    target_ids = target["input_ids"].squeeze()
    target_mask = target["attention_mask"].squeeze() return {
        "source_ids": source_ids,
        "source_mask": source_mask,
        "target_ids": target_ids,
        "target_mask": target_mask
    }
```

*   *è®­ç»ƒåŠŸèƒ½*

```
def train(epoch, tokenizer, model, device, loader, optimizer): model.train() total_loss = 0
    total_counts = 0 for _, data in enumerate(tqdm_notebook(loader, desc = "Train DL")): y = data["target_ids"].to(device, dtype = torch.long)
        y_ids = y[:, :-1].contiguous()
        lm_labels = y[:, 1:].clone().detach()
        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
        ids = data["source_ids"].to(device, dtype = torch.long)
        mask = data["source_mask"].to(device, dtype = torch.long) optimizer.zero_grad() outputs = model(
            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels
        ) loss = outputs[0] total_counts += 1
        total_loss += loss.item() loss.backward()
        optimizer.step() return total_loss/total_counts
```

*   *éªŒè¯åŠŸèƒ½*

```
def validate(epoch, tokenizer, model, device, loader): model.eval()
    total_loss = 0
    total_counts = 0 with torch.no_grad(): for _, data in enumerate(tqdm_notebook(loader, desc = "Valid DL")): y = data["target_ids"].to(device, dtype = torch.long)
            y_ids = y[:, :-1].contiguous()
            lm_labels = y[:, 1:].clone().detach()
            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
            ids = data["source_ids"].to(device, dtype = torch.long)
            mask = data["source_mask"].to(device, dtype = torch.long) outputs = model(
            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels
            ) loss = outputs[0] total_loss += loss.item()
            total_counts += 1 return total_loss / total_counts
```

*   è®©æˆ‘ä»¬æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·

```
def trainer(
    data, model_params, output_dir = "./outputs/"
): torch.manual_seed(model_params["SEED"])
    torch.cuda.manual_seed(model_params["SEED"])
    np.random.seed(model_params["SEED"])
    torch.backends.cudnn.deterministic = True    console.log(f'''Model: Loading {model_params['MODEL']}.....''') tokenizer = T5Tokenizer.from_pretrained(model_params["MODEL"]) model = T5ForConditionalGeneration.from_pretrained(model_params["MODEL"]) model = model.to(device) console.log(f"[DATA]: READING DATA.......") optimizer = torch.optim.Adam(
        params=model.parameters(), lr=model_params["LEARNING_RATE"]
    ) # Training loop
    console.log(f"[Initiating Fine Tuning]...\\n") ## model save path path = os.path.join(output_dir, "model_files") console.log("Starting with Random Selection")
    ## random selection
    prev_loss = []
    for randomSelection in tnrange(model_params["RANDOM_TRAIN_STEPS"], desc = 'Random Selection'):

        copyData = data.copy()
        copyData = shuffle(copyData)        train_size = 0.75
        random_permuts = np.random.permutation(len(copyData))
        train_nums = round(len(random_permuts) * train_size)
        train_dataset = [copyData[i] for i in random_permuts[:train_nums]]
        valid_dataset = [copyData[i] for i in random_permuts[train_nums:]]         training_set = T5Dataset(
            tokenizer, train_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]
        ) val_set = T5Dataset(
            tokenizer, valid_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]
        ) train_params = {
            "batch_size": model_params["TRAIN_BATCH_SIZE"],
            "shuffle": True,
            "num_workers": 0
        } val_params = {
            "batch_size": model_params["VALID_BATCH_SIZE"],
            "shuffle": False,
            "num_workers": 0
        }

        train_dl = DataLoader(training_set, **train_params)        ## training 
        console.log(f'[MODEL TRAINING]')
        clear_output(wait = True)
        for epoch in tnrange(model_params["TRAIN_EPOCHS"], desc = "Training"):

            total_loss = train(epoch, tokenizer, model, device, train_dl, optimizer) training_logger.add_row(str(randomSelection), str(epoch), str(total_loss))
            console.log(training_logger)
            if epoch == 0:
              console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")
              model.save_pretrained(os.path.join(output_dir, "model_files_initial"))
              tokenizer.save_pretrained(os.path.join(output_dir, "model_files_initial")) if epoch > 0: if min(prev_loss) > total_loss:
                    console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")
                    model.save_pretrained(path)
                    tokenizer.save_pretrained(path) prev_loss.append(total_loss)
        del train_dl, training_set
        ## validation
        valid_dl = DataLoader(val_set, **val_params)
        console.log(f'[MODEL VALIDATION]')
        for epoch in tnrange(model_params["VAL_EPOCHS"], desc = "Validation"): val_loss = validate(epoch, tokenizer, model, device, valid_dl)

            valid_loggger.add_row(str(randomSelection), str(val_loss))
            console.log(valid_loggger)
        console.save_text(os.path.join(output_dir, f"logs-random-{randomSelection}.txt")) console.log(f"[VALIDATAION DONE]")     
        del valid_dl, val_set
```

æ‚¨å¯ä»¥ä½¿ç”¨éšæœºé€‰æ‹©ç­–ç•¥å¯¹æ¯ N ä¸ªæ—¶æœŸçš„å°æ•°æ®å—è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨ N ä¸ªæ—¶æœŸåéšæœºé‡‡æ ·å¦ä¸€ä¸ªæ•°æ®å—ï¼Œå¹¶å†æ¬¡å¯¹æ–°é‡‡æ ·çš„æ•°æ®å—è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æ‚¨æ›´å¿«åœ°è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”æ¨¡å‹ä¹Ÿå¯ä»¥çœ‹åˆ°æ‰€æœ‰æ•°æ®ã€‚ä½†æ˜¯å»ºè®®ä»…åœ¨ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ä½¿ç”¨ã€‚

*   *æ¨¡å‹å‚æ•°*

```
model_params = {  
    "MODEL": "t5-base", # if you have a bigger machine you can use t5-large
    "TRAIN_BATCH_SIZE": 2,
    "VALID_BATCH_SIZE": 2,
    "TRAIN_EPOCHS": 10,
    "VAL_EPOCHS": 1,
    "LEARNING_RATE": 1e-4,
    "MAX_SOURCE_TEXT_LENGTH": 128,
    "MAX_TARGET_TEXT_LENGTH": 786,
    "SEED": 3007,
    "RANDOM_TRAIN_STEPS": 50
}
```

*   è®©æˆ‘ä»¬è®­ç»ƒè¿™åªé‡å…½ğŸ»

```
trainer(
    data = data,
    model_params = model_params,
    output_dir = "./outputs"
)
```

è¿™å°†å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¯ N ä¸ªè®­ç»ƒæ—¶æœŸåï¼Œæ‚¨ä¹Ÿå°†æ”¶åˆ°ä¸€ä¸ªéªŒè¯åˆ†æ•°ã€‚

# æ¦‚æ‹¬èµ·æ¥

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº† Sequence2Sequence Transformer èƒŒåçš„æ–¹æ³•ï¼Œæˆ–è€…è°·æ­Œå–œæ¬¢ç§°ä¹‹ä¸ºâ€œæ–‡æœ¬åˆ°æ–‡æœ¬â€è½¬æ¢å™¨(T5)ã€‚æˆ‘ä»¬è¿˜å¿«é€Ÿè¿è¡Œäº†æ•°æ®å‡†å¤‡æµç¨‹å’ŒåŸ¹è®­æµç¨‹ï¼Œä»¥ä¾¿è½»æ¾å¿«é€Ÿåœ°å¯¹é¢„åŸ¹è®­çš„ T5 è½¬æ¢å™¨è¿›è¡Œæ–‡æœ¬ç”Ÿæˆä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒã€‚å¸Œæœ›ä½ åœ¨å»ºé€ è¿™ä¸ªçš„è¿‡ç¨‹ä¸­å¾—åˆ°ä¸€äº›ä¹è¶£ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•è¿›è¡Œæ¨ç†å’Œéƒ¨ç½²è¿™ä¸ªæ¨¡å‹ã€‚

***Colab ç¬”è®°æœ¬:***

*   [æ•°æ®å‡†å¤‡](https://colab.research.google.com/drive/1Wgcqn_hFDyUIi5-IDUiJz3feeidjzI60?usp=sharing)
*   [åŸ¹è®­](https://colab.research.google.com/drive/1tVYHhqYbcNnF12309p2vvF2bDMvmWVa6?usp=sharing)