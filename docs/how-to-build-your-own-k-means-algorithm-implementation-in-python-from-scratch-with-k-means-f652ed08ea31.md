# å¦‚ä½•ç”¨ K-Means++åˆå§‹åŒ–ä»é›¶å¼€å§‹ç”¨ Python æ„å»ºè‡ªå·±çš„ K-Means ç®—æ³•å®ç°

> åŸæ–‡ï¼š<https://pub.towardsai.net/how-to-build-your-own-k-means-algorithm-implementation-in-python-from-scratch-with-k-means-f652ed08ea31?source=collection_archive---------5----------------------->

## æœ‰ä»€ä¹ˆæ¯”è‡ªå·±ä» 0 å¼€å§‹å®ç°æ›´å¥½çš„åŠ æ·±å¯¹ç®—æ³•åŸç†çŸ¥è¯†çš„æ–¹æ³•ï¼Ÿ

![](img/2755fb9ccfa0d8a6edf853ebf7a28337.png)

æ¢…å°”Â·æ™®å°”åœ¨ [Unsplash](https://unsplash.com/s/photos/clusters?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šçš„ç…§ç‰‡

## **ä»€ä¹ˆæ˜¯ K-Meansï¼Ÿ**

K-Means æ˜¯ä¸€ç§**æ— ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯** ï¼Œç”¨äºå°†å¤šä¸ª*ã€nã€‘ä¸ªè§‚å¯Ÿå€¼*æ‹†åˆ†æˆ*ã€kã€‘ä¸ªä¸åŒçš„èšç±»*ï¼Œå…¶ä¸­æ¯ä¸ªè§‚å¯Ÿå€¼å±äºè´¨å¿ƒæœ€è¿‘çš„èšç±»ã€‚ç»“æœå°†æ˜¯æ•°æ®é›†è¢«åˆ†å‰²æˆ *Voronoi å•å…ƒ*ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªè¦ç´ çš„æ•°æ®é›†ã€‚

![](img/b06254ec4106e0d4d4eacd8fa7a52859.png)

å›¾ä¸€ã€‚éšæœºç”Ÿæˆçš„äºŒç»´æ— æ ‡ç­¾æ•°æ®é›†ã€‚ä½œè€…æ’å›¾ã€‚

æ­£å¦‚æ‚¨åœ¨*å›¾ 1* ä¸­æ‰€æ³¨æ„åˆ°çš„ï¼Œè‚‰çœ¼å¾ˆå®¹æ˜“å°±å¯ä»¥çœ‹å‡ºè¿™ä¸ªæ•°æ®é›†å¯ä»¥è¢«åˆ’åˆ†æˆå…­ä¸ªä¸åŒçš„é›†ç¾¤*(æˆ–ç»„)*ã€‚ä½†æ˜¯ç®—æ³•å¦‚ä½•ç¡®å®šå“ªä¸ªè§‚æµ‹å€¼å±äºå“ªä¸ªèšç±»å‘¢ï¼Ÿ

ä¸ºäº†ç»™æ¯ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªç‰¹å®šçš„ç»„ï¼ŒK-Means ç®—æ³•éµå¾ªä»¥ä¸‹æ­¥éª¤:

1.  åˆå§‹åŒ–â€œkâ€ä¸ªè´¨å¿ƒï¼Œæ¯ä¸ªèšç±»ä¸€ä¸ªã€‚
2.  æ ¹æ®æœ€è¿‘çš„è´¨å¿ƒä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªèšç±»ã€‚
3.  åŸºäºæŒ‡å®šçš„ç‚¹é‡æ–°è®¡ç®—èšç±»ä¸­å¿ƒã€‚
4.  é‡å¤æ­¥éª¤ 2 å’Œ 3ï¼Œç›´åˆ°è´¨å¿ƒä¸å†æ”¹å˜ã€‚

ç§ï¼ŒK-Means å°±æ˜¯è¿™æ ·å¾—å‡ºæœ€ç»ˆç»“æœçš„ã€‚

## k-æ„å‘³ç€ç”¨ä¾‹

è¿™ç§èšç±»ç®—æ³•çš„ä¸€äº›æœ€å¸¸è§çš„ç”¨ä¾‹åŒ…æ‹¬è¯¸å¦‚ ***æœç´¢å¼•æ“ã€å¼‚å¸¸æ£€æµ‹*** å’Œ ***åŸºäºå…ˆå‰è¡Œä¸º(å…´è¶£ã€è´­ä¹°ç­‰)çš„å®¢æˆ·åˆ†å‰²*** ç­‰ä¸»é¢˜ã€‚)

## **è´¨å¿ƒçš„åˆå§‹åŒ–æ–¹æ³•**

æˆ‘ä»¬åº”è¯¥è€ƒè™‘çš„ä¸€ä»¶é‡è¦çš„äº‹æƒ…æ˜¯**æœ€ç»ˆçš„ç»“æœå°†å–å†³äº**å¯¹**èšç±»ä¸­å¿ƒçš„åˆå§‹åŒ–**æ–¹æ³•**ã€‚æœ€å¸¸ç”¨çš„ä¸¤ç§åˆå§‹åŒ–æ–¹æ³•æ˜¯*â€˜randomâ€™*å’Œ*â€˜k-means++â€™*ã€‚è¿™ä¸¤è€…ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯â€œk-means++â€è¯•å›¾æ¨åŠ¨è´¨å¿ƒå°½å¯èƒ½è¿œç¦»å½¼æ­¤ï¼Œè¿™æ„å‘³ç€**å®ƒä¼šæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆ**ã€‚**

å¯¹äºè¿™ä¸ªå®ç°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*â€˜k-means++â€™*ä½œä¸ºåˆå§‹åŒ–æ–¹æ³•ã€‚

## **å®ç°**

```
class myKMeans:
    def __init__(self, n_clusters, iters):
        """
        KMeans Class constructor.

        Args:
          n_clusters (int) : Number of clusters used for partitioning.
          iters (int) : Number of iterations until the algorithm stops.

        """
        self.n_clusters = n_clusters
        self.iters = iters

    def kmeans_plus_plus(self, X, n_clusters):
        pass

    def find_closest_centroids(self, X, centroids):
        pass

    def compute_centroids(self, X, idx, K):
        pass

    def fit_predict(self, X):
        pass
```

è¿™æ˜¯æˆ‘ä»¬å°†è¦æ„å»ºçš„ç±»çš„ç»“æ„ã€‚ *'kmeans_plus_plus()'ã€' find _ closed _ Centros()'*å’Œ*' compute _ Centros()'*æ–¹æ³•å°†åˆ†åˆ«æ‰§è¡Œç®—æ³•çš„ç¬¬ä¸€ã€ç¬¬äºŒå’Œç¬¬ä¸‰æ­¥ã€‚*â€˜æ‹Ÿåˆ _ é¢„æµ‹()â€™*æ–¹æ³•å°†è·å–æ•°æ®é›†å¹¶åœ¨æ ‡ç­¾ä¸Šè¿›è¡Œé¢„æµ‹ã€‚

```
def kmeans_plus_plus(self, X, n_clusters):
    """
    My implementation of the KMeans++ initialization method for computing the centroids.

    Args:
        X (ndarray): Dataset samples
        n_clusters (int): Number of clusters

    Returns:
        centroids (ndarray): Initial position of centroids
    """
    # Assign the first centroid to a random sample from the dataset.
    idx = random.randrange(len(X))
    centroids = [X[idx]]

    # For each cluster
    for _ in range(1, n_clusters):

        # Get the squared distance between that centroid and each sample in the dataset
        squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])

        # Convert the distances into probabilities that a specific sample could be the center of a new centroid
        proba = squared_distances / squared_distances.sum()

        for point, probability in enumerate(proba):
            # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.
            if probability == proba.max():
                centroid = point
                break

        centroids.append(X[centroid])

    return np.array(centroids)
```

*ä»¥ä¸ŠåŠŸèƒ½å®ç°äº† K-Means ç®—æ³•ç¬¬ä¸€æ­¥**(åˆå§‹åŒ–æ–¹æ³•)**ã€‚*

æˆ‘ä»¬ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸€ä¸ªæ ·æœ¬ï¼Œå¹¶å°†å…¶æŒ‡å®šä¸º ***ç¬¬ä¸€ä¸ªè´¨å¿ƒ*** ã€‚ç„¶å*é‡å¤è®¡ç®—*æ¯ä¸ªæ ·æœ¬*ä¸æ‰€æœ‰è´¨å¿ƒ*ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶å°†å‰©ä½™çš„æ‰€æœ‰è´¨å¿ƒåˆ†é…åˆ°è·ç¦»å…ˆå‰è®¡ç®—çš„ä¸­å¿ƒæœ€è¿œçš„*ç‚¹*ã€‚

```
def find_closest_centroids(self, X, centroids):
    """
    Computes the distance to the centroids and assigns the new label to each sample in the dataset.

    Args:
        X (ndarray): Dataset samples  
        centroids (ndarray): Number of clusters

    Returns:
        idx (ndarray): Closest centroids for each observation

    """

    # Set K as number of centroids
    K = centroids.shape[0]

    # Initialize the labels array to 0
    label = np.zeros(X.shape[0], dtype=int)

    # For each sample in the dataset
    for sample in range(len(X)):
        distance = []
        # Take every centroid
        for centroid in range(len(centroids)):
            # Compute Euclidean norm between a specific sample and a centroid
            norm = np.linalg.norm(X[sample] - centroids[centroid])
            distance.append(norm)

        # Assign the closest centroid as it's label
        label[sample] = distance.index(min(distance))

    return label
```

*ä¸Šçš„å‡½æ•°å®ç°äº† K-Means ç®—æ³•çš„ç¬¬äºŒæ­¥*(æ±‚æœ€è¿‘è´¨å¿ƒ)ã€‚

å¯¹äºæ•°æ®é›†ä¸­çš„*æ¯ä¸ªæ ·æœ¬*ï¼Œæˆ‘ä»¬å–*æ¯ä¸ªè´¨å¿ƒ*å’Œ**è®¡ç®—å®ƒä»¬ä¹‹é—´çš„æ¬§å‡ é‡Œå¾·èŒƒæ•°**ã€‚æˆ‘ä»¬å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†è§‚å¯Ÿç»“æœåˆ†é…ç»™*æœ€æ¥è¿‘çš„è´¨å¿ƒ*ã€‚

```
def compute_centroids(self, X, idx, K):
    """
    Returns the new centroids by computing the mean of the data points assigned to each centroid.

    Args:
        X (ndarray): Dataset samples 
        idx (ndarray): Closest centroids for each observation 
        K (int): Number of clusters

    Returns:
        centroids (ndarray): New centroids computed
    """

    # Number of samples and features
    m, n = X.shape

    # Initialize centroids to 0
    centroids = np.zeros((K, n))

    # For each centroid
    for k in range(K):   
        # Take all samples assigned to that specific centroid
        points = X[idx == k]
        # Compute their mean
        centroids[k] = np.mean(points, axis=0)

    return centroids
```

ä¸Šé¢çš„*åŠŸèƒ½å®ç°äº† K-Means ç®—æ³•çš„*ç¬¬ä¸‰æ­¥**(é‡æ–°è®¡ç®—æ–°çš„èšç±»ä¸­å¿ƒ)**ã€‚

å¯¹äº*çš„æ¯ä¸ªè´¨å¿ƒ*ï¼Œæˆ‘ä»¬å–*æ‰€æœ‰åˆ†é…ç»™è¯¥*ç‰¹å®šç»„çš„ç‚¹*å’Œ**è®¡ç®—å…¶å¹³å‡å€¼**ã€‚ç»“æœä¼šç»™æˆ‘ä»¬*æ–°çš„èšç±»ä¸­å¿ƒ*ã€‚*

```
def fit_predict(self, X):
    """
    My implementation of the KMeans algorithm.

    Args:
        X (ndarray): Dataset samples

    Returns:
        centroids (ndarray):  Computed centroids
        labels (ndarray):     Predicts for each sample in the dataset.
    """
    # Number of samples and features
    m, n = X.shape

    # Compute initial position of the centroids
    initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)

    centroids = initial_centroids   
    labels = np.zeros(m)

    prev_centroids = centroids

    # Run K-Means
    for i in range(self.iters):
        # For each example in X, assign it to the closest centroid
        labels = self.find_closest_centroids(X, centroids)

        # Given the memberships, compute new centroids
        centroids = self.compute_centroids(X, labels, self.n_clusters)

        # Check if centroids stopped changing positions
        if centroids.tolist() == prev_centroids.tolist():
            print(f'K-Means converged at {i+1} iterations')
            break
        else:
            prev_centroids = centroids

    return centroids, labels
```

æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œå°†è°ƒç”¨*' fit _ project()'*å‡½æ•°å¯¹æ•°æ®é›†ä¸­çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ã€‚

æœ€å*ï¼Œ*æ‚¨çš„ *K-Means ç±»*åº”è¯¥æ˜¯è¿™æ ·çš„**:**

```
class myKMeans:
    def __init__(self, n_clusters, iters):
        """
        KMeans Class constructor.

        Args:
          n_clusters (int) : Number of clusters used for partitioning.
          iters (int) : Number of iterations until the algorithm stops.

        """
        self.n_clusters = n_clusters
        self.iters = iters

    def kmeans_plus_plus(self, X, n_clusters):
        """
        My implementation of the KMeans++ initialization method for computing the centroids.

        Args:
            X (ndarray): Dataset samples
            n_clusters (int): Number of clusters

        Returns:
            centroids (ndarray): Initial position of centroids
        """
        # Assign the first centroid to a random sample from the dataset.
        idx = random.randrange(len(X))
        centroids = [X[idx]]

        # For each cluster
        for _ in range(1, n_clusters):

            # Get the squared distance between that centroid and each sample in the dataset
            squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])

            # Convert the distances into probabilities that a specific sample could be the center of a new centroid
            proba = squared_distances / squared_distances.sum()

            for point, probability in enumerate(proba):
                # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.
                if probability == proba.max():
                    centroid = point
                    break

            centroids.append(X[centroid])

        return np.array(centroids)

    def find_closest_centroids(self, X, centroids):
        """
        Computes the distance to the centroids and assigns the new label to each sample in the dataset.

        Args:
            X (ndarray): Dataset samples  
            centroids (ndarray): Number of clusters

        Returns:
            idx (ndarray): Closest centroids for each observation

        """

        # Set K as number of centroids
        K = centroids.shape[0]

        # Initialize the labels array to 0
        label = np.zeros(X.shape[0], dtype=int)

        # For each sample in the dataset
        for sample in range(len(X)):
            distance = []
            # Take every centroid
            for centroid in range(len(centroids)):
                # Compute Euclidean norm between a specific sample and a centroid
                norm = np.linalg.norm(X[sample] - centroids[centroid])
                distance.append(norm)

            # Assign the closest centroid as it's label
            label[sample] = distance.index(min(distance))

        return label

    def compute_centroids(self, X, idx, K):
        """
        Returns the new centroids by computing the mean of the data points assigned to each centroid.

        Args:
            X (ndarray): Dataset samples 
            idx (ndarray): Closest centroids for each observation 
            K (int): Number of clusters

        Returns:
            centroids (ndarray): New centroids computed
        """

        # Number of samples and features
        m, n = X.shape

        # Initialize centroids to 0
        centroids = np.zeros((K, n))

        # For each centroid
        for k in range(K):   
            # Take all samples assigned to that specific centroid
            points = X[idx == k]
            # Compute their mean
            centroids[k] = np.mean(points, axis=0)

        return centroids

    def fit_predict(self, X):
        """
        My implementation of the KMeans algorithm.

        Args:
            X (ndarray): Dataset samples

        Returns:
            centroids (ndarray):  Computed centroids
            labels (ndarray):     Predicts for each sample in the dataset.
        """
        # Number of samples and features
        m, n = X.shape

        # Compute initial position of the centroids
        initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)

        centroids = initial_centroids   
        labels = np.zeros(m)

        prev_centroids = centroids

        # Run K-Means
        for i in range(self.iters):
            # For each example in X, assign it to the closest centroid
            labels = self.find_closest_centroids(X, centroids)

            # Given the memberships, compute new centroids
            centroids = self.compute_centroids(X, labels, self.n_clusters)

            # Check if centroids stopped changing positions
            if centroids.tolist() == prev_centroids.tolist():
                print(f'K-Means converged at {i+1} iterations')
                break
            else:
                prev_centroids = centroids

        return labels, centroids
```

**é…·ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹**æˆ‘ä»¬çš„å®ç°çš„ç»“æœ**ä¸ç›¸æ¯”çœ‹èµ·æ¥æ˜¯æ€æ ·çš„(ç›¸æ¯”äº K-Means çš„ *sklearn* ç‰ˆæœ¬):**

```
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
```

**æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å¯¹å…¶æ‰§è¡Œèšç±»ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†ä½¿ç”¨ *'make_blobs()'* å‡½æ•°ä»*sklearn . dataset .*ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†**

```
from sklearn.datasets import make_blobs

# Generate 2D classification dataset
X, y = make_blobs(n_samples=1500, centers=6, n_features=2, random_state=67)
```

**ä¸Šé¢çš„ä»£ç ç‰‡æ®µå°†ä¸ºæˆ‘ä»¬ç”Ÿæˆä»¥ä¸‹æ•°æ®é›†:**

**![](img/b64b594c23ac5f797ad6497d027f0799.png)**

**å›¾ 2ã€‚éšæœºç”Ÿæˆçš„äºŒç»´æ ‡è®°æ•°æ®é›†ã€‚ä½œè€…çš„æ’å›¾ã€‚**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿è¡Œ K-Means çš„ä¸¤ä¸ªç‰ˆæœ¬(*æ‹¥æœ‰*å’Œ *sklearn* å®ç°)å¹¶çœ‹çœ‹å®ƒä»¬æ˜¯å¦‚ä½•è¿è¡Œçš„ã€‚**

```
# sklearn version of KMeans
kmeans = KMeans(n_clusters=5)
sklearn_labels = kmeans.fit_predict(X)
sklearn_centers = kmeans.cluster_centers_

# own implementation of KMeans
my_kmeans = myKMeans(5, 50)
mykmeans_labels, mykmeans_centers = my_kmeans.fit_predict(X)
```

**å¾ˆå¥½ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨è®ºï¼Œè®©æˆ‘ä»¬æŠŠå®ƒä»¬å’Œ Voronoi ç»†èƒä¸€èµ·å¯è§†åŒ–ã€‚ğŸ˜**

```
plt.figure(figsize=(12,4)) 
vor = Voronoi(sklearn_centers)
fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 1))
plt.subplot(1, 2, 1)
plt.title("sklearn KMeans Predicts")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.xlim([-13, 11])
plt.ylim([-14, 13])
plt.scatter(X[:, 0], X[:, 1], 4, c=sklearn_labels) 
plt.scatter(sklearn_centers[:, 0], sklearn_centers[:, 1], marker='x', c='red', s=50)
vor = Voronoi(mykmeans_centers)
fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 2))
plt.subplot(1, 2, 2)
plt.title("My KMeans Predicts")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.xlim([-13, 11])
plt.ylim([-14, 13])
plt.scatter(X[:, 0], X[:, 1], 4, c=mykmeans_labels) 
plt.scatter(mykmeans_centers[:, 0], mykmeans_centers[:, 1], marker='x', c='red', s=50)
plt.show()
```

**![](img/ee45e902cb2278b1d52220aaa346c282.png)**

**å›¾ 3ã€‚æˆ‘ä»¬ä»å¤´å¼€å§‹å®ç° K-Means å’Œ sklearn ç‰ˆæœ¬çš„æ¯”è¾ƒã€‚ä½œè€…æ’å›¾ã€‚**

**å“‡å“¦ã€‚å¦‚æœä½ é—®æˆ‘ï¼Œé‚£çœ‹èµ·æ¥çœŸçš„ä»¤äººå°è±¡æ·±åˆ» ã€‚ç»“æœåŸºæœ¬ç›¸åŒã€‚**

## **ç»“è®º**

**æ€»ä¹‹ï¼Œè¿™æˆ–å¤šæˆ–å°‘æ˜¯æ‚¨éœ€è¦äº†è§£çš„å…³äºè¿™ä¸ªå¼ºå¤§çš„èšç±»ç®—æ³•çš„ä¸€åˆ‡ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ äº†è§£ K-Means åŸåˆ™ã€‚æ„Ÿè°¢é˜…è¯»ï¼**

**å¦‚æœä½ å¯¹è¿™ç¯‡æ–‡ç« æœ‰ä»»ä½•æ„è§ï¼Œè¯·å†™åœ¨è¯„è®ºé‡Œï¼æˆ‘å¾ˆæƒ³è¯»è¯»å®ƒä»¬ğŸ˜‹**

## **å…³äºæˆ‘**

**å¤§å®¶å¥½ï¼Œæˆ‘å« Alexï¼Œæ˜¯ä¸€åå¹´è½»çƒ­æƒ…çš„æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦å­¦ç”Ÿã€‚**

**å¦‚æœä½ å–œæ¬¢çš„å†…å®¹ï¼Œè¯·è€ƒè™‘ä¸‹é™å…³æ³¨å’Œé¼“æŒï¼Œå› ä¸ºä»–ä»¬çœŸçš„å¾ˆæ„Ÿæ¿€ã€‚æ­¤å¤–ï¼Œè¯·éšæ—¶åœ¨ [LinkedIn](https://www.linkedin.com/in/alexandru-florin-belengeanu-74b1a3128/) ä¸Šä¸æˆ‘è”ç³»ï¼Œä»¥ä¾¿è·å¾—ä¸€äº›å…³äºæœºå™¨å­¦ä¹ ç›¸å…³ä¸»é¢˜çš„æ¯å‘¨è§è§£ã€‚**

## **å‚è€ƒ**

**[1]å¤§å«Â·äºšç‘Ÿå’Œè°¢å°”ç›–Â·ç“¦è¥¿é‡Œç»´èŒ¨åŸºï¼Œ [k-means++ç²¾å¿ƒæ’­ç§çš„ä¼˜åŠ¿](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) (2007)ï¼Œ[http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)**

**[2][https://en . Wikipedia . org/wiki/K-means _ clustering #åº”ç”¨](https://en.wikipedia.org/wiki/K-means_clustering#Applications)**

**[3][https://www . kdnuggets . com/2020/06/centroid-initial ization-k-means-clustering . html](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)**