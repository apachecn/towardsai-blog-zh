# å¦‚ä½•ç”¨ Python åˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„ TF-IDF å…³é”®è¯ç ”ç©¶å·¥å…·

> åŸæ–‡ï¼š<https://pub.towardsai.net/how-to-create-a-powerful-tf-idf-keyword-research-tool-in-python-f23c8265279e?source=collection_archive---------2----------------------->

## [ç¼–ç¨‹](https://towardsai.net/p/category/programming)

![](img/0c5605aca3497e588ba9b5d2d14189c1.png)

æˆ‘ä»¬æ­£å¤„äºæ•°å­—è¥é”€çš„æ—¶ä»£ï¼Œç°åœ¨**è¿™ä¸ªè¯**æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´é‡è¦ã€‚æ•°å­—è¥é”€æœ€æˆåŠŸçš„æŠ€æœ¯ä¹‹ä¸€æ˜¯ç«äº‰åˆ†æå’Œå…³é”®è¯ç ”ç©¶ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬çš„ç«äº‰å¯¹æ‰‹åœ¨è°ˆè®ºä»€ä¹ˆã€‚è¿™æ˜¯æœ€æœ‰ç”¨çš„æœç´¢å¼•æ“ä¼˜åŒ–ï¼Œä½†ä¹Ÿä¸ºåšå®¢å¸–å­çš„æƒ³æ³•ç­‰ã€‚

# ç¬¬ä¸€æ­¥:ä»ç½‘ç«™ä¸Šè·å–æ–‡æœ¬

åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä» URL ä¸­æå–å¹²å‡€æ–‡æœ¬çš„å‡½æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥åœ¨ä»¥åçš„åˆ†æä¸­ä½¿ç”¨å®ƒã€‚

```
import pandas as pd
import numpy as np
import urllib
from fake_useragent import UserAgent
import requests
import re
from urllib.request import Request, urlopen
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import math
from nltk.corpus import stopwords
stopWords = list(set(stopwords.words('english')))
from bs4 import BeautifulSoup

def get_text(url):
    try:
        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urlopen(req,timeout=5).read()
        soup = BeautifulSoup(webpage, "html.parser")
        texts = soup.findAll(text=True)
        res=u" ".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])
        return(res)
    except:
        return False
```

æˆ‘ä»¬æ¥ä¸¾ä¸ªä¾‹å­ã€‚

```
get_text('https://en.wikipedia.org/wiki/Machine_learning')[0:500]
#It will return the first 500 characters CentralNotice Machine learning From Wikipedia, the free encyclopedia Jump to navigation Jump to search For the journal, see Machine Learning (journal) . "Statistical learning" redirects here. For statistical learning in linguistics, see statistical learning in language acquisition . Scientific study of algorithms and statistical models that computer systems use to perform tasks without explicit instructions Part of a series on Machine learning and data mining Problems Class'
```

æˆåŠŸï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»ç½‘ç«™ä¸Šè·å¾—å¹²å‡€çš„æ–‡æœ¬ï¼ä½†æ˜¯æˆ‘ä»¬åˆ°åº•è¯¥å¦‚ä½•ä½¿ç”¨å®ƒå‘¢ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€ç« ã€‚

# ç¬¬äºŒæ­¥:è·å–ç«äº‰å¯¹æ‰‹çš„ç½‘å€

æ‰¾åˆ°æœ€ä½³ç«äº‰å¯¹æ‰‹çš„æœ€å¥½æ–¹æ³•æ˜¯åœ¨è°·æ­Œæœç´¢ä¸­è·å¾—æˆ‘ä»¬æ„Ÿå…´è¶£çš„å…³é”®è¯çš„é¡¶éƒ¨ç»“æœã€‚æˆ‘ä»¬å°†ä½¿ç”¨å‰ä¸€ç¯‡æ–‡ç« ä¸­çš„ä»£ç ï¼Œ[å¦‚ä½•ä½¿ç”¨ Python](https://predictivehacks.com/how-to-scrape-google-results-for-free-using-python/) å…è´¹æŠ“å–è°·æ­Œç»“æœã€‚

```
def google_results(keyword, n_results):
    query = keyword
    query = urllib.parse.quote_plus(query) # Format into URL encoding
    number_result = n_results
    ua = UserAgent()
    google_url = "https://www.google.com/search?q=" + query + "&num=" + str(number_result)
    response = requests.get(google_url, {"User-Agent": ua.random})
    soup = BeautifulSoup(response.text, "html.parser")
    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})
    results=[re.search('\/url\?q\=(.*)\&sa',str(i.find('a', href = True)['href'])) for i in result_div if "url" in str(i)]
    links=[i.group(1) for i in results if i != None]
    return (links)
```

æ¯”æ–¹è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›çœ‹åˆ°â€œæœºå™¨å­¦ä¹ åšå®¢â€è¿™ä¸ªå…³é”®è¯çš„â€œç«äº‰å¯¹æ‰‹â€ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ google results å‡½æ•°æ¥è·å–æ’åé å‰çš„ URLï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå˜é‡æ˜¯å…³é”®å­—ï¼Œç¬¬äºŒä¸ªå˜é‡æ˜¯ç»“æœçš„æ•°é‡ã€‚

```
google_results('machine learning blog',10)['https://towardsai.net/p/machine-learning/best-machine-learning-blogs-6730ea2df3bd', 'https://machinelearningmastery.com/blog/', 'https://towardsdatascience.com/how-to-start-a-machine-learning-blog-in-a-month-7eaf84692df9', 'http://ai.googleblog.com/', 'https://www.springboard.com/blog/machine-learning-blog/', 'https://blog.ml.cmu.edu/', 'https://blog.feedspot.com/machine_learning_blogs/', 'https://aws.amazon.com/blogs/machine-learning/', 'https://neptune.ai/blog/the-best-regularly-updated-machine-learning-blogs-or-resources', 'https://www.stxnext.com/blog/best-machine-learning-blogs-resources/']
```

# ç¬¬ä¸‰æ­¥:åˆ†æè¯¾æ–‡ï¼Œæ‰¾å‡ºæœ€é‡è¦çš„å•è¯ã€‚

è®©æˆ‘ä»¬æƒ³æƒ³ã€‚æœ€é‡è¦çš„è¯æ˜¯ä»€ä¹ˆï¼Ÿåœ¨æˆ‘ä»¬çš„åˆ†æä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ 3 ä¸ªæŒ‡æ ‡ã€‚**å¹³å‡ TF-IDFã€æœ€å¤§ TF-IDF** å’Œ**é¢‘ç‡ã€‚**ç®¡é“å¦‚ä¸‹ã€‚æˆ‘ä»¬å°†è·å¾—æ¯ä¸ªç½‘ç«™çš„æ–‡æœ¬(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯å‰ 12 ä¸ªç»“æœ),å¹¶å°†å®ƒä»¬ç”¨ä½œ TF-IDF çŸ¢é‡å™¨çš„è¯­æ–™åº“ã€‚ç„¶åä»è¿™ä¸ªçŸ©é˜µä¸­ï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ¯ä¸ªå•è¯çš„å¹³å‡å’Œæœ€å¤§ TF-IDF åˆ†æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ä» TF-IDF çŸ©é˜µä¸­è·å¾—é¢‘ç‡ï¼Œæ–¹æ³•æ˜¯è¯´å¦‚æœè¿™ä¸ªè¯åœ¨è¿™ä¸€è¡Œä¸­ä¸ç­‰äºé›¶ï¼Œå®ƒå°±åŒ…å«åœ¨ URL ä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨è®¡ç®—å®ƒçš„ç™¾åˆ†æ¯”ã€‚å®Œæ•´çš„å‡½æ•°å¦‚ä¸‹ã€‚

```
def tf_idf_analysis(keyword):
    links=google_results(keyword,12)
    text=[]
    for i in links:
        t=get_text(i)
        if t:
            text.append(t)

    v = TfidfVectorizer(min_df=2,analyzer='word',ngram_range=(1,2),stop_words=stopWords)
    x = v.fit_transform(text)

    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names())
    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)

    tf=pd.DataFrame((f>0).sum(axis=0))

    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')

    d.columns=['word','average_tfidf','max_tfidf','frequency']

#you can comment the following part if you want the number of URLs #that the word occurs. The percentage makes sense
#when we have a lot of URLs to check

    d['frequency']=round((d['frequency']/len(text))*100)

    return(d)
```

ç°åœ¨æˆ‘ä»¬çš„æœ€ç»ˆå‡½æ•°å·²ç»å‡†å¤‡å¥½äº†ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨**æœºå™¨å­¦ä¹ åšå®¢**å…³é”®å­—æ¥çœ‹çœ‹æˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ æ–¹é¢çš„ç«äº‰å¯¹æ‰‹ã€‚

![](img/0586950f38efcea9ebbe04b06efd9046.png)

```
x= tf_idf_analysis('machine learning blog')

#remove the numbers and sort by max tfidf and get the top20 words
x[x['word'].str.isalpha()].sort_values('max_tfidf',ascending=False).head(20)word  average_tfidf  max_tfidf  frequency
929      google       0.098790   0.626160       67.0
254         aws       0.052512   0.550785       25.0
171      amazon       0.060131   0.537993       33.0
1472      model       0.058276   0.521179       33.0
307        blog       0.131429   0.385008      100.0
133          ai       0.109516   0.358522       83.0
1222   learning       0.191090   0.352528      100.0
717         end       0.036682   0.304649       58.0
1332    machine       0.158022   0.295191      100.0
525     cookies       0.023013   0.263509       17.0
1980        see       0.030134   0.255031       58.0
439         cmu       0.028235   0.253162       17.0
2242    towards       0.035054   0.245614       42.0
862   followers       0.022837   0.245576       17.0
1949    science       0.057179   0.240060       58.0
670      domain       0.021410   0.236214       25.0
739       entry       0.022586   0.233097       17.0
944    gradient       0.019838   0.233097       17.0
377    brownlee       0.021105   0.226498       25.0
537     courses       0.024935   0.218224       25.0
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå…¶ä¸­ä¸€ä¸ªçƒ­é—¨è¯æ±‡æ˜¯äºšé©¬é€Š AWSã€‚å—¯ï¼Œä¹Ÿè®¸æˆ‘ä»¬ä¹Ÿåº”è¯¥å†™ç‚¹ä»€ä¹ˆğŸ˜‰ã€‚è¿™å¯èƒ½æ˜¯æ•°å­—è¥é”€çš„ä¸€ä¸ªå¼ºå¤§å·¥å…·ï¼Œæœ‰è®¸å¤šä»˜è´¹æœåŠ¡æ­£åœ¨è¿™æ ·åšã€‚æ‰€ä»¥å¼€å§‹å°è¯•å§ï¼

*åŸè½½äº*[*https://predictivehacks.com*](https://predictivehacks.com/how-to-create-a-powerful-tf-idf-keyword-research-tool/)*ã€‚*