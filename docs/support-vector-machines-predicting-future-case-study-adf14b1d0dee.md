# æ”¯æŒå‘é‡æœº:é¢„æµ‹æœªæ¥-æ¡ˆä¾‹ç ”ç©¶

> åŸæ–‡ï¼š<https://pub.towardsai.net/support-vector-machines-predicting-future-case-study-adf14b1d0dee?source=collection_archive---------4----------------------->

## ç›‘ç£å­¦ä¹ æ–¹æ³•çš„å»¶ç»­:ç¬¬ä¸‰éƒ¨åˆ†

æ­£å¦‚ä¹‹å‰åœ¨[æ”¯æŒå‘é‡æœºâ€”â€”ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/towards-artificial-intelligence/support-vector-machines-a78afdc9357f)ä¸­æ‰¿è¯ºçš„ï¼Œè®©æˆ‘ä»¬æ¥è°ˆè°ˆä¸€ä¸ªæƒŠäººçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ†æå’Œç†è§£æ”¯æŒå‘é‡åœ¨å®é™…ä¸šåŠ¡é—®é¢˜ä¸­çš„åº”ç”¨ï¼Œå¹¶ä¸ºæ²¡æœ‰äººçœŸæ­£çœ‹åˆ°çš„æƒŠäººç»“æœå’Œé¢„æµ‹åšå¥½å‡†å¤‡ã€‚

![](img/f3c01dc15fbcb4112575fc42290852de.png)

å›¾ç‰‡æ¥æº:ã€https://giphy.com/ 

## é—®é¢˜é™ˆè¿°:

åœ¨è¿™ä¸ªé—®é¢˜é™ˆè¿°ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä¸€ä¸ªæ¡ˆä¾‹ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®å½±å“è‚ç‚çš„è¯Šæ–­å› ç´ æ¥å°è¯•é¢„æµ‹è¿™ä¸ªäººæ˜¯å¦èƒ½å¤Ÿå­˜æ´»ã€‚

å…ˆè¯´ä¸€ä¸‹æˆ‘ä»¬è¦ç”¨çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«äººç¾¤ä¸­è‚ç‚çš„å‘ç”Ÿç‡ã€‚

## è¿™ä¸ªæ•°æ®é›†çš„æ¥æºå‘¢ï¼Ÿ

![](img/7ed8685ba80f351ae3ae3d1d2a31538b.png)

å›¾ç‰‡æ¥æº:ã€https://tenor.com/ +Photoshop

UCI æœºå™¨å­¦ä¹ çŸ¥è¯†åº“è¢«ç”¨æ¥è·å¾—è¿™ä¸ªæ•°æ®é›†..å®ƒæœ‰ä¸¤ç§ä¸åŒç±»å‹çš„ 155 ä¸ªè®°å½•ï¼Œå…¶ä¸­ 32 ä¸ªæ˜¯æ­»äº¡è®°å½•ï¼Œ123 ä¸ªæ˜¯ç°åœºè®°å½•ã€‚æ•°æ®é›†ä¸­æœ‰ 20 ä¸ªç‰¹å¾(14 ä¸ªäºŒå…ƒå±æ€§å’Œ 6 ä¸ªæ•°å€¼å±æ€§)

åœ¨æœ¬æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¤šç§æ–¹æ³•ï¼Œæ ¹æ®å½±å“è‚ç‚çš„è¯Šæ–­å› ç´ å’Œæ­£ç¡®çš„è¯¯å·®æŒ‡æ ‡ï¼ŒæˆåŠŸé¢„æµ‹æ‚£è€…èƒ½å¦å­˜æ´»ã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯æ··æ·†çŸ©é˜µã€‚

å¦‚æœä½ å¯¹è¿™ä¸ªæ¨é”€ä¸æ¸…æ¥šï¼Œè¯·å‚è€ƒæˆ‘ä»¬ä¹‹å‰å…³äº[æ··æ·†æŒ‡æ ‡](https://medium.com/towards-artificial-intelligence/world-of-classification-in-machine-learning-a3c1f008b1fc)çš„åšæ–‡ã€‚(å½’å…¥åˆ†ç±»åšå®¢ä¸–ç•Œ)

![](img/e0076a647294ce6259d74eb0d2553ff9.png)

å›¾ç‰‡æ¥æº:[https://lwmachinenglearning . WordPress . com/portfolio/unbalanced-data-credit-card-fraud-detection/](https://lwmachinelearning.wordpress.com/portfolio/unbalanced-data-credit-card-fraud-detection/)

## è®©æˆ‘ä»¬ä»å®é™…éƒ¨åˆ†å¼€å§‹:

![](img/197836c79d83012ec04bef5bae175b62.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

## æ­¥éª¤ 1:åŠ è½½å¿…éœ€çš„å’Œå¼ºåˆ¶çš„åº“:

```
#THIS WILL HELP US IGNORE THE WARNINGS WHILE RUNNING OUR CODE
import warnings
warnings.filterwarnings("ignore")
```

```
 import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

from sklearn.impute import SimpleImputer

from sklearn.svm import SVC

from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score

from sklearn.model_selection import GridSearchCV
```

## æ­¥éª¤ 2:è¯»å–è‚ç‚æ•°æ®é›†:

```
data = pd.read_csv("/content/hepatitis.csv")
```

## æ¢ç´¢æ€§æ•°æ®åˆ†æ:

***é‡è¦æ€§:***EDA æ˜¯ä¸€ç§æ—¨åœ¨æ­ç¤ºæ•°æ®é›†åº•å±‚ç»“æ„çš„è¯¦ç»†åˆ†æã€‚å®ƒå¯¹ä¸šåŠ¡é—®é¢˜å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒæ­ç¤ºäº†è¶‹åŠ¿ã€æ¨¡å¼å’Œä¸æ˜æ˜¾çš„è”ç³»ã€‚

```
#Checking the dimensions (rows and columns)
data.shape

#Checking the datatypes of each variable
data.dtypes

#Checking the head of the data (i.e top 5 rows)
data.head()

#Checking the basic summary statistics
data.describe()
#Checking the number of unique levels in each attribute
data.nunique()

#Target attribute Distribution
data.target.value_counts()

data.target.value_counts(normalize=True)*100
```

## ç¬¬ä¸‰æ­¥:æ•°æ®é¢„å¤„ç†:

**ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å‡†ç¡®åœ°é¢„å¤„ç†æ•°æ®ï¼Ÿ**

![](img/d99c4a6c3c24e0748b407b91d6dadc43.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

æ¯å½“æˆ‘ä»¬å†³å®šä½¿ç”¨æ•°æ®æ—¶ï¼Œç¬¬ä¸€æ­¥å°±æ˜¯æ”¶é›†æ•°æ®ï¼Œè¿™äº›æ•°æ®é€šå¸¸å¤„äºæœªåˆ†ç±»å’Œæœªæ¸…ç†çš„çŠ¶æ€ã€‚ä¸€æ—¦æˆ‘ä»¬å¼€å§‹å¤„ç†è¿™äº›æ•°æ®ï¼Œæ•°æ®ç§‘å­¦å®¶å°±å¾ˆéš¾é€šè¿‡è¿™ç§ç±»å‹çš„æ•°æ®æ‰¾åˆ°æ¸…æ™°çš„æ¨¡å¼å’Œç»“æœï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è®¸å¤šè¯¯æŠ¥ã€æ¼æŠ¥å’Œæ··æ·†ã€‚

å› æ­¤ï¼Œä¸ºäº†é˜²æ­¢è¿™ç§æ··ä¹±ï¼Œæˆ‘ä»¬å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…ç†å’Œé¢„å¤„ç†ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬è¿˜æ¶ˆé™¤äº†ç¼ºå¤±(å³æ•°æ®ä¸­çš„é›¶ç©ºé—´)æˆ–ä¸ä¸€è‡´çš„æ•°æ®å€¼ï¼Œä»¥å…è®¸ç®—æ³•æˆ–æ¨¡å‹å¹³ç¨³è¿è¡Œï¼Œè€Œä¸ä¼šé‡åˆ°ä»»ä½•é‡å¤§é”™è¯¯å€¼ã€‚

ä¸ºäº†ä½¿åŸå§‹æ•°æ®æ›´æ˜“äºç†è§£ã€å®ç”¨å’Œæœ‰æ•ˆï¼Œæ•°æ®é¢„å¤„ç†ä¹Ÿè¢«è®¤ä¸ºæ˜¯æ•°æ®æŒ–æ˜ä¸­çš„ä¸€ä¸ªé‡è¦æ–¹æ³•ã€‚è¿™æ•´ä¸ªæ•°æ®é¢„å¤„ç†è¿‡ç¨‹æœ‰åŠ©äºæ”¹å–„æˆ‘ä»¬çš„ç»“æœã€‚

```
#Let's drop the columns which are not that signicant and in use
data.drop(["ID"], axis = 1, inplace=True)
```

```
#Storing categorical and numerical values:
num_cols = ["age", "bili", "alk", "sgot", "albu", "protime"]
```

```
cat_cols = ['gender', 'steroid', 'antivirals', 'fatigue', 'malaise', 'anorexia', 'liverBig',
           'liverFirm', 'spleen', 'spiders', 'ascites', 'varices', 'histology']
```

```
 #Checking the head of dataset once again to see how dataframe looks
data.head()
```

```
 #Converting the attributes into appropriate type to avoid the future error
data[cat_cols] = data[cat_cols].astype('category')
```

```
#After converting the attribute types check the datatypes to be sure once again
data.dtypes
```

## æ­¥éª¤ 4:å°†æ•°æ®åˆ†ä¸ºâ€œXâ€å’Œâ€œYâ€:

```
 #Time to split the data into X and Y
X = data.drop(["target"], axis = 1)
y = data["target"]
```

```
#Getting the shape of data
print(X.shape, y.shape)
```

```
#Training the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123, stratify=y)
```

```
#Getting the shape of trained data to find the difference between untrained and trained data.
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```

```
#Check for distribution target variables
y_train.value_counts()

y_train.value_counts(normalize=True)*100
```

## æ­¥éª¤ 5:æ•°æ®é¢„å¤„ç†åœ¨å°†æ•°æ®åˆ†æˆâ€œXâ€å’Œâ€œYâ€ä¹‹å:

```
#Checking the null values
X_train.isna().sum()

X_test.isna().sum()
```

## æ¨¡å¼ä¸ºçš„æ’è¡¥ç¼ºå¤±åˆ†ç±»åˆ—:

```
df_cat_train = X_train[cat_cols]
df_cat_test = X_test[cat_cols]

cat_imputer = SimpleImputer(strategy='most_frequent')
cat_imputer.fit(df_cat_train)
```

```
df_cat_train = pd.DataFrame(cat_imputer.transform(df_cat_train), columns=cat_cols)
df_cat_test = pd.DataFrame(cat_imputer.transform(df_cat_test), columns=cat_cols)

df_num_train = X_train[num_cols]
df_num_test = X_test[num_cols]
```

## ç”¨ä¸­ä½æ•°æ’è¡¥ç¼ºå¤±çš„æ•°å­—åˆ—:

```
num_imputer = SimpleImputer(strategy='median')
num_imputer.fit(df_num_train[num_cols])
```

```
 df_num_train = pd.DataFrame(num_imputer.transform(df_num_train), columns=num_cols)
df_num_test =  pd.DataFrame(num_imputer.transform(df_num_test), columns=num_cols)
```

## ç°åœ¨ï¼Œå°†ä¼°ç®—çš„åˆ†ç±»åˆ—å’Œæ•°å­—åˆ—ç»“åˆèµ·æ¥:

```
 # Combine numeric and categorical in train
X_train = pd.concat([df_num_train, df_cat_train], axis = 1)

# Combine numeric and categorical in test
X_test = pd.concat([df_num_test, df_cat_test], axis = 1)
```

## æ ‡å‡†åŒ–æ•°å­—å±æ€§:

![](img/941752e7a80cded1edd9ed18d2694e09.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

ç”±äºæˆ‘ä»¬é‡‡ç”¨çš„æ–¹æ³•å‡è®¾äº†å„ç§å½¢å¼çš„åˆ†å¸ƒï¼Œå¦‚çº¿æ€§å’Œé€»è¾‘å›å½’ï¼Œå½“æˆ‘ä»¬çš„æ•°æ®å…·æœ‰ä¸åŒçš„è§„æ¨¡æ—¶ï¼Œæ ‡å‡†åŒ–æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„ç­–ç•¥ã€‚

å½“å›å½’æ¨¡å‹ä½¿ç”¨è¡¨ç¤ºä¸ºå¤šé¡¹å¼æˆ–äº¤äº’çš„å˜é‡æ—¶ï¼Œæ•°æ®ç§‘å­¦å®¶é€šå¸¸ä¼šå°†è¯¥æ¨¡å‹çš„æ•°æ®æ ‡å‡†åŒ–ã€‚ç”±äºè¿™äº›æœ¯è¯­éå¸¸é‡è¦ï¼Œèƒ½å¤Ÿæ­ç¤ºå“åº”å› å­å’Œé¢„æµ‹å› å­ä¹‹é—´çš„è”ç³»ï¼Œå› æ­¤å®ƒä»¬ä¹Ÿå¯èƒ½å¯¼è‡´é«˜åº¦çš„å¤šé‡å…±çº¿æ€§ã€‚

```
 scaler = StandardScaler()
scaler.fit(X_train[num_cols])
```

```
X_train_std = scaler.transform(X_train[num_cols])
X_test_std = scaler.transform(X_test[num_cols])
```

```
print(X_train_std.shape)
print(X_test_std.shape)
```

## ONEHOTENCODER:å°†åˆ†ç±»å±æ€§è½¬æ¢ä¸ºæ•°å­—å±æ€§:

***ä¸ºä»€ä¹ˆï¼Ÿ***

æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºå˜é‡éƒ½å¿…é¡»æ˜¯æ•°å­—ã€‚è¿™æ„å‘³ç€ï¼Œä¸ºäº†æ‹Ÿåˆå’Œè¯„ä¼°æ¨¡å‹ï¼Œåˆ†ç±»æ•°æ®å¿…é¡»é¦–å…ˆç¼–ç ä¸ºæ•°æ®ä¸­çš„æ•°å­—ã€‚

![](img/f50fbaf9c097d94150b8066c73b53368.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

```
 enc = OneHotEncoder(drop = 'first')
enc.fit(X_train[cat_cols])
```

```
X_train_ohe=enc.transform(X_train[cat_cols]).toarray()
X_test_ohe=enc.transform(X_test[cat_cols]).toarray()
```

## è¿æ¥å±æ€§:

ä½¿ç”¨ä¸€é”®ç¼–ç æ ‡å‡†åŒ–æ•°å­—å±æ€§å’Œåˆ†ç±»å±æ€§ã€‚

```
X_train_con = np.concatenate([X_train_std, X_train_ohe], axis=1)
X_test_con = np.concatenate([X_test_std, X_test_ohe], axis=1)
```

```
print(X_train_con.shape)
print(X_test_con.shape)
```

## ç¬¬å…­æ­¥:æœ€åç”¨çº¿æ€§ SVM å»ºæ¨¡:

**ä½¿ç”¨çº¿æ€§å†…æ ¸åˆ›å»º SVC åˆ†ç±»å™¨:**

```
linear_svm = SVC(kernel='linear', C=1)
```

```
#Training the classifier
linear_svm.fit(X=X_train, y= y_train)
```

```
#Predicting the results
train_predictions = linear_svm.predict(X_train)
test_predictions = linear_svm.predict(X_test)
```

## è¯¯å·®çŸ©é˜µ:

æœ‰åŠ©äºç¡®å®šå’Œé¢„æµ‹åˆ†ç±»æ¨¡å‹å¯è¡Œæ€§çš„è¯„ä¼°è¿‡ç¨‹ç§°ä¸ºæ··æ·†çŸ©é˜µï¼Œä¹Ÿç§°ä¸ºè¯¯å·®çŸ©é˜µã€‚é€šè¿‡ä½¿ç”¨æ··æ·†çŸ©é˜µï¼Œæ‚¨å¯ä»¥è§‚å¯Ÿåˆ°è®¸å¤šé¢„æµ‹é”™è¯¯ã€‚

![](img/1643f7026cdd2db52e55ccb50c12651b.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

```
#Defining the error matrix
def evaluate_model(act, pred):
   print("Confusion Matrix \n", confusion_matrix(act, pred))
   print("Accuracy : ", accuracy_score(act, pred))
   print("Recall   : ", recall_score(act, pred))
   print("Precision: ", precision_score(act, pred))
   print("F1_score : ", f1_score(act, pred))
```

```
### Train data accuracy
evaluate_model(y_train, train_predictions)

### Test data accuracy
evaluate_model(y_test, test_predictions)
```

è™½ç„¶æˆ‘å¾ˆå–œæ¬¢ä¸ºä½ ä»¬å†™æ–‡ç« ï¼Œä½†æˆ‘å¸Œæœ›ä½ ä»¬ä¹Ÿå–œæ¬¢å®æ–½è¿™ä¸ªæ¡ˆä¾‹å¹¶ä»ä¸­å­¦ä¹ ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦æ•°æ®é›†æºæˆ– GitHub gist çš„å¸®åŠ©(å¦‚æœæ‚¨å¯¹éƒ¨åˆ†ä»£ç æœ‰å›°éš¾)ï¼Œè¯·è”ç³»ï¼›æˆ‘ä»¬éå¸¸ä¹æ„å¸®å¿™ã€‚ğŸ˜â¤ï¸

![](img/a5ade4879643e2b730813df421182a80.png)

å›¾ç‰‡æ¥æº:[https://giphy.com/](https://giphy.com/)

## ç»§ç»­é¢„è§ï¼Œå­¦ä¹ ï¼Œæ¢ç´¢ï¼â¤ï¸

## å…³æ³¨æˆ‘ä»¬ï¼Œäº«å—å­¦ä¹ æ•°æ®ç§‘å­¦åšå®¢å’Œæ–‡ç« çš„ä¹è¶£:ğŸ’™

**é¢†è‹±:**ã€https://www.linkedin.com/company/dsmcs/ã€‘T4

**insta gram:**https://www.instagram.com/datasciencemeetscybersecurity/?hl=en

**GITHUB:**[https://github.com/Vidhi1290](https://github.com/Vidhi1290)

**æ¨ç‰¹:**[https://twitter.com/VidhiWaghela](https://twitter.com/VidhiWaghela)

**ä¸­ç­‰:**https://medium.com/@datasciencemeetscybersecurity-

**ç½‘å€:**[https://datasciencemeetscybersecurity.blogspot.com/](https://datasciencemeetscybersecurity.blogspot.com/)

**â€”å›¢é˜Ÿæ•°æ®ç§‘å­¦ä¸ç½‘ç»œå®‰å…¨â¤ï¸ç›¸é‡ğŸ’™**