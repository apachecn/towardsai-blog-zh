# è¿™å°±æ˜¯å¦‚ä½•ä½¿ç”¨ Apache Spark æ„å»ºå®¢æˆ·æµå¤±é¢„æµ‹æ¨¡å‹

> åŸæ–‡ï¼š<https://pub.towardsai.net/this-is-how-you-can-build-a-churn-prediction-model-using-spark-e187b7eca339?source=collection_archive---------3----------------------->

## ä¸€ä¸ªå…³äºå¦‚ä½•ä»…ä½¿ç”¨ Apache Spark æ„å»ºæµå¤±é¢„æµ‹ç®¡é“çš„ç«¯åˆ°ç«¯æ•™ç¨‹ã€‚

![](img/bb5e1d0b78864e5fe131be095b303b58.png)

ç”±ä½œè€…åˆ›å»ºçš„å›¾åƒå…·æœ‰ç¨³å®šçš„æ‰©æ•£æ€§ã€‚

è¿™ç¯‡æ–‡ç« æ˜¯å…³äºå¦‚ä½•ä½¿ç”¨æ¥è‡ª **Spark** çš„ ML æ ˆ**æ„å»ºæµå¤±é¢„æµ‹åˆ†ç±»å™¨**çš„**æ•™ç¨‹**ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªä¸€å®¶åä¸º **Sparkify** çš„**è™šæ‹Ÿå…¬å¸**çš„**æ•°æ®ï¼Œè¿™æ˜¯ä¸€å®¶éŸ³ä¹æµåª’ä½“å…¬å¸ã€‚æ•°æ®é›†åŒ…å«ä¸å¹³å°äº¤äº’çš„ç”¨æˆ·åˆ›å»ºçš„å„ç§äº‹ä»¶ã€‚**

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»”ç»†æŸ¥çœ‹è¿™äº›æ•°æ®ã€‚ä½†æ˜¯ï¼Œæˆ‘è¦æ„Ÿè°¢ *Udacity* å…¬å¼€äº†è¿™äº›æ•°æ®ã€‚æ²¡æœ‰ä»–ä»¬ï¼Œæˆ‘åšä¸å‡ºè¿™ä¸ªæ•™ç¨‹ã€‚

[åœ¨è¿™é‡Œ](https://udacity-dsnd.s3.amazonaws.com/sparkify/mini_sparkify_event_data.json)ä½ å¯ä»¥**ä¸‹è½½**æ•°æ®é›†çš„è¿·ä½ ç‰ˆæœ¬ï¼Œåœ¨è¿™é‡Œ[ä¸‹è½½](https://udacity-dsnd.s3.amazonaws.com/sparkify/sparkify_event_data.json)æ•´ä¸ªæ•°æ®é›†ã€‚

## ä»€ä¹ˆæ˜¯å®¢æˆ·æµå¤±ï¼Ÿ

å®¢æˆ·æµå¤±æ˜¯æŒ‡æœ‰äººé€‰æ‹©åœæ­¢ä½¿ç”¨ä½ çš„äº§å“æˆ–æœåŠ¡ã€‚

## è¿™æ ·çš„æ¨¡å‹ä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ

é€šè¿‡å»ºç«‹è¿™æ ·ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£ç”¨æˆ·ç¦»å¼€å…¬å¸çš„åŸå› ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç”¨æˆ·æ´»åŠ¨æ•°æ®æ”¹å–„å®¢æˆ·ä½“éªŒå’Œä¿ç•™çš„æ–¹æ³•ã€‚

# ç›®å½•

1.  å®šä¹‰ Spark ä¼šè¯
2.  åŠ è½½æ•°æ®é›†
3.  æ¸…ç†æ•°æ®é›†
4.  å®šä¹‰å®¢æˆ·æµå¤±æ ‡ç­¾
5.  æ¢ç´¢æ€§æ•°æ®åˆ†æ
6.  ç‰¹å¾å·¥ç¨‹
7.  å»ºæ¨¡
8.  ç»“æœ
9.  ç»“è®º

***æ³¨:*** *æˆ‘ä»¬æŠŠæ‰€æœ‰çš„ä»£ç éƒ½è®°åœ¨ä¸€ä¸ªç¬”è®°æœ¬ä¸Šã€‚*

# 1ï¸âƒ£å®šä¹‰äº†ä¸€ä¸ªç«èŠ±ä¼šè¯

```
spark = SparkSession.\
    builder.\
    appName("Sparkify Churn Prediction").\
    getOrCreate()
```

æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º **Sparkify æµå¤±é¢„æµ‹**çš„ spark ä¼šè¯ã€‚é€šè¿‡è¿™ä¸ªä¼šè¯ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½å¹¶è¿è¡Œæ‰€æœ‰çš„è®¡ç®—ã€‚

# 2ï¸âƒ£åŠ è½½æ•°æ®é›†

```
EVENT_DATA_LINK = "mini_sparkify_event_data.json"df = spark.read.json(EVENT_DATA_LINK)
df.persist()
```

æˆ‘ä»¬å°†**mini _ Spark ify _ event _ data . JSON**æ–‡ä»¶ä¸‹è½½åˆ°ä¸ç¬”è®°æœ¬ç›¸åŒçš„ç›®å½•ä¸‹ï¼Œå¹¶å€ŸåŠ© Spark ä¼šè¯åŠ è½½åˆ°å†…å­˜ä¸­ã€‚

```
df.printSchema() |-- artist: string (nullable = true)
|-- auth: string (nullable = true)
|-- firstName: string (nullable = true)
|-- gender: string (nullable = true)
|-- itemInSession: long (nullable = true)
|-- lastName: string (nullable = true)
|-- length: double (nullable = true)
|-- level: string (nullable = true)
|-- location: string (nullable = true)
|-- method: string (nullable = true)
|-- page: string (nullable = true)
|-- registration: long (nullable = true)
|-- sessionId: long (nullable = true)
|-- song: string (nullable = true)
|-- status: long (nullable = true)
|-- ts: long (nullable = true)
|-- userAgent: string (nullable = true)
|-- userId: string (nullable = true)
```

æœ¬æ–‡ä¸­ä½¿ç”¨çš„æœ€é‡è¦çš„åˆ—å¦‚ä¸‹:

*   *è‰ºæœ¯å®¶:*å½“å‰æ­£åœ¨æ’­æ”¾çš„æ­Œæ›²çš„è‰ºæœ¯å®¶ã€‚
*   *çº§åˆ«:*åˆ†ç±»å˜é‡ï¼Œå¯ä»¥*ä»˜è´¹*æˆ–*å…è´¹*ã€‚
*   *é¡µé¢:*ç”¨æˆ·åœ¨åº”ç”¨ç¨‹åºä¸­çš„ä½ç½®(ä¾‹å¦‚ï¼Œç™»å½•é¡µé¢ï¼Œåœ¨
*   *æ³¨å†Œ*:æ³¨å†Œçš„ UTC æ—¶é—´æˆ³ã€‚
*   *sessionId:* ç”¨æˆ·å½“å‰ä¼šè¯çš„ Idã€‚
*   *æ­Œæ›²:*å½“å‰æ­£åœ¨æ’­æ”¾çš„æ­Œæ›²ã€‚
*   *çŠ¶æ€*:äº‹ä»¶çš„ HTTP çŠ¶æ€ã€‚(ä¾‹å¦‚ï¼Œ200ã€307ã€404)ã€‚
*   *ts* :äº‹ä»¶çš„ UTC æ—¶é—´æˆ³ã€‚
*   *ç”¨æˆ· Id:* ç”¨æˆ·çš„ Idã€‚

# 3ï¸âƒ£æ¸…ç†æ•°æ®é›†

åœ¨å®é™…çš„ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å¯¹è¦æ¸…ç†çš„å†…å®¹åšäº†æ›´å¹¿æ³›çš„æ£€æŸ¥ï¼Œä½†ä¸ºäº†ä¿æŒç®€æ´ï¼Œæˆ‘ä»¬å°†åªæ˜¾ç¤ºæœ€é‡è¦çš„å†…å®¹ã€‚

## æ£€æŸ¥ NaNs/ç©ºå€¼

```
df.select([
    F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns
]).show() +------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+
|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId| song|status| ts|userAgent|userId|
+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+
| 58392|   0|     8346|  8346|            0|    8346| 58392|    0|    8346|     0|   0|        8346|        0|58392|     0|  0|     8346|     0|
+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+
```

æ•°æ®é›†ä¸­çš„å¤§å¤šæ•°ç©ºå€¼éƒ½ç”¨ **None è¡¨ç¤ºã€‚**

```
df.select([
     F.count(F.when(F.col(c) == "", c)).alias(c) for c in df.columns
]).show() +------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+
|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|
+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+
|     0|   0|        0|     0|            0|       0|     0|    0|       0|     0|   0|           0|        0|   0|     0|  0|        0|  8346|
+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+
```

åœ¨ **userId** åˆ—ä¸Šæœ‰ä¸€ä¸ªä¾‹å¤–ï¼Œç©ºå€¼ç”¨ä¸€ä¸ªç©ºå­—ç¬¦ä¸²è¡¨ç¤ºã€‚

## æœªæ³¨å†Œçš„ç”¨æˆ·

å­—ç¬¦ä¸²ä¸ºç©ºçš„ç”¨æˆ·æ˜¯æœªæ³¨å†Œçš„ç”¨æˆ·ï¼›å› æ­¤ï¼Œå¹³å°ä¸èƒ½ç»™ä»–ä»¬åˆ†é…ä»»ä½• IDã€‚

**æŸ¥çœ‹æœªæ³¨å†Œç”¨æˆ·é¡µé¢åˆ†å¸ƒ**

```
df.filter(F.col("userId") == "").select("page").groupby("page").count().show() +-------------------+-----+
|               page|count|
+-------------------+-----+
|               Home| 4375|
|              About|  429|
|              Login| 3241|
|               Help|  272|
|              Error|    6|
|           Register|   18|
|Submit Registration|    5|
+-------------------+-----+
```

ä½¿ç”¨æœªæ³¨å†Œçš„ç”¨æˆ·æ•°æ®ä¸èƒ½å¸®åŠ©æˆ‘ä»¬é¢„æµ‹å®¢æˆ·æµå¤±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›è¡Œä» Spark æ•°æ®å¸§ä¸­åˆ é™¤ã€‚

**åˆ é™¤æœªæ³¨å†Œç”¨æˆ·**

```
cleaned_df = df.filter(F.col("userId") != "")
```

## ç©ºè‰ºæœ¯å®¶è¡Œ

```
df.filter(
    F.isnull(F.col("artist"))
).select(
    ["artist", "song", "userId", "page"]
).groupby("page").count().show() +--------------------+-----+
|                page|count|
+--------------------+-----+
|              Cancel|   52|
|    Submit Downgrade|   63|
|         Thumbs Down| 2546|
|                Home|14457|
|           Downgrade| 2055|
|         Roll Advert| 3933|
|              Logout| 3226|
|       Save Settings|  310|
|Cancellation Conf...|   52|
|               About|  924|
|            Settings| 1514|
|               Login| 3241|
|     Add to Playlist| 6526|
|          Add Friend| 4277|
|           Thumbs Up|12551|
|                Help| 1726|
|             Upgrade|  499|
|               Error|  258|
|      Submit Upgrade|  159|
|            Register|   18|
+--------------------+-----+
```

å½“è‰ºæœ¯å®¶ä¸ºç©ºæ—¶ï¼Œç”¨æˆ·èŠ±åœ¨å…¶ä»–é¡µé¢ä¸Šçš„æ—¶é—´æ¯”å¬éŸ³ä¹çš„æ—¶é—´è¿˜å¤šã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›ä¿¡æ¯å¯¹äºç†è§£æ³¨å†Œç”¨æˆ·çš„è¡Œä¸ºæ˜¯æœ‰ä»·å€¼çš„ã€‚

## å¡«å……ç©ºå€¼

```
cleaned_df = cleaned_df.fillna({
    "length": 0,
    "artist": "unknown",
    "song": "unknown"
})
```

æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬æƒ³ç”¨ **None** å€¼å¡«å……è¿™äº›è¡Œã€‚æ­¤æ—¶ï¼Œå½“ç”¨æˆ·æ²¡æœ‰åœ¨å¬ä»»ä½•ä¸œè¥¿æ—¶ï¼Œç©ºè¡Œåªå‡ºç°åœ¨ä¸æ­Œæ›²ç›¸å…³çš„å­—æ®µä¸­ã€‚è¿™äº›è¡Œä¸­çš„ä¿¡æ¯å¾ˆæœ‰ä»·å€¼ï¼Œæˆ‘ä»¬ä¸å»ºè®®åˆ é™¤å®ƒä»¬ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å°†**æ­Œæ›²çš„é•¿åº¦**è®¾ç½®ä¸º **0** ï¼Œå°†**è‰ºäºº**å’Œ**æ­Œæ›²**è®¾ç½®ä¸º**æœªçŸ¥**ã€‚

# 4ï¸âƒ£å®šä¹‰å®¢æˆ·æµå¤±æ ‡ç­¾

```
cleaned_df.select(
    F.count(
        F.when(F.col("page") == "Cancellation Confirmation", "page")
    ).alias("Cancellation Confirmation")
).show() +-------------------------+
|Cancellation Confirmation|
+-------------------------+
|                       52|
+-------------------------+
```

å½“è®¢é˜…è¢«å–æ¶ˆæ—¶ï¼Œæˆ‘ä»¬å°†è®¤ä¸ºç”¨æˆ·è¢«â€œ**æ…åŠ¨â€**ã€‚

***æ³¨æ„:*** *å¦‚æœæˆ‘ä»¬æƒ³è¿›å…¥ä¸‹ä¸€ä¸ªçº§åˆ«ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨* ***é™çº§*** *äº‹ä»¶æ¥è¡¨ç¤ºå®¢æˆ·æµå¤±äº‹ä»¶ã€‚*

```
labeled_df = cleaned_df.withColumn(
    "churnEvent", 
    F.when(
       F.col("page") == "Cancellation Confirmation",
       1
    ).otherwise(0)
)
```

ä½œä¸ºä¸­é—´æ­¥éª¤ï¼Œæˆ‘ä»¬å°†åˆ›å»º **churnEvent** åˆ—ï¼Œè¿™æ˜¯ä¸€ä¸ªæ£€æŸ¥â€œ**å–æ¶ˆç¡®è®¤â€**äº‹ä»¶çš„æ ‡å¿—ã€‚æˆ‘ä»¬åœ¨**äº‹ä»¶**çº§åˆ«è®¾è®¡äº†**å˜åŒ–**åˆ—ã€‚

```
user.labeled_df = labeled_df.withColumn(
    "churn", 
    F.sum("churnEvent").over(Window.partitionBy("userId"))
)
labeled_df = labeled_df.withColumn(
    "churn", 
    F.when(F.col("churn") >= 1, 1).otherwise(0)
)
```

æˆ‘ä»¬å°† **churnEvent** æ ‡å¿—èšåˆåˆ°ç”¨æˆ·çº§åˆ«ï¼Œå¹¶å°†å…¶è£å‰ªä¸º 0 æˆ– 1ã€‚å› æ­¤ï¼Œç”¨æˆ·åœ¨æ‰€æœ‰äº‹ä»¶ä¸­éƒ½æœ‰ä¸€ä¸ª **churn = 1** æˆ– **no-churn = 0** æ ‡å¿—ã€‚

è¯·è®°ä½ï¼Œæœ€ç»ˆï¼Œæˆ‘ä»¬è¦å»ºç«‹ä¸€ä¸ªé¢„æµ‹ç”¨æˆ·æµå¤±ï¼Œè€Œä¸æ˜¯äº‹ä»¶æµå¤±çš„åˆ†ç±»å™¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆç”¨æˆ·è¦ä¹ˆæ˜¯**æµå¤±= 1** è¦ä¹ˆæ˜¯**æ— æµå¤±= 0** ã€‚

# 5ï¸âƒ£æ¢ç´¢æ€§æ•°æ®åˆ†æ

**æ³¨æ„:**ä¸ºäº†ä¿æŒæ–‡ç« ç®€çŸ­ï¼Œæˆ‘ä»¬å°†åªå±•ç¤ºé‚£äº›å¸®åŠ©æˆ‘ä»¬è®¾è®¡æœ‰ä»·å€¼ç‰¹æ€§çš„æƒ…èŠ‚ã€‚ä½ å¯ä»¥*åœ¨è¿™é‡Œçœ‹åˆ°æ‰€æœ‰çš„å›¾è¡¨*å’Œ*è¿™äº›å›¾è¡¨æ˜¯å¦‚ä½•ç»˜åˆ¶çš„* [ã€‚](https://github.com/IusztinPaul/distributed-churn-prediction)

## ç”¨æˆ·æµå¤±åˆ†å¸ƒ

![](img/13f5c9219921d957ec850ff58a9ac00d.png)

**æµå¤±**å’Œ**æœªæµå¤±**ç”¨æˆ·æ•°é‡ä¹‹é—´çš„æŸ±çŠ¶å›¾ã€‚[å›¾ç‰‡ç”±ä½œè€…æä¾›]

åˆ†å¸ƒé«˜åº¦åå‘äº**æ— æµå¤±**ç”¨æˆ·ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ **F1 åˆ†æ•°**åœ¨äº¤å‰éªŒè¯æ­¥éª¤ä¸­æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œä»¥è€ƒè™‘è¿™ä¸€å› ç´ ã€‚

## å¯èƒ½çš„é¡µé¢

```
eda_df.select("page").distinct().show() +--------------------+
|                page|
+--------------------+
|              Cancel|
|    Submit Downgrade|
|         Thumbs Down|
|                Home|
|           Downgrade|
|         Roll Advert|
|              Logout|
|       Save Settings|
|Cancellation Conf...|
|               About|
|            Settings|
|     Add to Playlist|
|          Add Friend|
|            NextSong|
|           Thumbs Up|
|                Help|
|             Upgrade|
|               Error|
|      Submit Upgrade|
+--------------------+eda_df.filter(
    F.col("artist") != "unknown"
).select("page").distinct().show()+--------+
|    page|
+--------+
|NextSong|
+--------+
```

å”¯ä¸€æ’­æ”¾éŸ³ä¹çš„é¡µé¢å«åš **NextSong** ã€‚

## æ¯ä¸ªç”¨æˆ·å¹³å‡è®¿é—®é¡µé¢æ•°çš„åˆ†å¸ƒ

![](img/a06702563887e4e51870277ba7b2f830.png)

æŸ±çŠ¶å›¾æ˜¾ç¤ºäº†**æµå¤±**å’Œ**æœªæµå¤±**ç”¨æˆ·ä¹‹é—´çš„å¹³å‡è®¿é—®é¡µé¢æ•°ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ç•™åœ¨å¹³å°ä¸Šçš„ç”¨æˆ·å¹³å‡è®¿é—®äº†æ›´å¤šçš„é¡µé¢ã€‚

## æ¯ä¸ªç”¨æˆ·æ”¶å¬çš„å¹³å‡æ­Œæ›²æ•°é‡çš„åˆ†å¸ƒ

![](img/de5952899c4feaf7029277e7703830a2.png)

æŸ±çŠ¶å›¾æ˜¾ç¤ºäº†åœ¨**æµå¤±ç”¨æˆ·**å’Œ**æœªæµå¤±ç”¨æˆ·**ä¹‹é—´æ’­æ”¾æ­Œæ›²çš„å¹³å‡æ•°é‡ã€‚[å›¾ç‰‡ç”±ä½œè€…æä¾›]

å¹³å‡è€Œè¨€ï¼Œç•™åœ¨å¹³å°ä¸Šçš„ç”¨æˆ·å¬äº†æ›´å¤šçš„æ­Œæ›²ã€‚

## æ¯ä¸ªç”¨æˆ·æ”¶å¬çš„è‰ºæœ¯å®¶å¹³å‡æ•°é‡çš„åˆ†å¸ƒ

![](img/353930c4b887dc8373cdfe7bbaed0d44.png)

æŸ±çŠ¶å›¾æ˜¾ç¤ºäº†åœ¨**æµå¤±ç”¨æˆ·**å’Œ**æœªæµå¤±ç”¨æˆ·**ä¹‹é—´è¢«æ”¶å¬è‰ºæœ¯å®¶çš„å¹³å‡æ•°é‡ã€‚[å›¾ç‰‡ç”±ä½œè€…æä¾›]

å¹³å‡è€Œè¨€ï¼Œç•™åœ¨å¹³å°ä¸Šçš„ç”¨æˆ·åœ¨å¬æ›´å¹¿æ³›çš„è‰ºæœ¯å®¶çš„éŸ³ä¹ã€‚

## è‡ªæ³¨å†Œåˆ†å¸ƒä»¥æ¥çš„æ—¶é—´å¢é‡

![](img/6e36b48c5150c082147b245484d91ef5.png)

ç›´æ–¹å›¾æ˜¾ç¤ºäº†**æµå¤±**å’Œ**æœªæµå¤±**ç”¨æˆ·è‡ªæ³¨å†Œä»¥æ¥ç»è¿‡çš„æ—¶é—´åˆ†å¸ƒã€‚[å›¾ç‰‡ç”±ä½œè€…æä¾›]

***ã€æ—¶é—´Î´=****ç”¨æˆ·åœ¨å¹³å°æ³¨å†Œåçš„ç§’æ•°ã€‚*

å¯¹äº**æµå¤±**ç”¨æˆ·æ¥è¯´ï¼Œæ³¨å†Œåçš„æ—¶é—´å¢é‡çœ‹èµ·æ¥åƒæ˜¯*å³å*ã€‚åŒæ—¶**æ— æµå¤±**åˆ†å¸ƒå‘ˆæ­£æ€åˆ†å¸ƒã€‚å› æ­¤ï¼Œæ—¶é—´å¢é‡çš„**å¹³å‡å€¼æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢„æµ‹å€¼ã€‚**

# 6ï¸âƒ£ç‰¹è‰²å·¥ç¨‹

**æµå¤±åˆ†ç±»**å°†åœ¨**ç”¨æˆ·çº§åˆ«**æ‰§è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªç”¨æˆ·çš„æ•°æ®èšé›†åœ¨ä¸€è¡Œä¸­ã€‚

## ä¸ºä»€ä¹ˆè¦åœ¨ç”¨æˆ·çº§åˆ«æ±‡æ€»æ•°æ®ï¼Ÿ

æœ€ç»ˆé¢„æµ‹åº”è¯¥ç”¨äºé¢„æµ‹ç‰¹å®šç”¨æˆ·æ˜¯å¦å€¾å‘äºç¦»å¼€å…¬å¸ï¼Œè€Œä¸æ˜¯ç”¨æˆ·å°†é€€è®¢çš„äº‹ä»¶ã€‚å› æ­¤ï¼Œä¸€ä¸ªå•ç‹¬çš„äº‹ä»¶å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯æ— ç”¨çš„ï¼Œä½†æ˜¯å®ƒçš„é›†åˆæ˜¯æå…¶æœ‰ä»·å€¼çš„ã€‚

## å®šä¹‰ä¸€äº›æ•ˆç”¨å‡½æ•°:

```
def count_with_condition(condition):
    """Utility function to count only specific rows based on the 'condition'."""
    return F.count(F.when(condition, True)) def count_distinct_with_condition(condition, values):
    """Utility function to count only distinct & specific rows based on the 'condition'."""
    return F.count_distinct(F.when(condition, values))
```

## **åˆ›å»ºç‰¹å¾å·¥ç¨‹ Spark æ•°æ®æ¡†æ¶**

æ¦‚æ‹¬åœ°è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ EDA æ­¥éª¤ä¸­æä¾›çš„åŠŸèƒ½ã€‚åœ¨[ç¬”è®°æœ¬](https://github.com/IusztinPaul/distributed-churn-prediction)ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ›´å¤šçš„æ½œåœ¨åŠŸèƒ½ï¼Œä½†è¿™å››ä¸ªåŠŸèƒ½åœ¨**æµå¤±ç”¨æˆ·**å’Œ**æœªæµå¤±ç”¨æˆ·**ä¹‹é—´æ˜¾ç¤ºå‡ºæœ€å¼ºçš„é¢„æµ‹èƒ½åŠ›:

*   *è®¿é—®è¿‡çš„é¡µé¢æ€»æ•°*ã€‚
*   æ’­æ”¾çš„*é¦–æ­Œæ›²æ€»æ•°*ã€‚
*   *æ€»è‰ºäººæ€»æ•°*ã€‚
*   *è‡ª*æ³¨å†Œ*ä»¥æ¥çš„æ—¶é—´æˆ³*(ç§’)ã€‚

```
aggregated_df = labeled_df.groupby("userId").agg(
    F.count("page").alias("pages"),
    count_with_condition(
        F.col("page") == "NextSong"
    ).alias("plays"),
    count_distinct_with_condition(
        F.col("artist") != "unknown", F.col("artist")
    ).alias("artists"),
    F.max(F.col("ts") - F.col("registration")).alias("delta"),
    F.max("churn").alias("churn")
)
aggregated_df.show(n=5) +------+-----+-----+-------+-----------+-----+
|userId|pages|plays|artists|      delta|churn|
+------+-----+-----+-------+-----------+-----+
|100010|  381|  275|    252| 4807612000|    0|
|100014|  310|  257|    233| 7351206000|    1|
|100021|  319|  230|    207| 5593438000|    1|
|   101| 2149| 1797|   1241| 4662657000|    1|
|    11|  848|  647|    534|10754921000|    0|
+------+-----+-----+-------+-----------+-----+
```

## å°†æ•°æ®å¸§æ˜ å°„åˆ°ç«èŠ±çŸ¢é‡

```
assembler = VectorAssembler(
    inputCols=[
        "pages", "plays", "artists", "delta"
], outputCol="unscaled_features")
engineered_df = assembler.transform(aggregated_df)
engineered_df = engineered_df.select(
    F.col("unscaled_features"),
    F.col("churn").alias("label")
)
```

**ç«èŠ±æ¨¡å‹**æœŸæœ›åœ¨è¾“å…¥ç«¯æœ‰ä¸€ä¸ªç«èŠ±çŸ¢é‡ã€‚åŒæ ·ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œä»–ä»¬æœŸæœ›**è¾“å…¥**ç‰¹æ€§ä½äºåä¸º**ç‰¹æ€§**çš„åˆ—ä¸­ï¼Œè€Œ**ç›®æ ‡**ä½äºåä¸º**æ ‡ç­¾**çš„åˆ—ä¸­ã€‚

# 7ï¸âƒ£é€ å‹

æˆ‘ä»¬å°†è®­ç»ƒå’Œæµ‹è¯•ä¸‰ä¸ªæ¨¡å‹:

1.  é€»è¾‘å›å½’
2.  æœ´ç´ è´å¶æ–¯
3.  æ¢¯åº¦æ¨è¿›æ ‘

æˆ‘ä»¬å°†ä½¿ç”¨ä¸‰é‡äº¤å‰éªŒè¯æ¥å¯»æ‰¾æœ€ä½³è¶…å‚æ•°ã€‚

æˆ‘ä»¬å°†åœ¨åˆ—è½¦åˆ†å‰²ä¸­ä½¿ç”¨ **80%** çš„æ•°æ®ï¼Œåœ¨æµ‹è¯•åˆ†å‰²ä¸­ä½¿ç”¨ **20%** çš„æ•°æ®ã€‚

æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†å®šæ ‡å™¨å°†ç‰¹å¾æ ‡å‡†åŒ–ã€‚

å› ä¸ºæ ‡ç­¾é«˜åº¦ä¸å¹³è¡¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **F1 åˆ†æ•°**æ¥è¯„ä¼°æ¨¡å‹ã€‚ **F1 å¾—åˆ†**æŒ‡æ ‡åœ¨è€ƒè™‘ä¸å¹³è¡¡åˆ†é…é—®é¢˜çš„**ç²¾åº¦**å’Œ**å¬å›**ä¸‹è¿è¡Œã€‚

## å®šä¹‰ä¸€äº›æ•ˆç”¨å‡½æ•°:

å¦‚æœä½ åªå¯¹ç»“æœæ„Ÿå…´è¶£ï¼Œè·³è¿‡å®ƒä»¬ã€‚

*Spark ML è¯­æ³•ä¸ Sklearn ä¸­ä½¿ç”¨çš„è¯­æ³•éå¸¸ç›¸ä¼¼ã€‚*

```
def run(pipeline, paramGrid, train_df, test_df):
    """
    Main function used to train & test a given model.
    The training step uses cross-validation to find the best hyper-parameters for the model. :param pipeline: Model pipeline.
    :param paramGrid: Parameter grid used for cross-validation.
    :param train_df: Training dataframe.
    :param test_df: Testing dataframe.
    :return: the best model from cross-validation.
    """ fitted_model = fit_model(paramGrid, pipeline, train_df)
    evaluate_model(fitted_model, test_df) return fitted_model def fit_model(paramGrid, pipeline, train_df):
    """
    Function that trains the model using cross-validation.
    Also, it prints the best validation results and hyper-parameters. :param paramGrid: Parameter grid used for cross-validation.
    :param pipeline: Model pipeline.
    :param train_df: Training dataframe.
    :return: the best model from cross-validation.
    """ crossval = CrossValidator(
        estimator=pipeline,
        estimatorParamMaps=paramGrid,
        evaluator = MulticlassClassificationEvaluator(
              metricName="f1", 
              beta=1.0
        ),
        parallelism=3,
        numFolds=3
    ) fitted_model = crossval.fit(train_df)
    print_best_validation_score(fitted_model)
    print_best_parameters(fitted_model) return fitted_model def create_pipeline(model):
    """
    Create a pipeline based on a model. :param model: The end model that will be used for training.
    :return: the built pipeline.
    """ scaler = StandardScaler(
        inputCol="unscaled_features", 
        outputCol="features"
    )
    pipeline = Pipeline(stages=[scaler, model]) return pipeline def print_best_validation_score(cross_validation_model):
    """Prints the best validation score based on the results from the cross-validation model."""
    print()
    print("-" * 60)
    print(f"F1 score, on the validation split, for the best model: {np.max(cross_validation_model.avgMetrics) * 100:.2f}%")
    print("-" * 60) def print_best_parameters(cross_validation_model):
    """Prints the best hyper-parameters based on the results from the cross-validation model.""" parameters = cross_validation_model \
       .getEstimatorParamMaps() [np.argmax(cross_validation_model.avgMetrics)] print()
    print("-" * 60)
    print("Best model hyper-parameters:")
    for param, value in parameters.items():
        print(f"{param}: {value}")
    print("-" * 60) def evaluate_model(model, test_df):
    """Evaluate the model on the test set using F1 score and print the results.""" predictions = model.transform(test_df)
    evaluator =  MulticlassClassificationEvaluator(
            metricName="f1", 
            beta=1.0
          )
    metric = evaluator.evaluate(predictions) print()
    print("-" * 60)
    print(f"F1 score, on the test set is: {metric*100:.2f}%")
    print("-" * 60) return metric
```

## æ‹†åˆ†æ•°æ®

```
train_df, test_df = engineered_df.randomSplit([0.8, 0.2], seed=42)
```

## é€»è¾‘å›å½’

```
lr = LogisticRegression()
pipeline = create_pipeline(lr)paramGrid = ParamGridBuilder() \
    .addGrid(lr.maxIter, [10, 25, 50])  \
    .addGrid(lr.regParam, [0.05, 0.1, 0.2]) \
    .addGrid(lr.elasticNetParam, [0.05, 0.1, 0.2]) \
    .build()run(
   pipeline,
   paramGrid,
   train_df.alias("train_df_lr"),
   test_df.alias("test_df_lr")
);
```

## æœ´ç´ è´å¶æ–¯

```
nb = NaiveBayes()
pipeline = create_pipeline(nb)paramGrid = ParamGridBuilder() \
    .addGrid(nb.smoothing, [0.5, 1, 2])  \
    .build()run(
   pipeline,
   paramGrid,
   train_df.alias("train_df_nb"),
   test_df.alias("test_df_nb")
);
```

## æ¢¯åº¦æ¨è¿›

```
gbt = GBTClassifier()
pipeline = create_pipeline(gbt)paramGrid = ParamGridBuilder() \
    .addGrid(gbt.maxIter, [10, 20, 30]) \
    .addGrid(gbt.stepSize, [0.05, 0.1]) \
    .build()run(
   pipeline,
   paramGrid, 
   train_df.alias("train_df_gbt"),
   test_df.alias("test_df_gbt")
);
```

**è®­ç»ƒ&æµ‹è¯•ä»£ç çš„ä¸‰ä¸ªç‰‡æ®µ**éƒ½æœ‰*ç›¸åŒçš„ç»“æ„*ã€‚

æˆ‘ä»¬å°†**ç®¡é“**å®šä¹‰ä¸º*ç¼©æ”¾æ­¥éª¤*å’Œ*å®é™…æ¨¡å‹*ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¶…å‚æ•°çš„**äº¤å‰éªŒè¯ç½‘æ ¼**ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›å¯¹è±¡ä¼ é€’ç»™ **run** å‡½æ•°ã€‚å…¶åœ¨ç»™å®šçš„æ•°æ®åˆ†å‰²å’Œé…ç½®ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚

# 8ï¸âƒ£ç»“æœ

```
|        Model        | Validation |   Test   |
|:-------------------:|:----------:|:--------:|
| Logistic Regression |   0.6958   |  0.5952  |
|     Naive Bayes     |   0.6672   |  0.5952  |
|  Gradient Boosting  |  *0.7333*  | *0.8473* |
```

ä¸é€»è¾‘å›å½’å’Œ Naive Bay ç›¸æ¯”ï¼Œ **GBT** æ¨¡å‹å…·æœ‰æ›´å¥½çš„****F1 å¾—åˆ†**ï¼Œè¿™æ˜¯å› ä¸º **GBT** æ¨¡å‹æ›´å¤æ‚ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£éçº¿æ€§å…³ç³»ã€‚**

**é€šå¸¸ï¼Œåœ¨ä¸è€ƒè™‘ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„**ç›¸å…³æ€§**æˆ–**éçº¿æ€§å…³ç³»**ç­‰å› ç´ çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦æ¨è¿›æ–¹æ³•æ¯”å…¶ä»–æ–¹æ³•å¦‚ **LR** æˆ– **NB** è¡¨ç°å¾—æ›´å¥½ã€‚åŸºäºæ ‘çš„æ¨¡å‹å¯¹è¿™äº›é—®é¢˜ä¸æ•æ„Ÿï¼Œå› ä¸ºå®ƒä»¬ç‹¬ç«‹åœ°åˆ›å»ºå¶å­å¹¶æ”¯æŒå¤šç»´å…³ç³»ã€‚**

# **9ï¸âƒ£ç»“è®º**

**å¤ªå¥½äº†ï¼æˆ‘ä»¬è®¾æ³•åªç”¨ **Spark** è®­ç»ƒä¸€ä¸ªåƒæ ·çš„**åˆ†ç±»å™¨**ã€‚**

**æˆ‘ä»¬**åŠ è½½**æ•°æ®é›†ï¼Œ**æ¸…ç†**å®ƒï¼Œ**åˆ†æ**å®ƒï¼Œæœ€åï¼Œæˆ‘ä»¬**åˆ›å»ºäº†**ä¸€å¥—æœ‰ç”¨çš„**ç‰¹å¾**æ¥é¢„æµ‹**å®¢æˆ·æµå¤±**ã€‚**

**æœ‰äº†å·¥ç¨‹æ•°æ®ï¼Œæˆ‘ä»¬**è®­ç»ƒ**å’Œ**æµ‹è¯•**ä¸‰ä¸ªæµå¤±é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨**äº¤å‰éªŒè¯**ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ä»¥ä¸‹å‹å·çš„**æœ€ä½³è¶…å‚æ•°**:**

*   **é€»è¾‘å›å½’**
*   **æœ´ç´ è´å¶æ–¯**
*   **æ¢¯åº¦æ¨è¿›**

**æˆ‘ä»¬æ¯”è¾ƒäº†ç»“æœï¼Œå‘ç° **GBT** è½¦å‹åœ¨éªŒè¯å’Œæµ‹è¯•åˆ†å‰²ä¸­çš„ **F1 å¾—åˆ†æœ€é«˜**ã€‚**

**ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åšä»¥ä¸‹å·¥ä½œ:**

*   **æ·»åŠ æ›´å¤šåŠŸèƒ½**
*   **è§£å†³æ ‡ç­¾å¤±è¡¡**
*   **ä½¿ç”¨**é™çº§**äº‹ä»¶ç”Ÿæˆæ›´å¤š**æµå¤±**æ ‡ç­¾**
*   **æ›´å¤šè¶…å‚æ•°è°ƒæ•´**
*   **ä½¿ç”¨ XGBoost æˆ– LightGBM**

> **ä½ è¿˜æœ‰ä»€ä¹ˆè¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹çš„å»ºè®®ï¼Ÿ**

*****æ³¨:*** *ä½ å¯ä»¥åœ¨è¿™é‡Œ* *è®¿é—® GitHub åº“* [*ã€‚*](https://github.com/IusztinPaul/distributed-churn-prediction)**

**ğŸ‰è°¢è°¢ä½ çœ‹æˆ‘çš„æ–‡ç« ï¼**

**ğŸ“¢å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œå¹¶ä¸”æƒ³åˆ†äº«æˆ‘è¿›å…¥ AIã€ML å’Œ MLOps çš„å­¦ä¹ ä¹‹æ—…ï¼Œä½ ä¹Ÿå¯ä»¥å…³æ³¨æˆ‘çš„[**LinkedIn**](https://www.linkedin.com/in/pauliusztin/)**ã€‚****