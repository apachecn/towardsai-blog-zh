# å¤šå…ƒå›å½’çš„å˜å½¢é‡‘åˆšâ€”[ç¬¬ä¸€éƒ¨åˆ†]

> åŸæ–‡ï¼š<https://pub.towardsai.net/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9?source=collection_archive---------1----------------------->

# ğŸ’ä½œä¸ºç‰¹å¾æå–å™¨çš„å˜å‹å™¨ğŸ’

æˆ‘åœ¨ Kaggle å‚åŠ çš„ [FB3 ç«èµ›](https://www.kaggle.com/competitions/feedback-prize-english-language-learning)æ¿€åŠ±æˆ‘å†™ä¸€ç¯‡å…³äºæˆ‘æµ‹è¯•è¿‡çš„æ–¹æ³•çš„å¸–å­ã€‚å†åŠ ä¸Šæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ˜ç¡®çš„å…³äºå¦‚ä½•ä½¿ç”¨å˜å‹å™¨è§£å†³å¤šå…ƒå›å½’é—®é¢˜çš„æ•™ç¨‹ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºåˆ†äº«ä¸€ä¸‹æˆ‘çš„å·¥ä½œä¼šå¾ˆæœ‰ç”¨ã€‚

æ‰€æœ‰è¿™äº›å·¥ä½œéƒ½åœ¨[æˆ‘çš„å¡æ ¼å°”ç¬”è®°æœ¬](https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners)ä¸­ç»§ç»­

![](img/526717509a83f7e1031b6c35ee3f2eb2.png)

[https://unsplash.com/photos/AVYo3X6XZYg](https://unsplash.com/photos/AVYo3X6XZYg)

# ä»‹ç»

æˆ‘ä»¬å¹¶ä¸éƒ½æœ‰ç¦æ¥¼æ‹œğŸ§¡çš„æ‰åï¼ŒæŸæ ¼æ£®ğŸ¤çš„æ¸…æ™°ï¼Œæ™®é²æ–¯ç‰¹çš„å¤©æ‰ğŸ’™ï¼Œä¹Ÿæ²¡æœ‰**èŒ¨å¨æ ¼**çš„é£æ ¼å’Œæ‰‹è…•ğŸ’œï¼Œä¹Ÿä¸æ˜¯ä¼å°”æ³°**çš„æœ¬äº‹**ğŸ’šï¼Œä¹Ÿä¸æ˜¯å”æœ¬å**çš„å…ˆçŸ¥å…ˆè§‰**ğŸ’–â€¦

è¿™ä»½æ¸…å•è¿œéè¯¦å°½æ— é—ï¼Œæ„Ÿè°¢ä¸Šå¸ï¼Œæœ‰å¤©æ‰çš„ä½œå®¶å’Œå“²å­¦å®¶å…è®¸æˆ‘ä»¬æš‚æ—¶é€ƒç¦»è¿™ä¸ªå”¯ç‰©ä¸»ä¹‰çš„ä¸–ç•Œã€‚

ä½†å°±æˆ‘ä»¬æ™®é€šäººè€Œè¨€ï¼Œä¸ç®¡æ˜¯ä¸æ˜¯æ–‡å­¦ï¼Œæˆ‘ä»¬è‡³å°‘å¯ä»¥å¸Œæœ›å°½åŠ›å°Šé‡è¯­è¨€çš„è§„åˆ™ï¼Œå†™å¾—â€œæ­£ç¡®â€ã€‚è€å¸ˆä»¬å¸®åŠ©æˆ‘ä»¬å­¦ä¹ è¯­è¨€çš„åŸºç¡€çŸ¥è¯†ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿåè¿‡æ¥ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå¸®åŠ©ä»–ä»¬ï¼Œåˆ©ç”¨æˆ‘ä»¬çš„çŸ¥è¯†ï¼ŒèŠ‚çœæ—¶é—´æ¥ä¿®æ”¹å­¦ç”Ÿçš„ä½œæ–‡ï¼Œå¸®åŠ©ä»–ä»¬åŒºåˆ†æ¯ä¸ªå­¦ç”Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æ›´å¥½åœ°è°ƒæ•´ä»–ä»¬çš„æ•™å­¦æ–¹æ³•ï¼Œä»¥é€‚åº”æ¯ä¸ªå­¦ç”Ÿçš„æ°´å¹³ã€‚

åœ¨è¿™åœºæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬è¢«è¦æ±‚ä½¿ç”¨ 8-12 å¹´çº§è‹±è¯­å­¦ä¹ è€…å†™çš„é¢„å…ˆè¯„åˆ†çš„è®®è®ºæ–‡ï¼Œæ ¹æ®å…­ä¸ªåˆ†ææŒ‡æ ‡åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹:**è¡”æ¥ã€å¥æ³•ã€è¯æ±‡ã€çŸ­è¯­ã€è¯­æ³•**å’Œ**æƒ¯ä¾‹**ã€‚åˆ†æ•°èŒƒå›´ä» 1.0 åˆ° 5.0ï¼Œå¢é‡ä¸º 0.5ã€‚

æˆ‘ä»¬å°†æè¿°å¦‚ä½•ä½¿ç”¨æ‹¥æŠ±è„¸æ¨¡å‹æ¥è§£å†³è¿™ç§ç±»å‹çš„é—®é¢˜ï¼Œæˆ‘é€‰æ‹©äº†`deberta-v3-base`æ¨¡å‹(è¿™é‡Œçš„[æ˜¯ç›¸åº”çš„æ¨¡å‹å¡)ï¼Œæˆ‘å°†å±•ç¤ºæˆ‘ä»¬å¦‚ä½•ä»¥ä¸¤ç§æœ‰æ•ˆçš„æ–¹å¼ä½¿ç”¨å®ƒ:](https://huggingface.co/microsoft/deberta-v3-base?text=The+goal+of+life+is+%5BMASK%5D.)

ğŸ¤™**ç‰¹å¾æå–**:æˆ‘ä»¬ä½¿ç”¨éšè—çŠ¶æ€ä½œä¸ºç‰¹å¾ï¼Œåªåœ¨å…¶ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè€Œä¸ä¿®æ”¹é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å¯¹äºæœ¬èŠ‚[,@ cdeotte](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577)æå‡ºäº†è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç»å¦™ç”¨æ³•ï¼Œå³ä½¿ç”¨å¤šä¸ªéå¾®è°ƒçš„å˜å‹å™¨åµŒå…¥ï¼Œç„¶åå°†å®ƒä»¬è¿æ¥èµ·æ¥å¹¶è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„åˆ†ç±»å™¨:æˆ‘å¼ºçƒˆé‚€è¯·æ‚¨æŸ¥çœ‹ç›¸å…³çš„[è®¨è®º](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577)å’Œ[ç¬”è®°æœ¬](https://www.kaggle.com/code/cdeotte/rapids-svr-cv-0-450-lb-0-44x)

ğŸ¤™**å¾®è°ƒ**:æˆ‘ä»¬å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¿™ä¹Ÿæ„å‘³ç€å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚è¿™ç§æ–¹æ³•å°†åœ¨[åé¢çš„å¸–å­](https://zghrib.medium.com/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c)ä¸­è®¨è®º

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸€æ­¥ä¸€æ­¥åœ°ä»‹ç»ç¬¬ä¸€ç§æ–¹æ³•:ğŸ‘åŸºäºç¼–ç å™¨çš„å˜å‹å™¨ç®€ä»‹

è¿™ä¸ªæƒ³æ³•æ˜¯ä½¿ç”¨åŸºäº BERT çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç»è¿‡é¢„å…ˆè®­ç»ƒï¼Œå¯ä»¥é¢„æµ‹æ–‡æœ¬çš„å±è”½å…ƒç´ ï¼Œä»¥åŠä¸€ä¸ªè‡ªå®šä¹‰çš„åˆ†ç±»å™¨:å·¥ä½œæµç¨‹å¦‚ä¸‹:

![](img/7e9228951e01da8e430e43524ab9a16a.png)

å¸¦æœ‰åŸºäºç¼–ç å™¨çš„
å˜å‹å™¨çš„åˆ†ç±»å™¨/å›å½’å™¨å¤´

**1ã€‚ç”Ÿæˆä»¤ç‰Œç¼–ç :**
é¦–å…ˆï¼Œä»¤ç‰ŒåŒ–å™¨ç”Ÿæˆä¸€ä¸ªåä¸º**ä»¤ç‰Œç¼–ç **çš„çƒ­ç¼–ç :æ¯ä¸ªå‘é‡çš„ç»´æ•°ç­‰äºä»¤ç‰ŒåŒ–å™¨è¯æ±‡è¡¨`[batch_size, vocab_size]`ã€‚HuggingFace çš„`AutoTokenizer`ç±»å°†è‡ªåŠ¨åŠ è½½å¯¹åº”äºæ£€æŸ¥ç‚¹åç§°çš„è®°å·èµ‹äºˆå™¨(å¯¹äºè¿™ä¸ªç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`deberta-v3-base`)ï¼Œè®°å·èµ‹äºˆå™¨å°†ç”Ÿæˆä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬:

*   `input_ids`:å¥å­ä¸­æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•
*   `token_type_ids`:å½“æœ‰å¤šä¸ªåºåˆ—æ—¶ï¼Œæ ‡è¯†ä¸€ä¸ªä»¤ç‰Œå±äºå“ªä¸ªåºåˆ—
*   `attention_mask`:ä»çœŸå®è®°å·ä¸­è¯†åˆ«å¡«å……å…ƒç´ 

ç„¶åï¼Œæ¨¡å‹å°†é‡‡ç”¨ä»¤ç‰Œç¼–ç ï¼Œå¹¶å¦‚ä¸‹è¿›è¡Œ:

**2 .ç”ŸæˆåµŒå…¥:**
è¯¥æ¨¡å‹å°†ä»¤ç‰Œç¼–ç è½¬æ¢ä¸º**å¯†é›†åµŒå…¥**ã€‚ä¸ä»¤ç‰Œç¼–ç ä¸åŒï¼ŒåµŒå…¥æ˜¯**å¯†é›†çš„** =éé›¶å€¼ã€‚ä»¤ç‰Œç¼–ç æ˜¯ç”±ä»¤ç‰ŒåŒ–å™¨ç”Ÿæˆçš„ã€‚
- >æˆ‘ä»¬ç”¨æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å°ºå¯¸å¾—åˆ°ä¸€ä¸ªç»´åº¦ä¸º`[batch_size, max_len]`çš„å¼ é‡

**3ã€‚ç”Ÿæˆéšè—çŠ¶æ€:**
æ¨¡å‹é€šè¿‡ç¼–ç å™¨å †æ ˆæä¾›åµŒå…¥ï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œè¾“å…¥è¿”å›éšè—çŠ¶æ€ã€‚æˆ‘ä»¬è·å¾—ä¸€ä¸ªæœ€ç»ˆå¼ é‡`[batch_size, max_len, hidden_states_dim]`
æˆ‘ä»¬åŠ è½½ä¸€ä¸ª`AutoModel`å¯¹è±¡æ¥åˆå§‹åŒ–ä¸€ä¸ªå…·æœ‰æ‰€æœ‰æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯`microsoft/deberta-v3-base`

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å‡†å¤‡æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä»¥ä¾¿ç”±è½¬æ¢å™¨è¿›è¡Œå¤„ç†:

# ğŸªµå‡†å¤‡æ•°æ®é›†

æˆ‘ä»¬å°†é€šè¿‡å°æ‰¹é‡è¿›è¡Œï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`torch.utils.data.DataLoader`å’Œ`torch.utils.data.Dataset`(æ‚¨å¯ä»¥åœ¨æ­¤å¤„æŸ¥çœ‹å‚è€ƒ)ã€‚`Dataset`å…è®¸è¿”å›å¸¦æœ‰ç›¸åº”æ ‡ç­¾çš„æ•°æ®é›†æ ·æœ¬ã€‚`DataLoader`åœ¨æ•°æ®é›†å‘¨å›´åŒ…è£…äº†ä¸€ä¸ª iterable ä»¥æ–¹ä¾¿è®¿é—®æ ·æœ¬ï¼Œæä¾›äº†è®¸å¤šå®ç”¨ç¨‹åºï¼Œä¾‹å¦‚åœ¨æ¯ä¸ªæ—¶æœŸé‡æ–°æ’åˆ—æ•°æ®ä»¥å‡å°‘æ¨¡å‹è¿‡åº¦æ‹Ÿåˆï¼Œæˆ–è€…å…è®¸ä½¿ç”¨å¤šé‡å¤„ç†æ¥åŠ é€Ÿæ•°æ®æ£€ç´¢ã€‚

ä¸ºäº†å¼€å‘æˆ‘ä»¬çš„å®šåˆ¶`Dataset`ï¼Œæˆ‘ä»¬å¿…é¡»è¦†ç›–`__init__`ã€`__len__`å’Œ`__getitem__`å‡½æ•°ã€‚
æœ€é‡è¦çš„å‡½æ•°æ˜¯`__getitem__`:å®ƒä»æ•°æ®é›†ä¸­ç»™å®šç´¢å¼• idx å¤„è¿”å›ä¸€ä¸ªæ ·æœ¬ã€‚å‡½æ•°çš„è¾“å‡ºæ ¼å¼å¿…é¡»ç¬¦åˆ**æ¨¡å‹çš„é¢„æœŸæ ¼å¼**:

*   `input_ids`:æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œ id åˆ—è¡¨ã€‚
*   `attention_mask`:æŒ‡å®šæ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›ä»¤ç‰Œçš„ç´¢å¼•åˆ—è¡¨
*   `labels`:åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¤„ç†ä¸€ä¸ª**å¤šç±»å›å½’**é—®é¢˜ï¼Œæ ‡ç­¾æ˜¯*å…­ä¸ªåˆ†æåˆ†æ•°*çš„å‘é‡ã€‚

å¯¹äºè®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬å°†å‚è€ƒä¸å¯å¦è®¤çš„ç»å…¸äº¤å‰éªŒè¯æ–¹æ¡ˆ

ğŸ•**å¤šæ ‡ç­¾æ•°æ®åˆ†å±‚:**

åœ¨æˆ‘ä»¬çš„æ•°æ®ç§‘å­¦å®¶ç¤¾åŒºä¸­ï¼Œæœ‰è¯æ®è¡¨æ˜å¦‚ä½•åˆ†å‰²äº¤å‰éªŒè¯æŠ˜å å¯¹æ¨¡å‹æ€§èƒ½æœ‰ç›´æ¥å½±å“ã€‚
é€šå¸¸ï¼Œå¯¹äºå•ç±»é—®é¢˜ï¼Œè¤¶çš±ä¸å•ä¸ªç›®æ ‡ä¸€èµ·åˆ†å±‚(ç¦»æ•£ç›®æ ‡çš„*ç±»åˆ†å¸ƒæˆ–è¿ç»­ç›®æ ‡çš„é¢å…ƒåˆ†å¸ƒ*)ã€‚

> ä½†æ˜¯ï¼Œåœ¨å¤šç±»é—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ğŸ¤”ï¼Ÿ

å—¯ï¼Œå·²ç»å¼€å±•äº†è®¸å¤šå·¥ä½œæ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚:

*   2011 : [Sechidis â€”å…³äºå¤šæ ‡ç­¾æ•°æ®çš„åˆ†å±‚](http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf)
*   2017:[szymÅ„skiâ€”â€”ç¬¬ä¸€å±Šä¸å¹³è¡¡é¢†åŸŸå­¦ä¹ å›½é™…ç ”è®¨ä¼šè®ºæ–‡é›†](http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html)

[è¿™é‡Œ](https://www.slideshare.net/tsoumakas/on-the-stratification-of-multilabel-data)æ˜¯ä¸€ä¸ªæ¸©å’Œçš„æ¼”ç¤ºï¼Œè§£é‡Šç®—æ³•ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨é‡‡ç”¨[è¿­ä»£åˆ†å±‚](https://github.com/trent-b/iterative-stratification)å®æ–½çš„è¿­ä»£æ–¹æ³•ç®—æ³•:

```
import pandas as pd
from iterstrat.ml_stratifiers import MultilabelStratifiedKFoldtrain = pd.read_csv(PATH_TO_TRAIN)
print("TRAIN SHAPE", train.shape)
test = pd.read_csv(PATH_TO_TEST)
print("TEST SHAPE", test.shape)
label_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
cv = MultilabelStratifiedKFold(
          n_splits=N_FOLDS, 
          shuffle=True, 
          random_state=SEED
          )
train = train.reset_index(drop=True)
for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[label_cols])):
    train.loc[val_idx , "fold"] = int(fold)

train["fold"] = train["fold"].astype(int)
```

ç°åœ¨è®©æˆ‘ä»¬åƒå‰é¢æè¿°çš„é‚£æ ·å®ç°æ•°æ®é›†è¿­ä»£å™¨:

```
# lets define the batch genetator
class CustomIterator(torch.utils.data.Dataset):
    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):
        self.df = df
        self.tokenizer = tokenizer
        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length
        self.labels = labels
        self.is_train = is_train

    def __getitem__(self,idx):
        tokens = self.tokenizer(
                    self.df.loc[idx, 'full_text'],#.to_list(),
                    add_special_tokens=True,
                    padding='max_length',
                    max_length=self.max_seq_length,
                    truncation=True,
                    return_tensors='pt',
                    return_attention_mask=True
                )     
        res = {
            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),
            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()
        }

        if self.is_train:
            res["labels"] = torch.tensor(
                self.df.loc[idx, self.labels].to_list(), 
            ).to(CONFIG.get('device')) 

        return res

    def __len__(self):
        return len(self.df)
```

è¿™ä¸ªè‡ªå®šä¹‰çš„`Dataset`å°†åœ¨ä»¥åè¢«ç”¨äºå¾®è°ƒè½¬æ¢å™¨æˆ–è€…ä»…ä»…ä½œä¸ºä¸€ä¸ªç‰¹å¾æå–å™¨ã€‚

PS:æˆ‘æ·»åŠ äº†`is_train`å‚æ•°æ¥å†³å®šæ˜¯å¦è¿”å›â€œæ ‡ç­¾â€å­—æ®µ(åªæœ‰è®­ç»ƒæ•°æ®é›†åŒ…å«æ ‡ç­¾å­—æ®µ)

# â›Transformers æ˜¯ Extractorsâ›çš„ç‰¹è‰²

é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç¼–ç å™¨æƒé‡è¢«**å†»ç»“**ï¼Œéšè—çŠ¶æ€è¢«å¤šå…ƒå›å½’å™¨ç”¨ä½œ**ç‹¬ç«‹ç‰¹å¾**ã€‚
ç”±äºéšè—çŠ¶æ€åªè®¡ç®—ä¸€æ¬¡ï¼Œå¦‚æœæˆ‘ä»¬**ä¸å¤„ç† GPU**ï¼Œè¿™ç§æ–¹æ³•æ˜¯æœ€å¥½çš„é€‰æ‹©:

![](img/2c107b051a73f5501363a3daef0e9022.png)

ç¼–ç å™¨å˜å‹å™¨ä½œä¸ºç‰¹å¾æå–å™¨:åªæœ‰å¤´éƒ¨æ˜¯å¯è®­ç»ƒçš„ï¼Œæ‰€æœ‰çš„å˜å‹å™¨å±‚è¢«å†»ç»“

ç•™ç»™æˆ‘ä»¬çš„å”¯ä¸€ä¸€æŠŠè‡ªç”±ä¹‹æ–§æ˜¯å¦‚ä½•å°†éšè—çŠ¶æ€å¼ é‡`[batch_size, max_len, hidden_states_dim]`ç®€åŒ–ä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡è¡¨ç¤º:æˆ‘å¼ºçƒˆå»ºè®®ä½ å‚è€ƒæ— ä»·çš„ [@rhtsingh notebook](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently) ï¼Œå®ƒâ€œè¯¦å°½åœ°â€åˆ—ä¸¾äº†éšè—çŠ¶æ€ç¼–ç çš„ä¸åŒæ–¹å¼**ã€**ã€‚æˆ‘æµ‹è¯•äº†ä»¥ä¸‹æŠ€å·§:

ğŸ¤™ **CLS åµŒå…¥:** BERT å¼•å…¥ä¸€ä¸ª**ã€CLSã€‘**token æ ‡ç­¾ï¼Œç«™åœ¨æ¯å¥è¯çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œæ•æ‰æ•´ä¸ªå¥å­çš„ä¸Šä¸‹æ–‡ã€‚cls åµŒå…¥ç®€å•åœ°åŒ…æ‹¬é€‰æ‹©æ¯ä¸ªéšè—çŠ¶æ€å‘é‡çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œä»è€Œå¾—åˆ°`[batch_size, 1, hidden_states_dim]`å‘é‡

```
import torch
import torch.nn as nn
import transformers
from transformers import (
    AutoModel, AutoConfig, 
    AutoTokenizer, logging,
    AdamW, get_linear_schedule_with_warmup,
    DataCollatorWithPadding,
    Trainer, TrainingArguments
)
from transformers.modeling_outputs import SequenceClassifierOutput# https://github.com/UKPLab/sentence-transformers/blob/0422a5e07a5a998948721dea435235b342a9f610/sentence_transformers/models/Pooling.py
# https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently
def cls_embedding(outputs):
    """Since Transformers are contextual model, 
    the idea is [CLS] token would have captured the entire context 
    and would be sufficient for simple downstream tasks such as classification
    Select the first token for each hidden state

    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]
    @return:  tensor of dimensions = [batch_size, hidden_states_dim]
    """
    return outputs.last_hidden_state[:, 0, :].to(CONFIG.get('device'))
```

ğŸ¤™**æ„å‘³ç€æ±‡é›†**:æˆ‘ä»¬å°†è€ƒè™‘æ¯ä¸ªéšè—çŠ¶æ€ç»´åº¦çš„`max_len` ç»´åº¦åµŒå…¥çš„å¹³å‡å€¼ï¼Œè€Œä¸æ˜¯é€‰æ‹©ç¬¬ä¸€ä¸ªå…ƒç´ :æˆ‘ä»¬è·å¾—ä¸€ä¸ª`[batch_size, 1, hidden_states_dim]`çš„å¼ é‡ï¼Œæˆ–è€…ä»…ä»…æ˜¯æœªæ’åºçš„å½¢å¼:`[batch_size, hidden_states_dim]`

```
def mean_pooling(inputs, outputs):
    """
    For each hidden_state, average along with max_len embeddings, 
    but we will condider only the highlighted tokens by the attention mask

    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field
    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]
    @return:  tensor of dimensions = [batch_size, hidden_states_dim]
    """
    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()
    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)
    sum_mask = input_mask_expanded.sum(1)
    sum_mask = torch.clamp(sum_mask, min=1e-9)
    mean_embeddings = sum_embeddings / sum_mask
    return mean_embeddings
```

ğŸ¤™**æœ€å¤§æ±‡é›†**:ä¸ºäº†å¾—åˆ°æœ€å¤§æ±‡é›†ï¼Œæˆ‘ä»¬å°†åœ¨æ¯ä¸ªéšè—çŠ¶æ€ç»´åº¦ä¸Šçš„`max_len`åµŒå…¥ä¸­å–æœ€å¤§å€¼ï¼Œç»“æœæ˜¯ä¸€ä¸ª`[batch_size, hidden_states_dim]`ç»´åº¦çš„å¼ é‡

```
def max_pooling(inputs, outputs):
    """
    For each hidden_state, get the max element along with max_len embeddings,
    considering only the non masked element difined by the attention mask computed by the tokenizer

    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field
    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]
    @return:  tensor of dimensions = [batch_size, hidden_states_dim]

    """
    last_hidden_state = outputs.last_hidden_state
    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()
    last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value
    max_embeddings = torch.max(last_hidden_state, 1).values
    return max_embeddings
```

ğŸ¤™**å¹³å‡æœ€å¤§æ± åŒ–:**æˆ‘ä»¬åº”ç”¨å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ï¼Œç„¶åè¿æ¥ä¸¤è€…ä»¥è·å¾—`[batch_size, 2*hidden_states_dim]`ç»´åº¦å¼ é‡

```
def mean_max_pooling(inputs, outputs):
    """
    Apply mean and max-pooling embeddings, then we concatenate the two onto a single final representation

    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]
    @return:  tensor of dimensions = [batch_size, 2*hidden_states_dim]
    """
    mean_pooling_embeddings = mean_pooling(inputs, outputs)
    max_pooling_embeddings = max_pooling(inputs, outputs)
    mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)
    return mean_max_embeddings
```

ä¸‹é¢æ˜¯è·å–æ‰€æœ‰åµŒå…¥çš„ä»£ç ç¤ºä¾‹:

```
 def get_embedding(dataloader, model, n):
    """
    Run the model to predict hidden states then apply all the transformations implemented above

    @param dataloader: a torch.utils.data.DataLoader the iterator along with the custom torch.utils.data.Dataset
    @param model : the huggingface AutoModel that generates the hidden states
    """
    embeddings = {}
    model = model.to(CONFIG.get('device'))
    for batch in tqdm_notebook(dataloader):
        with torch.no_grad():
            # please note here that the labels fileds is not necessary 
            # since we are not going to fine tune the model but just get the vectors output
            outputs = model(
                input_ids=batch['input_ids'].squeeze(),
                attention_mask=batch['attention_mask'].squeeze()
            )
        for embed_name, embed_func in zip(['cls_embeddings', "mean_pooling", "max_pooling", "mean_max_pooling"], 
                                          [cls_embedding, mean_pooling, max_pooling, mean_max_pooling]):
            if embed_name == 'cls_embeddings':
                embed = embed_func(outputs)
            else:
                embed = embed_func(batch, outputs)
            embeddings[embed_name] = torch.cat(
                (
                    embeddings.get(embed_name, torch.empty(embed.size()).to(CONFIG.get('device'))), 
                    embed
                ),
                0
            )
    threshold = min(n,CONFIG.get('train_batch_size'))
    for key in embeddings:
        embeddings[key] = embeddings[key][threshold:,:]
    return embeddings
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ç”ŸæˆåµŒå…¥:

```
model = AutoModel.from_pretrained(CONFIG["model_name"], config=config)
# TRAIN #
df_iter = CustomIterator(train, tokenizer)
train_dataloader = torch.utils.data.DataLoader(
    df_iter, 
    batch_size=CONFIG["train_batch_size"],
    shuffle=False
)
embeddings = get_embedding(train_dataloader, model, n=len(train))
# TEST #
df_iter = CustomIterator(test, tokenizer, is_train=False)
test_dataloader = torch.utils.data.DataLoader(
    df_iter, 
    batch_size=CONFIG["train_batch_size"],
    shuffle=False
)
test_embeddings = get_embedding(test_dataloader, model, n=len(test))
```

# ğŸ”éšè—çŠ¶æ€å¯è§†åŒ–:

åœ¨è®­ç»ƒåˆ†ç±»å™¨ä»¥è·å¾— 2D å¯è§†åŒ–ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹åµŒå…¥ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åªå¯¹`cls_embeddings`åº”ç”¨å®ƒï¼Œå¯¹äºå…¶ä»–ç±»å‹çš„åµŒå…¥ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚

æˆ‘ä»¬å¿…é¡»å°†éšè—çŠ¶æ€å‡å°‘åˆ° 2Dï¼Œè®¸å¤šæœ‰æ•ˆçš„æ¨¡å‹å¯ä»¥ç”¨æ¥å‡å°‘åµŒå…¥çš„ç»´æ•°: [UMAP](https://umap-learn.readthedocs.io/en/latest/) ï¼Œ[ä¸»æˆåˆ†åˆ†æ](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)ï¼Œ [T-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)

æˆ‘ä»¬å°†ä½¿ç”¨ PCA ç®—æ³•:
1ã€‚**æ•°æ®æ ‡å‡†åŒ–**:ä½¿ç”¨ scikit learn
2 çš„`StandardScaler`æ ‡å‡†åŒ–åµŒå…¥ã€‚ **2D é™ç»´**:æ‹ŸåˆåµŒå…¥çš„ PCA æ¨¡å‹ï¼Œæå–å‰ä¸¤ä¸ªåˆ†é‡
3ã€‚ **Hexbin å¯è§†åŒ–**:å¯¹äºæ¯ä¸ªç›®æ ‡ç±»ï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–æ¯ä¸ªåˆ†æ•°çš„ bin(ä» 1 åˆ° 5ï¼Œæ­¥é•¿= 0.5)

æˆ‘ä»¬æ¥çœ‹çœ‹è¯æ±‡è¯¾å‰§æƒ…:

![](img/a1aa560d2f3a93459232c0de2a595dea.png)

è¯æ±‡å¾—åˆ†åˆ†å¸ƒçš„ Hexbin å›¾ï¼Œä»¥åŠ cls _ embeddings çš„å‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†

PS:è¿™ä¸ªå¯è§†åŒ–éƒ¨åˆ†æŠ€æœ¯å¾ˆå— t [çš„ HuggingFace NLP GitHub ä¾‹å­](https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb)çš„å¯å‘

ä»è¿™ä¸ªå›¾ä¸­å¯ä»¥çœ‹å‡ºä¸€äº›æ¨¡å¼:å¯¹å¤§å¤šæ•°äººæ¥è¯´ï¼Œæç«¯åˆ†æ•°æ˜¯åˆ†å¼€çš„ï¼Œ2.5 åˆ†ä¼¼ä¹åˆ†æ•£åœ¨æ‰€æœ‰åœ°æ–¹ï¼Œè€Œå¯¹å…¶ä»–äººæ¥è¯´ï¼Œæœ‰æ˜æ˜¾çš„é‡å ã€‚

PS:ä¸è¦å¿˜è®°ï¼Œè¿™äº›åµŒå…¥æ˜¯ç”±ä¸€ä¸ªæ¨¡å‹**ç”Ÿæˆçš„ï¼Œè¯¥æ¨¡å‹é¢„å…ˆè®­ç»ƒç”¨äºé¢„æµ‹å¥å­ä¸­çš„å±è”½è¯**ï¼Œè€Œä¸æ˜¯å¯¹åˆ†æ•°è¿›è¡Œåˆ†ç±»ã€‚

# âš™å¤šå…ƒå›å½’å¤´åŸ¹è®­

è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„åµŒå…¥ä¸Šè®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’æ¨¡å‹:æˆ‘é€‰æ‹©äº†ä¸€ä¸ªåŸºäºæ¢¯åº¦æ¨è¿›çš„æ¨¡å‹: **Xgboost**

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­:å¤šç±»å›å½’ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ scikit-learn çš„`MultiOutputRegressor`ä¼°è®¡å™¨ã€‚æˆ‘ä¼šè®©ä½ ä» [@SWIMMY ä¼˜ç§€ç¬”è®°æœ¬](https://www.kaggle.com/code/swimmy/stacking-xgboost-lgbm-ridge-catboost)ä¸­æŸ¥çœ‹ä¸åŒçš„åŸºäºæ ‘çš„æ¨¡å‹+ç”¨ä¸€ä¸ªå…ƒæ¨¡å‹å †å çš„åŸå§‹å’Œè¿›ä¸€æ­¥çš„å®ç°ã€‚

ä¸ºäº†æŸ¥çœ‹å“ªä¸ªæ± å…·æœ‰æœ€ä½³çš„åˆ†ç¦»è¡¨ç¤ºï¼Œæˆ‘ä»¬å°†å¯¹æ¯ä¸ªæ± åµŒå…¥ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°ã€‚å…¨å±€åº¦é‡åŒ…æ‹¬å¹³å‡ 6 ä¸ªç›®æ ‡åˆ—çš„ RMSE:è¿™ä¸ªåº¦é‡ç§°ä¸º **MCRMSE** (å¹³å‡åˆ—å‡æ–¹æ ¹è¯¯å·®)

è®©æˆ‘ä»¬å®šä¹‰è¯„ä¼°æŒ‡æ ‡:

```
def comp_score(y_true,y_pred):
    rmse_scores = []
    for i in range(len(CONFIG['label_cols'])):
        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))
    return np.mean(rmse_scores)
```

ç°åœ¨å¼€å§‹ç®€å†åŸ¹è®­:

```
import joblib
y_true = train[CONFIG['label_cols']].values
cv_rmse = pd.DataFrame(0, index=range(N_FOLDS), columns=embeddings.keys())

oof_pred = {
        emb_type : np.zeros((len(train), len(label_cols)))
        for emb_type in embeddings
    }

for emb_type, emb in embeddings.items(): 
    print(f"CV for {emb_type}")
    emb = normalize(
        emb, 
        p=1.0, 
        dim = 1
    ).cpu()

    for fold, val_fold in train.groupby('fold'):
        print(f"*** FOLD == {fold} **")
        x_train, x_val = np.delete(emb, val_fold.index, axis=0), emb[val_fold.index]
        y_train, y_val = np.delete(y_true, val_fold.index, axis=0), y_true[val_fold.index]
        xgb_estimator = xgb.XGBRegressor(
                n_estimators=500, random_state=0, 
                objective='reg:squarederror')
        # create MultiOutputClassifier instance with XGBoost model inside
        xgb_model = MultiOutputRegressor(xgb_estimator, n_jobs=2)
        # model4 = XGBClassifier(early_stopping_rounds=10)
        xgb_model.fit(x_train, y_train)
        oof_pred[emb_type][val_fold.index] = xgb_model.predict(x_val)
        for i, col in enumerate(CONFIG['label_cols']):
            rmse_fold = np.sqrt(mean_squared_error(y_val[:,i], oof_pred[emb_type][val_fold.index,i]))
            print(f'{col} RMSE = {rmse_fold:.3f}')

        cv_rmse.loc[fold, emb_type] = comp_score(y_val, oof_pred[emb_type][val_fold.index])
        print(f'COMP METRIC = {cv_rmse.loc[fold, emb_type]:.3f}')        
        joblib.dump(xgb_model, f'xgb_{emb_type}_{fold}.pkl')
```

åœ¨ CV è®­ç»ƒæœŸé—´ï¼Œå¯¹äºæ¯ç§åµŒå…¥ç±»å‹ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¿æŒä¸€ä¸ªæŠ˜å åˆ†å¼€ï¼Œå¹¶ä¸”åœ¨å‰©ä½™çš„æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬é¢„æµ‹çœ‹ä¸è§çš„è¤¶çš±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è·å¾—äº†éæŠ˜å é¢„æµ‹(OOF é¢„æµ‹)ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªé¢„æµ‹éƒ½æ˜¯åœ¨çœ‹ä¸è§çš„æ•°æ®ä¸Šå®Œæˆçš„ã€‚

ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°æ¯ä¸ªç±»ä¸Šçš„ RMSE å’Œæ¯ä¸ªåµŒå…¥ç±»å‹çš„ OOF é¢„æµ‹ä¸Šçš„å…¨å±€ MCRMSE:æˆ‘ä»¬è·å¾—ä»¥ä¸‹æ€§èƒ½:

![](img/5763a3b61da5c26e39161d4cdfe2984c.png)

æŒ‰åµŒå…¥ç±»å‹åˆ’åˆ†çš„ OOF XGBoost æ€§èƒ½

*   çœ‹èµ·æ¥**å¹³å‡æœ€å¤§æ± **æä¾›äº†æœ€å¥½çš„æ€§èƒ½ï¼Œä½†æ˜¯**å¹³å‡æ± **éå¸¸æ¥è¿‘ã€‚ç”±äºå¹³å‡æœ€å¤§æ± çš„å®¹é‡æ˜¯å¹³å‡æ± çš„ä¸¤å€ï¼Œæˆ‘ä»¬å°†é€‰æ‹©å¹³å‡ç¼–ç æ¥å¾®è°ƒè½¬æ¢å™¨
*   æˆ‘ä»¬è¯­æ–™åº“ä¸­æ•ˆç‡æœ€ä½çš„æ± æ–¹æ³•æ˜¯**æœ€å¤§æ± **
*   å¯¹äºåˆ†æåº¦é‡çš„æ‰€æœ‰è¡¨ç¤ºï¼Œè¯æ±‡æ˜¯æœ€å®¹æ˜“ä¼°è®¡çš„ç›®æ ‡(æœ€ä½çš„ RMSE)ï¼Œè€Œå†…èšæ˜¯æœ€éš¾ä¼°è®¡çš„ç›®æ ‡(æœ€é«˜çš„ RMSE)

ä¸ºäº†æ¨æ–­æ–°çš„é¢„æµ‹ï¼Œæˆ‘çš„ä¸€ä¸ªæœ‹å‹ [Mathurin AchÃ©](https://www.kaggle.com/mathurinach) ï¼Œä»–ä¹Ÿæ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„æ•°æ®ç§‘å­¦å®¶å’Œ Kaggle å¤§å¸ˆï¼Œæ•™äº†æˆ‘ä¸¤ä¸ªæ–¹æ³•:

1.  ä½¿ç”¨ CV æ¨¡å‹æ¥é¢„æµ‹æ–°çš„æ•°æ®:å¯¹é¢„æµ‹è¿›è¡Œå¹³å‡å¯ä»¥**å‡å°‘æ–¹å·®**ä½†æ˜¯ä¼šå¢åŠ æ¯ä¸ªæ ·æœ¬æ—¶é—´çš„é¢„æµ‹ï¼Œå¦‚æœæˆ‘ä»¬å¿…é¡»éƒ¨ç½²è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»ä¿å­˜æ‰€æœ‰çš„ CV æ¨¡å‹
2.  å¯¹æ•´ä¸ªè®­ç»ƒæ•°æ®è®­ç»ƒä¸€æ¬¡ä¿ç•™çš„æ¨¡å‹ï¼Œç„¶åé¢„æµ‹æ–°çš„é¢„æµ‹ï¼Œè¿™å¯èƒ½ä¼šç¨å¾®å¢åŠ æ³›åŒ–è¯¯å·® aï¼Œä½†åœ¨éƒ¨ç½²çš„æƒ…å†µä¸‹(é¢„æµ‹æ—¶é—´å’Œè¦ç›‘è§†çš„å•ä¸ªæ¨¡å‹)å»ºè®®è¿™æ ·åš

åªè¦æˆ‘ä»¬å¤„äº Kaggle ç«äº‰ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å°±é€‰æ‹©ç¬¬ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨å‡å€¼-æœ€å¤§å€¼æ± :

```
import glob
# init output with zeros
xgb_infer = np.zeros((len(test), len(label_cols)))
for model_path in glob.glob("./xgb_mean_max_pooling_*.pkl"):
    print(f"load {model_path} model")
    xgb_model = joblib.load(model_path)
    emb = normalize(
        test_embeddings["mean_max_pooling"], 
        p=1.0, 
        dim = 1
    ).cpu()
    # add fold model prediction
    xgb_infer = np.add(xgb_infer, xgb_model.predict(emb))
# devide by the number of folds
xgb_infer = xgb_infer*(1/N_FOLDS)
```

# ğŸ™å­¦åˆ†:

æˆ‘åœ¨è¿™ä¸€éƒ¨åˆ†çš„å·¥ä½œå—åˆ°äº†è¿™äº›ä¼˜ç§€èµ„æºçš„å¯å‘ï¼Œè¯·ä¸è¦çŠ¹è±«å»æŸ¥é˜…å®ƒä»¬:

*   [**@rhtsingh** ç¬”è®°æœ¬](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently):æ¢ç´¢å˜å‹å™¨è¡¨è±¡çš„ä¸åŒæ–¹å¼
*   [**@Y.NAKAMA** ç¬”è®°æœ¬](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train):æŸå¤±å‡½æ•°ä¸å¤šæ ‡ç­¾åˆ†å±‚äº¤å‰éªŒè¯
*   [Huggingface NLP GitHub](https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb) :é™ç»´åŠå˜å‹å™¨åµŒå…¥å¯è§†åŒ–

# ç»“è®º:

æ„Ÿè°¢æ‚¨é˜…è¯»æˆ‘çš„å¸–å­ğŸ¤—å¸Œæœ›æœ‰ç”¨ï¼æé†’ä¸€ä¸‹ï¼Œæˆ‘æ‰€æœ‰çš„ä½œå“éƒ½å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°[ğŸã€‚](https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners)

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„è½¬æ¢å™¨æ¥æå–ä¸Šä¸‹æ–‡æ•è·åµŒå…¥ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’å™¨(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ Xgboost)æ¥å¯¹å­¦ç”Ÿè®ºæ–‡çš„åˆ†ææŒ‡æ ‡è¿›è¡Œå»ºæ¨¡ã€‚

åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†è§£å†³åŒæ ·çš„é—®é¢˜ï¼Œä½†è¿™æ¬¡æ˜¯é€šè¿‡**å¾®è°ƒ**è½¬æ¢å™¨ï¼Œå¹¶æ›´æ–°å…¶æ‰€æœ‰çš„ç¼–ç å™¨å †æ ˆã€‚æ­¤å¤–ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [**æƒé‡&åå·®**](https://wandb.ai/site) å¤§å¹³å°æ¥è·Ÿè¸ªæ¨¡å‹æ€§èƒ½å¹¶åˆ›å»ºæ¨¡å‹å·¥ä»¶ã€‚å¤šå…ƒå›å½’ä»»åŠ¡çš„è½¬æ¢å™¨