# å¤šå…ƒå›å½’çš„å˜å½¢é‡‘åˆšâ€”[ç¬¬äºŒéƒ¨åˆ†]

> åŸæ–‡ï¼š<https://pub.towardsai.net/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c?source=collection_archive---------4----------------------->

# ğŸ¤–å¾®è°ƒğŸ¤–

åœ¨ [FB3 ç«èµ›](https://www.kaggle.com/competitions/feedback-prize-english-language-learning)çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨ 8-12 å¹´çº§è‹±è¯­å­¦ä¹ è€…å†™çš„é¢„å…ˆè¯„åˆ†çš„è®®è®ºæ–‡å»ºç«‹å…­ä¸ªåˆ†ææŒ‡æ ‡æ¨¡å‹ã€‚æˆ‘ä»¬å¿…é¡»æ¨¡æ‹Ÿçš„æŠ€èƒ½å¦‚ä¸‹:**è¡”æ¥ã€å¥æ³•ã€è¯æ±‡ã€æªè¾ã€è¯­æ³•**å’Œ**çº¦å®š**ã€‚åˆ†æ•°èŒƒå›´ä» 1.0 åˆ° 5.0ï¼Œå¢é‡ä¸º 0.5ã€‚

![](img/57ecb0c8dc8d8c9903e0758d96e20555.png)

è‡´æ•¬é©¬èµ›å°”Â·æ™®é²æ–¯ç‰¹:@ [é©¬é‡Œå¥¥Â·å¸ƒé›·è¾¾](https://fr.dreamstime.com/marcel-proust-auteur-fran%C3%A7ais-illustration-vecteur-image131669085)

åœ¨æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„è½¬æ¢å™¨æ¥æå–ä¸Šä¸‹æ–‡æ•è·åµŒå…¥ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒä¸€ä¸ªå¤šå…ƒå›å½’å™¨ã€‚

è¿™æ¬¡æˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å¯¹æ•´ä¸ªå˜å‹å™¨è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¿™ä¹Ÿæ„å‘³ç€æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚

æ­¤å¤–ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ weights and biases å¹³å°:ä»ä½¿ç”¨ **wandb** API ç™»å½•ï¼Œåˆ°åˆ›å»ºå’Œä½¿ç”¨æ¨¡å‹å·¥ä»¶ä»¥åŠé€šè¿‡æ¨¡å‹è·Ÿè¸ªã€‚

æ‰€æœ‰ä»£ç æºéƒ½å¯ä»¥ä»[æˆ‘çš„ Kaggle ç¬”è®°æœ¬](https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners)ä¸­æ£€ç´¢åˆ°

**ä¿¡ç”¨**:è¿™éƒ¨åˆ†æˆ‘å€Ÿç”¨äº† [@debarshichanda](https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#notebook-container) æ¨¡å‹çš„æ¶æ„ã€‚

# ğŸ›Imports å’Œé…ç½®

é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®šä¹‰`CONFIG`å­—å…¸å’Œä¸ transformer ç›¸å…³çš„å¯¼å…¥ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨å®ƒä»¬:

```
import torch
import torch.nn as nn
import transformers
from transformers import (
    AutoModel, AutoConfig, 
    AutoTokenizer, logging,
    AdamW, get_linear_schedule_with_warmup,
    DataCollatorWithPadding,
    Trainer, TrainingArguments
)
from transformers.modeling_outputs import SequenceClassifierOutput

logging.set_verbosity_error()
logging.set_verbosity_warning()

CONFIG = {
    "model_name": "microsoft/deberta-v3-base",# "distilbert-base-uncased",
    "device": 'cuda' if torch.cuda.is_available() else 'cpu',
    "dropout": random.uniform(0.01, 0.60),
    "max_length": 512,
    "train_batch_size": 8,
    "valid_batch_size": 16,
    "epochs": 10,
    "folds" : 3,
    "max_grad_norm": 1000,
    "weight_decay": 1e-6,
    "learning_rate": 1e-5,
     "loss_type": "rmse",
    "n_accumulate" : 1,
    "label_cols" : ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], 

}
```

# ğŸ§®Custom æ•°æ®é›†è¿­ä»£å™¨:

å¦‚å‰ä¸€ç¯‡æ–‡ç« æ‰€è§£é‡Šçš„ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª`torch.utils.data.Dataset`çš„å­ç±»ï¼Œå¹¶è¦†ç›–`__init__`ã€`__len__`å’Œ`__getitem__`ç‰¹æ®Šæ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
import pandas as pd

train = pd.read_csv(PATH_TO_TRAIN)
test = pd.read_csv(PATH_TO_TEST)

# lets define the batch genetator
class CustomIterator(torch.utils.data.Dataset):
    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):
        self.df = df
        self.tokenizer = tokenizer
        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length
        self.labels = labels
        self.is_train = is_train

    def __getitem__(self,idx):
        tokens = self.tokenizer(
                    self.df.loc[idx, 'full_text'],#.to_list(),
                    add_special_tokens=True,
                    padding='max_length',
                    max_length=self.max_seq_length,
                    truncation=True,
                    return_tensors='pt',
                    return_attention_mask=True
                )     
        res = {
            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),
            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()
        }

        if self.is_train:
            res["labels"] = torch.tensor(
                self.df.loc[idx, self.labels].to_list(), 
            ).to(CONFIG.get('device')) 

        return res

    def __len__(self):
        return len(self.df)
```

# ğŸ¤–å¾®è°ƒå˜å‹å™¨

ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œéšè—çŠ¶æ€ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯å¯è®­ç»ƒçš„:ä¸ºæ­¤ï¼Œå®ƒè¦æ±‚åˆ†ç±»å¤´æ˜¯**å¯å¾®åˆ†çš„ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œä½œä¸ºåˆ†ç±»å™¨ã€‚**

![](img/876fd3f755637417b3a6c3f5f849c015.png)

Zeineb Ghrib

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ HuggingFace æä¾›çš„ç®€å•ä¸”åŠŸèƒ½å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼° API:`Trainer`åŸºäº`microsoft/deberta-v3-base`é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒç¼–ç å™¨å˜å‹å™¨ã€‚

æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ï¼Œç”¨ä¸€ä¸ªå¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œå¤´æ¥æ‰©å±•`microsoft/deberta-v3-base`ã€‚

å®šåˆ¶æ¨¡å‹å°†åŒ…æ‹¬:

*   **é¢„è®­ç»ƒåŸºçº¿æ¨¡å‹**:åŠ è½½å¸¦æœ‰`AutoModel.from_pretrained`åŠŸèƒ½çš„é¢„è®­ç»ƒ`microsoft/deberta-v3-base`
*   **å¹³å‡æ± å±‚**:æˆ‘ä»¬éœ€è¦å¯¹ä¹‹å‰çš„å¹³å‡æ± å‡½æ•°åšä¸€äº›ä¿®æ”¹(å‚è§[ç¬¬ä¸€éƒ¨åˆ†](https://medium.com/@zghrib/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9)å¸–å­):ä»`torch.nn.Module`ç»§æ‰¿æ± ç±»ï¼Œå¹¶åœ¨`forward`æ–¹æ³•ä¸­å®šä¹‰å¹³å‡æ± å‡½æ•°(å‚è§ä¸‹é¢çš„ä»£ç )
*   **æ¼å¤±å±‚**:æ·»åŠ ä¸€ä¸ªæ¼å¤±å±‚è¿›è¡Œæ­£åˆ™åŒ–
*   **çº¿æ€§å›¾å±‚**:è¾“å…¥å°ºå¯¸= `hidden_state_dim`ï¼Œè¾“å‡ºå°ºå¯¸=ç›®æ ‡ç‰¹å¾æ•°é‡(6)

çº¿æ€§å±‚çš„ logits è¾“å‡ºé€šè¿‡`forward`æ–¹æ³•ä¸Šçš„`SequenceClassifierOutput`-`ModelOutput`ç±»çš„å­ç±»è¿”å›(æ‰€æœ‰æ¨¡å‹çš„è¾“å‡ºå¿…é¡»æ˜¯`ModelOutput`å­ç±»çš„å®ä¾‹:[å¼•ç”¨æ­¤å¤„ä¸º](https://huggingface.co/docs/transformers/main_classes/output))

```
class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()  
    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        mean_embeddings = sum_embeddings / sum_mask
        return mean_embeddings

class FeedBackModel(nn.Module):
    def __init__(self, model_name):
        super(FeedBackModel, self).__init__()
        self.config = AutoConfig.from_pretrained(model_name)
        self.config.hidden_dropout_prob = 0
        self.config.attention_probs_dropout_prob = 0
        self.model = AutoModel.from_pretrained(model_name, config=self.config)
        self.drop = nn.Dropout(p=0.2)
        self.pooler = MeanPooling()
        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))

    def forward(self, input_ids, attention_mask):
        out = self.model(input_ids=input_ids,
                         attention_mask=attention_mask, 
                         output_hidden_states=False)
        out = self.pooler(out.last_hidden_state, attention_mask)
        out = self.drop(out)
        outputs = self.fc(out)
        return SequenceClassifierOutput(logits=outputs)
```

## æŸå¤±å’Œåº¦é‡

ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨`Trainer`ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå¯¹åº”äºç›®æ ‡è¯„ä¼°æŒ‡æ ‡çš„æ–°æŸå¤±å‡½æ•°(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸º MCRMSE)ã€‚è¯¥æŸå¤±å‡½æ•°å°†ç”¨äºè®­ç»ƒå˜å‹å™¨ã€‚å®ç°çš„æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ª`subclassing Trainer`å¹¶è¦†ç›–`compute_loss()`æ–¹æ³•ã€‚

åŒæ ·ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¯„ä¼°æ­¥éª¤ä¸­è·å¾—æ¯ä¸ªç›®æ ‡ç±»çš„å±€éƒ¨è¯„ä¼°ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸º`Trainer`æä¾›ä¸€ä¸ªè‡ªå®šä¹‰çš„`compute_metrics()`å‡½æ•°ï¼Œè¯¥å‡½æ•°å…è®¸è®¡ç®—å…­ä¸ªç›®æ ‡ä¸­æ¯ä¸ªç›®æ ‡çš„ RMSE(å¦åˆ™ï¼Œè¯„ä¼°å°†åªè¿”å›æŸå¤±è¯„ä¼° MCRMSE)ã€‚

```
class RMSELoss(nn.Module):
    """
    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)
    """
    def __init__(self, reduction='mean', eps=1e-9):
        super().__init__()
        self.mse = nn.MSELoss(reduction='none')
        self.reduction = reduction
        self.eps = eps

    def forward(self, predictions, targets):
        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)
        if self.reduction == 'none':
            loss = loss
        elif self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'mean':
            loss = loss.mean()
        return loss

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        outputs = model(inputs['input_ids'], inputs['attention_mask'])
        loss_func = RMSELoss(reduction='mean')
        loss = loss_func(outputs.logits.float(), inputs['labels'].float())
        return (loss, outputs) if return_outputs else loss

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))
    res = {
        f"{analytic.upper()}_RMSE" : colwise_rmse[i]
        for i, analytic in enumerate(CONFIG["label_cols"])
    }
    res["MCRMSE"] = np.mean(colwise_rmse)
    return res
```

# ğŸ§šWeights å’Œ BiasesğŸ§š

å°½ç®¡ HuggingFace Transformers æä¾›äº†å¹¿æ³›çš„è®­ç»ƒæ£€æŸ¥ç‚¹åŠŸèƒ½ã€‚W & B æä¾›å¼ºå¤§çš„å®éªŒè·Ÿè¸ªå’Œæ¨¡å‹ç‰ˆæœ¬åŒ–å·¥å…·ï¼Œå¸¦æœ‰å‹å¥½çš„äº¤äº’å¼ä»ªè¡¨ç›˜ã€‚æ¯ä¸ªå®éªŒé¡¹ç›®éƒ½æ˜¯ç‹¬ç«‹åˆ’åˆ†çš„ã€‚

æŸ¥çœ‹[è¿™æ¬¾å‡ºè‰²çš„ç¬”è®°æœ¬](https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases#%F0%9F%96%A5-Dashboard-(experiment-tracking)ï¼Œå®ƒè¯¦ç»†æè¿°äº†å¦‚ä½•åœ¨ kaggle ä¸­ä½¿ç”¨ W & B:

> *W&B æä¾›äº†ä¸¤ä¸ªä¸»è¦çš„å®ç”¨ç¨‹åº:*
> 
> ğŸ¤™**ä»ªè¡¨æ¿**(å®éªŒè·Ÿè¸ª):å®æ—¶è®°å½•å’Œå¯è§†åŒ–å®éªŒ=å°†æ•°æ®å’Œç»“æœä¿å­˜åœ¨ä¸€ä¸ªæ–¹ä¾¿çš„åœ°æ–¹ã€‚æŠŠè¿™çœ‹ä½œæ˜¯ä¸€ä¸ªå®éªŒçš„å®åº“ã€‚
> 
> ğŸ¤™**å·¥ä»¶**(æ•°æ®é›†+æ¨¡å‹ç‰ˆæœ¬åŒ–):å­˜å‚¨å’Œç‰ˆæœ¬åŒ–æ•°æ®é›†ã€æ¨¡å‹å’Œç»“æœ=ç¡®åˆ‡åœ°çŸ¥é“æ¨¡å‹è¢«è®­ç»ƒçš„æ•°æ®ã€‚

ä¸ºäº†è¿æ¥åˆ° Weights & Biasesï¼Œæˆ‘ä»¬éœ€è¦ä»[https://wandb.ai/authorize](https://wandb.ai/authorize)è®¿é—®æ‚¨çš„ API å¯†é’¥ã€‚
æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä½¿ç”¨ Kaggle å†…æ ¸ç™»å½•:

*   è¿è¡Œ`wandb.login(key=your-api-key)` cmd:å®ƒä¼šè¦æ±‚ API key:ä½ å¯ä»¥ä»[https://wandb.ai/authorize](https://wandb.ai/authorize)å¤åˆ¶/ç²˜è´´ã€‚

*å¦‚æœä½ ä¸ä½¿ç”¨ Kaggle* ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€éƒ¨åˆ†

*   ä½¿ç”¨ Kaggle secrets å­˜å‚¨æ‚¨çš„ API å¯†é’¥:å¹¶ä½¿ç”¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µç™»å½•ã€‚

1.  ç‚¹å‡»ç¬”è®°æœ¬ç¼–è¾‘å™¨ä¸­çš„`Add-ons`èœå•ï¼Œç„¶åç‚¹å‡»`Secrets`:

![](img/10c8db78e76639fdd664e0949ea47fb3.png)

2.å°† api-key å­˜å‚¨ä¸ºå°†é™„åŠ åˆ°å½“å‰ç¬”è®°æœ¬çš„é”®å€¼å¯¹:

![](img/257fe84457dc82921c33891157d60fb3.png)

3.å¤åˆ¶å¹¶ç²˜è´´ä»£ç ç‰‡æ®µä»¥è®¿é—® api-keyï¼Œç„¶åä½¿ç”¨`wandb.login()`è¿æ¥åˆ° W & B:

```
from kaggle_secrets import UserSecretsClient
import wandb

user_secrets = UserSecretsClient()
api_key = user_secrets.get_secret("wandb_api")
wandb.login(key=api_key)
```

# ğŸ› Wandb è®ºç‚¹

å¯¹äºæ¯ä¸ª CV è¿­ä»£â€˜I â€™,æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º **FB3-fold-i** çš„æ–°è¿è¡Œï¼Œå…¶ä¸­â€˜I â€™=è¿­ä»£çš„ç¼–å·ï¼Œåœ¨ä¸€ä¸ªåä¸º **Feedback3-deberta** çš„é¡¹ç›®ä¸­ã€‚

ä¸€äº›å…¶ä»–å‚æ•°:

*   `group`:ç¾¤ç»„å‚æ•°ç‰¹åˆ«ç”¨äºå°†å•ä¸ªå®éªŒç»„ç»‡æˆä¸€ä¸ªæ›´å¤§çš„å®éªŒï¼Œ[ä¸‹é¢æ˜¯ä¸€äº›ç”¨ä¾‹ç¤ºä¾‹](https://docs.wandb.ai/guides/track/advanced/grouping)
*   `tags`:æˆ‘ä»¬å°†æ·»åŠ å‹å·åç§°å’Œå…¬åˆ¶æ ‡ç­¾ã€‚æ­£å¦‚ W & B ä¸­æ‰€è§£é‡Šçš„ï¼Œdoc æ ‡ç­¾å¯¹äºç»„ç»‡ä¸€èµ·è¿è¡Œæˆ–åº”ç”¨ä¸´æ—¶æ ‡ç­¾å¦‚â€œåŸºçº¿â€æˆ–â€œç”Ÿäº§â€æ˜¯æœ‰ç”¨çš„ã€‚å¾ˆå®¹æ˜“åœ¨ UI ä¸­æ·»åŠ å’Œåˆ é™¤æ ‡ç­¾ï¼Œæˆ–è€…è¿‡æ»¤åˆ°åªè¿è¡Œç‰¹å®šçš„æ ‡ç­¾ã€‚
*   é€šå¸¸ä¸æ˜¯â€œè®­ç»ƒâ€å°±æ˜¯â€œè¯„ä¼°â€ã€‚ç¨åï¼Œå®ƒå°†å…è®¸å¯¹ç›¸ä¼¼çš„è¿è¡Œè¿›è¡Œè¿‡æ»¤å’Œåˆ†ç»„ã€‚æˆ‘ä»¬å°†æŠŠ job_type è®¾ç½®ä¸º**â€œtrainâ€**
*   `anonymous`:è¯¥å‚æ•°å…è®¸æ§åˆ¶åŒ¿åè®°å½•ã€‚æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸º**â€œmustâ€**ï¼Œè¿™å°†æŠŠè·‘æ­¥å‘é€åˆ°ä¸€ä¸ªåŒ¿åå¸æˆ·ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ³¨å†Œç”¨æˆ·å¸æˆ·ã€‚å¯¹äºå…¶ä»–é€‰é¡¹ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹[æ–‡æ¡£](https://docs.wandb.ai/ref/python/init)

å¯¹äºæ¯ä¸ª CV è¿­ä»£ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®ä¾‹åŒ–ä¸€ä¸ª wandb è¿è¡Œ:

```
run = wandb.init(project="FB3-deberta-v3", 
                   config=CONFIG,
                   job_type='train',
                   group="FB3-BASELINE-MODEL",
                   tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],
                   name=f'FB3-fold-{fold}',
                   anonymous='must')
```

ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰æ‹¥æŠ±è„¸`Trainer`å°†ä½¿ç”¨çš„è®­ç»ƒå‚æ•°

# ğŸ› Training è®ºç‚¹

åœ¨å®ä¾‹åŒ–æˆ‘ä»¬çš„å®šåˆ¶æ•™ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª`TrainingArguments`æ¥å®šä¹‰è®­ç»ƒé…ç½®ã€‚
æˆ‘ä»¬å°†è®¾ç½®ä»¥ä¸‹å‚æ•°:

*   `output_dir`:æ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹å°†è¢«å†™å…¥çš„è¾“å‡ºç›®å½•:æ¯ä¸ª CV è¿­ä»£å°†æœ‰å®ƒè‡ªå·±çš„ç›®å½•ï¼Œå…¶åç§°ç­‰äºä»¥**â€œè¾“å‡º-â€**ä¸ºå‰ç¼€çš„è¿­ä»£æ¬¡æ•°
*   `evaluation_strategy`:è®¾ç½®ä¸º**â€œepochâ€**ï¼Œè¡¨ç¤ºåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚
*   `per_device_train_batch_size`:è®­ç»ƒçš„æ‰¹é‡ã€‚æˆ‘ä»¬å°†å®ƒè®¾ç½®ä¸º 8
*   `per_device_eval_batch_size`:è¯„ä¼°çš„æ‰¹é‡ã€‚æˆ‘ä»¬å°†å®ƒè®¾ç½®ä¸º 16(ä»¥åŠ å¿«æ—¶é—´æ‰§è¡Œ)
*   `num_train_epochs`:è®­ç»ƒæ—¶æœŸæ•°ã€‚æé†’ä¸€ä¸‹ï¼Œåœ¨ä¸€ä¸ªæ—¶æœŸå†…ï¼Œæ¨¡å‹å·²ç»çœ‹åˆ°äº†è®­ç»ƒæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬
*   `group_by_length`:åªè¦æˆ‘ä»¬å°†ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œæˆ‘ä»¬å°±å°†è¯¥å‚æ•°è®¾ç½®ä¸º`True`ï¼Œä»¥å°†è®­ç»ƒæ•°æ®é›†ä¸­é•¿åº¦å¤§è‡´ç›¸åŒçš„æ ·æœ¬åˆ†ç»„åœ¨ä¸€èµ·(ä»¥æœ€å°åŒ–æ‰€åº”ç”¨çš„å¡«å……å¹¶æé«˜æ•ˆç‡)
*   `max_grad_norm`:æœ€å¤§æ¢¯åº¦èŒƒæ•°(ç”¨äºæ¢¯åº¦è£å‰ª)ã€‚
*   `learning_rate`:AdamW ä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ã€‚æé†’ä¸€ä¸‹ï¼ŒAdamW ä¼˜åŒ–å™¨
*   `weight_decay`:åº”ç”¨äº AdamW ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡:åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡è¡°å‡åº”ç”¨äºé™¤åå·®å’Œæ ‡å‡†åŒ–å±‚ä¹‹å¤–çš„æ‰€æœ‰å±‚

****æ³¨:**
æƒé‡è¡°å‡æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ƒç»™æŸå¤±å‡½æ•°(é€šå¸¸æ˜¯æƒé‡çš„ L2 èŒƒæ•°)å¢åŠ äº†ä¸€ä¸ªå°çš„æƒ©ç½šã€‚
æŸè€—=æŸè€—+æƒé‡ _ è¡°å‡ _ å‚æ•°* L2 _ èŒƒæ•° _ æƒé‡**

**ä¸€äº›å®ç°ä»…å¯¹æƒé‡è€Œä¸æ˜¯åå·®åº”ç”¨æƒé‡è¡°å‡ã€‚å¦ä¸€æ–¹é¢ï¼ŒPyTorch å°†æƒé‡è¡°å‡åº”ç”¨äºæƒé‡å’Œåç§»ã€‚**

> ****ä¸ºä»€ä¹ˆä½“é‡ä¼šè¡°å‡ï¼Ÿ****
> 
> **1.ä»¥é˜²æ­¢è¿‡åº¦æ‹Ÿåˆã€‚
> 2ã€‚ä¸ºäº†é¿å…çˆ†ç‚¸æ¢¯åº¦:ç”±äºé¢å¤–çš„ L2 èŒƒæ•°ï¼Œé™¤äº†æŸå¤±ä¹‹å¤–ï¼Œç½‘ç»œçš„æ¯æ¬¡è¿­ä»£å°†è¯•å›¾ä¼˜åŒ–æ¨¡å‹æƒé‡ã€‚è¿™å°†æœ‰åŠ©äºä¿æŒæƒé‡å°½å¯èƒ½å°ï¼Œé˜²æ­¢æƒé‡å¢é•¿å¤±æ§ï¼Œä»è€Œé¿å…çˆ†ç‚¸æ¢¯åº¦**

*   **`gradient_accumulation_steps`:åœ¨æ‰§è¡Œå‘åä¼ é€’ä¹‹å‰ï¼Œæ¢¯åº¦åº”è¯¥ç´¯ç§¯çš„æ­¥æ•°:å½“ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œæ¢¯åº¦è®¡ç®—æ˜¯åœ¨è¾ƒå°çš„æ­¥ä¸­å®Œæˆçš„ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¸ºä¸€æ‰¹å®Œæˆï¼›1(è¡¨ç¤ºæ²¡æœ‰æ¢¯åº¦ç´¯ç§¯))**

> ****æ³¨** :
> åœ¨æœ¬ [Stackoverflow è®¨è®º](https://stackoverflow.com/questions/74065165/getting-cuda-error-when-trying-to-train-mbart-model)ä¸­ï¼Œå·²ç»è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨ set `gradient_accumulation_steps`å‚æ•°æ¥é¿å… OOM é”™è¯¯:å°†`gradient_accumulation_steps`å‚æ•°è®¾ç½®ä¸ºä¸€ä¸ªé€‚åˆå†…å­˜çš„æ•°å­—ï¼Œå¹¶å°†`per_device_train_batch_size`ä¿®æ”¹ä¸º`original_batch_size/gradient_accumulation_steps`:è¿™æ ·æ¢¯åº¦å°†åœ¨`gradient_accumulation_steps`ä¸Šç´¯ç§¯ï¼Œå¹¶é€šè¿‡`gradient_accumulation_steps` * `original_batch_size/gradient_accumulation_steps` = `original_batch_size`æ ·æœ¬æ‰§è¡Œå‘åä¼ é€’ã€‚è®­ç»ƒæ­¥éª¤çš„æ€»æ•°æ˜¯:**

**![](img/2903204a08c267677da85ead89619bbc.png)**

*   **`load_best_model_at_end`:æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸º Trueï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹:åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`save_strategy`å¿…é¡»ä¸`evaluation_strategy` : **epoch** ç›¸åŒ**
*   **`metric_for_best_model`:æˆ‘ä»¬å°†è®¾ç½®ç«äº‰æŒ‡æ ‡ **MCRMSE** æˆ– **eval_MCRMSE** (å¸¦æœ‰ eval_ prefix)**
*   **`greater_is_better`:è®¾ç½®ä¸º Falseï¼Œå› ä¸ºæˆ‘ä»¬æƒ³å¾—åˆ° MCRMSE è¾ƒä½çš„æ¨¡å‹**
*   **`save_total_limit`:æˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸º 1ï¼Œä»¥ä¾¿æ¯æ¬¡æ€»æ˜¯ä¿ç•™ä¸€ä¸ªæ£€æŸ¥ç‚¹(output_dir ä¸­è¾ƒæ—§çš„æ£€æŸ¥ç‚¹å°†è¢«åˆ é™¤)ã€‚**
*   **`report_to`:ç”±äºæˆ‘ä»¬è¿æ¥åˆ° W & Bï¼Œæˆ‘ä»¬å°†æŠŠ report_to logs è®¾ç½®ä¸º**â€œwandbâ€****
*   **`label_name`:å°† label_name å‚æ•°åˆ—è¡¨è®¾ç½®ä¸º **["labels"]ï¼Œ**ï¼Œå¯¹åº”ç›®æ ‡ç±»å¯¹åº”çš„è‡ªå®šä¹‰`Dataloader`ç”Ÿæˆçš„é¢„å®šä¹‰å­—æ®µ**

```
training_args = TrainingArguments(
        output_dir=f"outputs-{fold}/",
        evaluation_strategy="epoch",
        per_device_train_batch_size=CONFIG['train_batch_size'],
        per_device_eval_batch_size=CONFIG['valid_batch_size'],
        num_train_epochs=CONFIG['epochs'],
        learning_rate=CONFIG['learning_rate'],
        weight_decay=CONFIG['weight_decay'],
        gradient_accumulation_steps=CONFIG['n_accumulate'],
        seed=SEED,
        group_by_length=True,
        max_grad_norm=CONFIG['max_grad_norm'],
        metric_for_best_model='eval_MCRMSE',
        load_best_model_at_end=True,
        greater_is_better=False,
        save_strategy="epoch",
        save_total_limit=1,
        report_to="wandb",
        label_names=["labels"]
    )
```

**æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä¸º`Trainer`å®šä¹‰ä¸€äº›å…¶ä»–å‚æ•°:**

*   ****æ•°æ®æ•´ç†å™¨**:æˆ‘ä»¬éœ€è¦å®šä¹‰å¦‚ä½•ä»`Dataloader` è¿”å›çš„æ•°æ®è¾“å…¥åˆ—è¡¨ä¸­åˆ›å»ºä¸€ä¸ªæ‰¹å¤„ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`DataCollatorWithPadding`æ¥åŠ¨æ€å¡«å……æ¥æ”¶åˆ°çš„è¾“å…¥ã€‚**
*   ****ä¼˜åŒ–å™¨**:æˆ‘ä»¬å°†åœ¨æ‰€æœ‰å±‚ä½¿ç”¨`AdamW`ï¼Œé™¤äº†åç½®å’Œæ ‡å‡†åŒ–å±‚**
*   ****è°ƒåº¦å™¨**:æˆ‘ä»¬å°†ä½¿ç”¨`get_linear_schedule_with_warmup`åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é¢„çƒ­æœŸçš„è°ƒåº¦ï¼Œåœ¨æ­¤æœŸé—´ï¼Œå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹ lr(åœ¨ä¼˜åŒ–å™¨ä¸­è®¾ç½®)ï¼Œç„¶åä»ä¼˜åŒ–å™¨ä¸­è®¾ç½®çš„åˆå§‹ lr çº¿æ€§å‡å°‘åˆ° 0ã€‚
    è°ƒåº¦å™¨å…è®¸æˆ‘ä»¬ä¿æŒå¯¹å­¦ä¹ ç‡çš„æ§åˆ¶ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ç¡®ä¿å­¦ä¹ ç‡çš„æ¯æ¬¡æ›´æ–°ä¸è¶…è¿‡Î»å€¼(æŸ¥çœ‹è¿™ä¸ª [Stackoverflow è®¨è®º](https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer)å…³äºä¼˜åŒ–å™¨è°ƒåº¦å™¨çš„æ•ˆç”¨)**

**è¦å¯åŠ¨äº¤å‰éªŒè¯åŸ¹è®­ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»æŒ‰ç…§[ç¬¬ä¸€éƒ¨åˆ†å¸–å­](https://zghrib.medium.com/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9)ä¸­çš„è¯´æ˜åˆ›å»ºç®€å†æŠ˜å æ :**

```
# set seed to produce similar folds
cv = MultilabelStratifiedKFold(n_splits=CONFIG.get("folds", 3), shuffle=True, random_state=SEED)

train = train.reset_index(drop=True)
for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[CONFIG['label_cols']])):
    train.loc[val_idx , "fold"] = int(fold)

train["fold"] = train["fold"].astype(int)
```

**CV åŸ¹è®­å·¥ä½œæµç¨‹å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å®æ–½:**

```
# Data Collator for Dynamic Padding
collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)
# init predictions by fold
predictions = {}
for fold in range(0, CONFIG['folds']):
    print(f" ---- Fold: {fold} ----")
    run = wandb.init(project="FB3-deberta-v3", 
                     config=CONFIG,
                     job_type='train',
                     group="FB3-BASELINE-MODEL",
                     tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],
                     name=f'FB3-fold-{fold}',
                     anonymous='must')
    # the reset index is VERY IMPORTANT for the Dataset iterator
    df_train = train[train.fold != fold].reset_index(drop=True)
    df_valid = train[train.fold == fold].reset_index(drop=True)
    # create iterators
    train_dataset = CustomIterator(df_train, tokenizer)
    valid_dataset = CustomIterator(df_valid, tokenizer)
    # init model
    model = FeedBackModel(CONFIG['model_name'])
    model.to(CONFIG['device'])

    # SET THE OPITMIZER AND THE SCHEDULER
    # no decay for bias and normalization layers
    param_optimizer = list(model.named_parameters())
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_parameters = [
        {
            "params": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
            "weight_decay": CONFIG['weight_decay'],
        },
        {
            "params": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = AdamW(optimizer_parameters, lr=CONFIG['learning_rate'])
    num_training_steps = (len(train_dataset) * CONFIG['epochs']) // (CONFIG['train_batch_size'] * CONFIG['n_accumulate'])
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0.1*num_training_steps,
        num_training_steps=num_training_steps
    )
    # CREATE THE TRAINER
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        data_collator=collate_fn,
        optimizers=(optimizer, scheduler),
        compute_metrics=compute_metrics
    )
    # LAUNCH THE TRAINER
    trainer.train()
```

**ä½ å¯ä»¥è®¿é—®æˆ‘ä¸ºè¿™ä¸ªé¡¹ç›®åˆ›å»ºçš„å…¬å…± W&B ä»ªè¡¨æ¿:[https://wandb.ai/athena75/FB3-deberta-v3?å·¥ä½œç©ºé—´=ç”¨æˆ·-é›…å…¸å¨œ 75](https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75)**

# **ğŸ—¿åˆ›å»º W&B å·¥ä»¶**

**å¯¹äºä»¥åçš„ä½¿ç”¨ï¼Œä¸€æ—¦æ¨¡å‹è¢«å¾®è°ƒï¼ŒW&B éå¸¸æ–¹ä¾¿åœ°åˆ›å»ºæ¨¡å‹å·¥ä»¶ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä»¥åä½¿ç”¨å®ƒä»¬ï¼Œå¹¶åˆ›å»ºæˆ‘ä»¬æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚**

**è¦åˆ›å»ºä¸€ä¸ªæ¨¡å‹å·¥ä»¶ï¼Œæ‚¨æ‰€è¦åšçš„å°±æ˜¯:**

1.  **åˆ›å»ºä¸€ä¸ªåå­—æ¸…æ™°ä¸€è‡´çš„`wandb.Artifact`å¯¹è±¡ï¼Œä½ å¿…é¡»æŒ‡å®š`type`å‚æ•°ï¼Œå®ƒå¯ä»¥æ˜¯`dataset`æˆ–`model`ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯`model`**
2.  **å°†æœ¬åœ°ç›®å½•æ·»åŠ åˆ°å·¥ä»¶ä¸­:äº‹å®ä¸Šï¼Œä¸€æ—¦æ‚¨å®ä¾‹åŒ–äº†æ¨¡å‹å¹¶å¼€å§‹å¾®è°ƒå®ƒï¼Œå®ƒå°±ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«æ¨¡å‹`bin`ä»¥åŠæ¨¡å‹çŠ¶æ€å’Œé…ç½®çš„æœ¬åœ°æ£€æŸ¥ç‚¹ã€‚ä½ å¾—æŠŠå®ƒåŠ åˆ°è‰ºæœ¯å“ä¸Šã€‚**
3.  **ä¸€æ—¦å·¥ä»¶æ‹¥æœ‰äº†æ‰€æœ‰éœ€è¦çš„æ–‡ä»¶ï¼Œæ‚¨å°±å¯ä»¥è°ƒç”¨`wandb.log_artifact()`æ¥è®°å½•å®ƒã€‚**

**ä¸‹é¢æ˜¯ä¸€ä¸ªä¸ºæ¯ä¸ª CV æ¨¡å‹åˆ›å»ºå·¥ä»¶çš„ä»£ç ç‰‡æ®µç¤ºä¾‹:**

```
for fold in range(0, CONFIG['folds']):
    run = wandb.init(project="FB3-deberta-v3", 
                         config=CONFIG,
                         job_type='train',
                         group="FB3-BASELINE-MODEL",
                         tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],
                         name=f'FB3-fold-{fold}',
                         anonymous='must')

    trainer = CustomTrainer(
              .....
    )
    ##### TRAIN / FINE-TUNE ####
    # create model artifact
    model_artifact = wandb.Artifact(f'FB3-fold-{fold}', type="model",
                                   description=f"MultilabelStratified - fold--{fold}")
    # save locally the model - it would create a local dir
    trainer.save_model(f'fold-{fold}')
    # add the local dir to the artifact
    model_artifact.add_dir(f'fold-{fold}')
    # log artifact
    # it would save the artifact version and declare the artifact as an output of the run
    run.log_artifact(model_artifact)

    run.finish() 
```

# **inferenceâœ¨:çš„âœ¨Use W&B è‰ºæœ¯å“**

**ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥**ä½¿ç”¨ Weights&bias æœåŠ¡å™¨ä¸Šå­˜å‚¨çš„å·¥ä»¶**ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”Ÿæˆæ¨¡å‹é¢„æµ‹å¹¶ç”Ÿæˆèšåˆè¾“å‡ºé¢„æµ‹ã€‚**

**PS:å¯ä»¥ç›´æ¥ä» W&B æ¥å£æå–ä½¿ç”¨ä»£ç [https://wandb . ai/Athena 75/feedback 3-deberta/artifacts/model/FB3-fold-0/93c 08783 e5b 7c 696451 a/usage](https://wandb.ai/athena75/Feedback3-deberta/artifacts/model/FB3-fold-0/93c08783e5b7c696451a/usage)**

1.  **ç™»å½•åˆ°æ‚¨çš„ wandb å¸æˆ·ï¼Œç”¨`wandb.init()`å®ä¾‹åŒ–ä¸€ä¸ªé»˜è®¤è¿è¡Œ**
2.  **å‘`use_artifact()`æ–¹æ³•æŒ‡å‡ºå·¥ä»¶çš„è·¯å¾„ä»¥åŠæ£€ç´¢å·¥ä»¶çš„ç±»å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯`model`)**
3.  **ä½¿ç”¨`download()`æ–¹æ³•åœ¨æœ¬åœ°ä¸‹è½½å·¥ä»¶ç›®å½•**
4.  **åŠ è½½æœ¬åœ°æ¨¡å‹å¹¶ä½¿ç”¨å®ƒè¿›è¡Œé¢„æµ‹**

**å®æ–½ç¤ºä¾‹:**

```
predictions = torch.zeros((len(test), len(CONFIG['label_cols']))

for fold in range(CONFIG["folds"]):
    print(f"---- FOLD {fold} -------")
    # instantiate deafault run
    run = wandb.init()
    # Indicate the artifact we want to use with the use_artifact method.
    artifact = run.use_artifact(f'athena75/FB3-deberta-10/FB3-fold-{fold}:v0', type='model')
    # download locally the model
    artifact_dir = artifact.download()
    # load the loacal model
    # it is a pytorch moeal: loaded as follows
    # https://pytorch.org/tutorials/beginner/saving_loading_models.html
    model = FeedBackModel(CONFIG['model_name'])
    model.load_state_dict(torch.load(f'artifacts/FB3-fold-{fold}:v0/pytorch_model.bin'))
    # generate test embediings
    test_dataset = CustomIterator(test, tokenizer, is_train=False)
    test_dataloader = torch.utils.data.DataLoader(
            test_dataset, 
            batch_size=CONFIG["train_batch_size"],
            shuffle=False
        )
    input_ids, attention_mask = tuple(next(iter(test_dataloader)).values())
    input_ids = input_ids.to('cpu')
    attention_mask = attention_mask.to('cpu')
    # genreate predictions
    fold_preds = model(input_ids, attention_mask)
    predictions = fold_preds.logits.add(predictions)
    # remove local dir to save space
    shutil.rmtree('artifacts')
```

# **ğŸ™å­¦åˆ†:**

**è¿™é¡¹å·¥ä½œæ˜¯è¿™äº›ä¼˜ç§€èµ„æºçš„ç‹¬ç‰¹èµ·æºï¼Œè¯·ä¸è¦çŠ¹è±«æŠ•ç¥¨æ”¯æŒ Kaggle èµ„æº:**

*   **[**@rhtsingh** ç¬”è®°æœ¬](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently):æ¢ç´¢å˜å‹å™¨è¡¨è±¡çš„ä¸åŒæ–¹å¼**
*   **[**@debarshichanda** ä½œå“](https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#Loss-Function):å€Ÿç”¨äº†è®­ç»ƒå™¨è¶…å‚æ•°å’Œæ¨¡å‹æ¶æ„**
*   **[**@Y.NAKAMA** ç¬”è®°æœ¬](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train):æŸå¤±å‡½æ•°å’Œå¤šæ ‡ç­¾â€”åˆ†å±‚äº¤å‰éªŒè¯**
*   **[**@shreydan** ç¬”è®°æœ¬](https://www.kaggle.com/code/shreydan/using-transformers-for-the-first-time-pytorch/notebook):ç¬”è®°æœ¬çš„å…¨çƒé£æ ¼**

# **ç»“è®º:**

**éå¸¸æ„Ÿè°¢ä½ é˜…è¯»æˆ‘çš„å¸–å­ï¼ğŸ¥°ï¼Œæˆ‘è·Ÿä½ åˆ†äº«æˆ‘æ‰€æœ‰çš„ä½œå“:è¿™ä¸ª [Kaggle ç¬”è®°æœ¬](https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners)ï¼Œè¿˜æœ‰æˆ‘çš„å…¬ [W & B ä»ªè¡¨ç›˜](https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75)ã€‚**

**æˆ‘å¸Œæœ›è¿™æ˜¯æ˜ç¡®çš„ï¼Œå¹¶éšæ—¶é—®æˆ‘é—®é¢˜ã€‚**

**ğŸ“¬æˆ‘çš„é‚®ä»¶åœ°å€æ˜¯:**schopenhacker75@gmail.com**ğŸ“¬**

**åœ¨åé¢çš„ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ‰“ç®—è®¨è®ºå¦‚ä½•åœ¨ç”Ÿäº§ä¸­éƒ¨ç½²ä¸€ä¸ªè½¬æ¢å™¨ã€‚æˆ–è€…å¦‚ä½•ä¸º NLP å˜å‹å™¨å»ºç«‹ä¸€ä¸ª **MLOps ç®¡é“ï¼Œ**æˆ‘è¿˜æ²¡æœ‰å†³å®šâ€¦**