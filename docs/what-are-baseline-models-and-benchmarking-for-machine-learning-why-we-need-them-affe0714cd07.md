# ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ çš„åŸºçº¿æ¨¡å‹å’ŒåŸºå‡†ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å®ƒä»¬ï¼Ÿç¬¬ 1 éƒ¨åˆ†åˆ†ç±»

> åŸæ–‡ï¼š<https://pub.towardsai.net/what-are-baseline-models-and-benchmarking-for-machine-learning-why-we-need-them-affe0714cd07?source=collection_archive---------1----------------------->

## [æœºå™¨å­¦ä¹ ](https://towardsai.net/p/category/machine-learning)

éšæœºã€æœºå™¨å­¦ä¹ ã€è‡ªåŠ¨åŒ– ML åŸºçº¿æ¨¡å‹å’Œ ML åŸºå‡†æµ‹è¯•â€¦

![](img/851805ff55299e02a12fbabc0e584fa1.png)

è·³é«˜å¥¥è¿â€”â€”æ¥æº[æ­¤å¤„](https://www.worldathletics.org/disciplines/jumps/high-jump)

æˆ‘ä»¬å¯ä»¥ç”¨ä»»ä½•å‡†å¤‡å¥½çš„æ•°æ®æ¥è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¦‚ä½•ç¡®å®šä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Ÿæœ¬æ–‡çš„ç›®çš„æ˜¯è§£é‡Šæ•°æ®ç§‘å­¦ä¸­çš„åŸºçº¿æ¨¡å‹ã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ•°æ®é›†ï¼Œä½ å¯ä»¥åœ¨æ–‡ç« æœ«å°¾çœ‹åˆ°å®Œæ•´çš„ python ä»£ç ã€‚

# ä»€ä¹ˆæ˜¯åŸºçº¿æ¨¡å‹ï¼Ÿ

åŸºçº¿æ¨¡å‹æ˜¯æˆ‘ä»¬è®­ç»ƒçš„ ML æ¨¡å‹çš„å‚è€ƒã€‚å¯¹äºåŸºçº¿æ¨¡å‹ï¼Œæ•°æ®ç§‘å­¦å®¶è¯•å›¾è§£é‡Šä»–ä»¬è®­ç»ƒçš„æ¨¡å‹å¦‚ä½•å¥½ï¼ŒåŸºçº¿æ¨¡å‹çš„åˆ†æ•°æ˜¯æ•°æ®ç§‘å­¦å®¶çš„é˜ˆå€¼ã€‚

# åŸºçº¿æ¨¡å‹æœ‰å“ªäº›ç±»å‹ï¼Ÿ

æœ‰ä¸‰ç§ç±»å‹çš„åŸºçº¿æ¨¡å‹ï¼Œå³éšæœºåŸºçº¿æ¨¡å‹ã€ML åŸºçº¿æ¨¡å‹å’Œè‡ªåŠ¨åŒ– ML åŸºçº¿æ¨¡å‹ã€‚

## éšæœºåŸºçº¿æ¨¡å‹

åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæ•°æ®å¹¶ä¸æ€»æ˜¯å¯ä»¥é¢„æµ‹çš„ã€‚åœ¨è¿™äº›é—®é¢˜ä¸­ï¼Œæœ€å¥½çš„åŸºçº¿æ¨¡å‹æ˜¯è™šæ‹Ÿåˆ†ç±»å™¨æˆ–è™šæ‹Ÿå›å½’å™¨ã€‚åŸºçº¿æ¨¡å‹æ˜¾ç¤ºä½ çš„ ml æ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°å¦‚ä½•ä½¿ç”¨éšæœºåŸºçº¿æ¨¡å‹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªéšæœºæ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚

```
import pandas as pd
import numpy as npnp.random.seed(0)
random_dim = (1000,3)
random_X = np.random.random(random_dim)
random_reg_y = np.random.random(random_dim[0])
random_clf_y = np.random.randint(random_dim[1], size=random_dim[0])train_clf = np.concatenate((random_X, random_clf_y.reshape(random_dim[0], 1)), axis=1)
col_list = [str(i +1) for i in range(random_dim[1])]
col_list.append('target')
train_clf = pd.DataFrame(train_clf, columns=col_list)train_clf['target'] = train_clf['target'].astype('str')
train_clf
```

![](img/9aed463bb748f656789d0e86bd6cdf2a.png)

éšæœºåˆ†ç±»æ•°æ®é›†-æŒ‰ä½œè€…åˆ†ç±»çš„å½±åƒ

ç„¶åæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ pycaret compare_models å‡½æ•°æ¥æ¯”è¾ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ ¹æ®ç»“æœï¼Œæœ€å¥½çš„æ¨¡å‹æ˜¯**è™šæ‹Ÿåˆ†ç±»å™¨**ï¼Œå› ä¸ºç‰¹å¾å’Œç›®æ ‡ä¹‹é—´æ²¡æœ‰å…³ç³»ã€‚

```
from pycaret.classification import *clf = setup(data=train_clf, 
            target='target', 
            numeric_features=col_list[:-1], 
            silent=True)compare_models(sort='Accuracy')
```

![](img/895ce6015197d5921221ec53555f54dd.png)

è™šæ‹Ÿåˆ†ç±»å™¨â€”æŒ‰ä½œè€…åˆ†ç±»çš„å›¾åƒ

## æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹

å¦‚æœæ•°æ®æ˜¯å¯é¢„æµ‹çš„ï¼Œç¬¬äºŒæ­¥æ˜¯åˆ›å»ºä¸€ä¸ª ml åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸ªåŸºçº¿æ¨¡å‹å‘æˆ‘ä»¬å±•ç¤ºäº†å“ªäº›ç‰¹å¾å¯¹äºé¢„æµ‹æ˜¯é‡è¦çš„ï¼Œå“ªäº›æ˜¯ä¸é‡è¦çš„ã€‚é€šå¸¸ï¼Œml åŸºçº¿æ¨¡å‹ä¸ç‰¹å¾å·¥ç¨‹ä¸€èµ·ä½¿ç”¨ã€‚

## 1.åŸºçº¿åˆ†æ•°

ç¬¬ä¸€æ­¥æ˜¯åŸºçº¿ ml æ¨¡å‹çš„åˆ†æ•°è®¡ç®—ã€‚

```
from pycaret.classification import *

CAT_FEATURES = ['Sex', 'Embarked']
NUM_FEATURES = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
IGN_FEATURES = ['PassengerId', 'Name', 'Ticket', 'Cabin']

clf = setup(data=titanic_train, 
            target='Survived',
            categorical_features = CAT_FEATURES,
            numeric_features = NUM_FEATURES,
            ignore_features = IGN_FEATURES)baseline_model = create_model('rf')

baseline_preds = predict_model(baseline_model, raw_score=True)
baseline_preds
```

![](img/defd7fa13fc3f69edb64d0940711a35f.png)

åŸºçº¿æ¨¡å‹(éšæœºæ£®æ—)å¾—åˆ†-ä½œè€…æä¾›çš„å›¾ç‰‡

## 2.ç‰¹å¾å·¥ç¨‹

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å‘æ•°æ®é›†æ·»åŠ æ–°è¦ç´ ã€‚

```
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

*# Name*
titanic_train_FeaEng = titanic_train.copy()
name_last = titanic_train_FeaEng['Name'].str.split(' ', n=1, expand=True)[1]
title = name_last.str.split(' ', n=1, expand=True)[0]
titanic_train_FeaEng['Title'] = title

name_len = titanic_train_FeaEng['Name'].str.len()
titanic_train_FeaEng['Name_len'] = name_len

*# Cabin*
cabin_first = []
cabin_last = []
cabin_len = []

for cabin **in** titanic_train_FeaEng['Cabin']:
    try:
        re_list = re.split('(\d+)',cabin)
        if len(re_list) > 1:
            cabin_first.append(re_list[0])
            cabin_last.append(int(re_list[-2]))
            cabin_len.append(len(re_list))
        else:
            cabin_first.append('None')
            cabin_last.append(0)
            cabin_len.append(0)
    except:
        cabin_first.append('None')
        cabin_last.append(0)
        cabin_len.append(0)

titanic_train_FeaEng['Cabin_First'] = cabin_first
titanic_train_FeaEng['Cabin_Last'] = cabin_last
titanic_train_FeaEng['Cabin_Len'] = cabin_len

*...*
```

## 3.ç‰¹å¾é‡è¦æ€§

åœ¨ç‰¹å¾å·¥ç¨‹ä¹‹åï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‘æ•°æ®é›†æ·»åŠ æ–°ç‰¹å¾ï¼Œå¹¶ä¸”æˆ‘ä»¬æŸ¥çœ‹åŸºçº¿æœºå™¨å­¦ä¹ çš„åˆ†æ•°ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ›´å¥½çš„åˆ†æ•°ï¼Œè¿™æ„å‘³ç€æ–°çš„ç‰¹å¾æœ‰åˆ©äºé¢„æµ‹ã€‚

```
feature_score_dict = {}

for index, feature **in** enumerate(new_features):
    old_features_temp = old_features.copy()
    old_features_temp.append(feature)
    titanic_train_FeaEng_temp = titanic_train_FeaEng[
old_features_temp].copy()

    clf = setup(data=titanic_train_FeaEng_temp, 
            target='Survived')

    baseline_model = create_model('rf')
    scores = pull()
    feature_score_dict[feature] = scores
```

## 4.åˆ†æ•°æ•°æ®å‡†å¤‡

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å‡†å¤‡åŒ…æ‹¬ç”¨äºå¯è§†åŒ–çš„åˆ†æ•°çš„æ•°æ®é›†ã€‚

```
metric_list = []
feature_list = []
score_list = []

for key **in** feature_score_dict.keys():
    metric_list.extend(list(feature_score_dict[key].columns))
    score_list.extend(list(feature_score_dict[key].loc['Mean', :]))
    feature_list.extend([key for i **in** range(len(feature_score_dict[key].columns))])

all_scores_pd = pd.DataFrame()
all_scores_pd['Metric'] = metric_list
all_scores_pd['Feature'] = feature_list
all_scores_pd['Score'] = score_list
```

## 5.å½¢è±¡åŒ–

```
import matplotlib.pyplot as plt
import seaborn as sns

col_list = ['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa']
score_color = {'Accuracy':'C0', 'AUC':'C1', 'Recall':'C2', 'Prec.':'C3', 'F1':'C4', 'Kappa':'C5'}...
```

![](img/e5a025d2b2530f9c0588e236f211959e.png)

æ–°ç‰¹å¾å¯¹é¢„æµ‹çš„é‡è¦æ€§â€”ä½œè€…æä¾›çš„å›¾ç‰‡

## è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹

æœ€ç»ˆçš„åŸºçº¿æ¨¡å‹æ˜¯è‡ªåŠ¨åŒ–çš„ ml åŸºçº¿æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¨¡å‹ï¼Œå¯ä»¥ä½œä¸ºä½ çš„ ml æ¨¡å‹çš„åŸºå‡†ã€‚å¦‚æœä½ çš„ ml æ¨¡å‹æ¯”è‡ªåŠ¨åŒ–åŸºçº¿æ¨¡å‹æ›´å¥½ï¼Œè¿™æ˜¯æ¨¡å‹å¯ä»¥æˆä¸ºäº§å“çš„ä¸€ä¸ªéå¸¸å¼ºçƒˆçš„ä¿¡å·ã€‚

## 1.LightAutoML

é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£…å¹¶å¯¼å…¥ lightautoml åº“ã€‚

```
%%capture
!pip install -U lightautoml*# Imports from our package*
from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML
from lightautoml.dataset.roles import DatetimeRole
from lightautoml.tasks import Task

import torch
```

ä¹‹åï¼Œæˆ‘ä»¬ä¸º lightautoml åº“å‡†å¤‡ä»»åŠ¡ã€è§’è‰²å’Œåº¦é‡ã€‚

```
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

N_THREADS = 4 *# threads cnt for lgbm and linear models*
N_FOLDS = 5 *# folds cnt for AutoML*
RANDOM_STATE = 42 *# fixed random state for various reasons*
TEST_SIZE = 0.2 *# Test size for metric check*
TIMEOUT = 300 *# Time in seconds for automl run*

np.random.seed(RANDOM_STATE)
torch.set_num_threads(N_THREADS)

def acc_score(y_true, y_pred, **kwargs):
    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)

def f1_metric(y_true, y_pred, **kwargs):
    return f1_score(y_true, (y_pred > 0.5).astype(int), **kwargs)

task = Task('binary', metric = acc_score)

roles = {
    'target': 'Survived',
    'drop': ['Passengerid', 'Name', 'Ticket'],
}
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥è®¡ç®—äº¤å‰éªŒè¯åˆ†æ•°ã€‚

```
%%time 
from sklearn.model_selection import StratifiedKFold

n_fold = 3
skf = StratifiedKFold(n_splits=n_fold)
skf.get_n_splits(titanic_train)

...print('lightautoml_acc_score: ', lightautoml_acc_score)lightautoml_acc_score:  0.7957351290684626
```

## 2.H2O æ±½è½¦å…¬å¸

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ h2o åº“ã€‚

```
import h2o
from h2o.automl import H2OAutoMLh2o.init()
```

![](img/d30a3439f78d789c916ccdedc17bceb3.png)

H2O Init â€”ä½œè€…å›¾ç‰‡

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥è®¡ç®—äº¤å‰éªŒè¯åˆ†æ•°ã€‚

```
%%time
acc_list = []
for train_index, test_index **in** skf.split(titanic_train, titanic_train['Survived']):
    X_train, X_test = titanic_train.loc[train_index, :], titanic_train.loc[test_index, :]
    y = X_test['Survived'].astype(int)
    X_test.drop(['Survived'], axis=1, inplace=True)...print('h2o_tautoml_acc_score: ', h2o_tautoml_acc_score)h2o_tautoml_acc_score:  0.8271604938271605
```

## 3.å½¢è±¡åŒ–

åœ¨è®¡ç®—è‡ªåŠ¨ ml æ¨¡å‹çš„åˆ†æ•°ä¹‹åã€‚ç°åœ¨ï¼Œä½ å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ä½ åº”è¯¥é€šè¿‡çš„å¼ºåŠ› ml ç”Ÿäº§çš„åˆ†æ•°ã€‚

```
fig, ax = plt.subplots(figsize=(24, 8))
ax.plot([0, 10], [h2o_tautoml_acc_score, h2o_tautoml_acc_score], color='r')
ax.text(10, h2o_tautoml_acc_score, 'Base_H2O')
ax.plot([0, 10], [lightautoml_acc_score, lightautoml_acc_score], color='r')
ax.text(10, lightautoml_acc_score, 'Base_LightAutoMl');
```

![](img/c82427fd24d2eff277e0a14a262ba377.png)

è‡ªåŠ¨ ML åˆ†æ•°-æŒ‰ä½œè€…æ’åºçš„å›¾ç‰‡

åœ¨æ–‡ç« çš„è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è®¨è®ºäº†åˆ†ç±»é—®é¢˜ä¸­çš„åŸºçº¿æ¨¡å‹ç±»å‹ã€‚åœ¨æ–‡ç« çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºå›å½’é—®é¢˜ä¸­çš„åŸºçº¿æ¨¡å‹ã€‚

ä»è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°å®Œæ•´çš„ python ä»£ç å’Œæ‰€æœ‰çš„æƒ…èŠ‚ğŸ‘‰ [Kaggle ç¬”è®°æœ¬](https://www.kaggle.com/hasanbasriakcay/baseline-models-clf-random-ml-automl)ã€‚

ğŸ‘‹æ„Ÿè°¢é˜…è¯»ã€‚å¦‚æœä½ å–œæ¬¢æˆ‘çš„ä½œå“ï¼Œåˆ«å¿˜äº†å–œæ¬¢å®ƒï¼Œåœ¨ Medium å’Œ [LinkedIn](https://www.linkedin.com/in/hasan-basri-akcay/) ä¸Šå…³æ³¨æˆ‘[ã€‚è¿™å°†æ¿€åŠ±æˆ‘ä¸ºåª’ä½“ç¤¾åŒºæä¾›æ›´å¤šçš„å†…å®¹ï¼ğŸ˜Š](https://medium.com/@hasan.basri.akcay)

# å‚è€ƒèµ„æ–™:

[1]:[https://www . ka ggle . com/hasanbasriakcay/baseline-models-clf-random-ml-automl](https://www.kaggle.com/hasanbasriakcay/baseline-models-clf-random-ml-automl)
ã€2ã€‘:[https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data)
ã€3ã€‘:[https://pycaret.gitbook.io/docs/](https://pycaret.gitbook.io/docs/)
ã€4ã€‘:[https://lightautoml.readthedocs.io/en/latest/index.html](https://lightautoml.readthedocs.io/en/latest/index.html)
ã€5ã€‘:[https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)

# æ›´å¤šâ€¦

[](https://medium.com/databulls/welcome-2022-what-has-changed-in-data-science-in-2021-dac24bd37929) [## æ¬¢è¿ï¼Œ2022ğŸ‰ã€‚2021 å¹´æ•°æ®ç§‘å­¦å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿ

### æœ€å¥½çš„æ•°æ®ç§‘å­¦å·¥å…·ã€æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œå¦‚äº‘è®¡ç®—äº§å“ã€è‡ªåŠ¨åŒ– ML å·¥å…·ã€è¯¾ç¨‹ã€IDEsâ€¦

medium.com](https://medium.com/databulls/welcome-2022-what-has-changed-in-data-science-in-2021-dac24bd37929) [](https://medium.com/databulls/what-are-the-differences-between-data-scientists-that-earn-500-and-225-000-yearly-ea60ccdf03d7) [## æŒ£ 500 çš„æ•°æ®ç§‘å­¦å®¶æœ‰ä»€ä¹ˆåŒºåˆ«ğŸ’²å’Œ 225.000ğŸ’²æ¯å¹´ï¼Ÿ

### è¿™ç¯‡æ–‡ç« æ˜¯å…³äºé‡è¦çš„äººæ‰ï¼Œå·¥å…·ï¼Œå›½å®¶çš„ç‰¹ç‚¹ï¼Œå’Œå…¬å¸çš„ç‰¹ç‚¹ï¼Œä¸ºé«˜æ”¶å…¥åœ¨â€¦

medium.com](https://medium.com/databulls/what-are-the-differences-between-data-scientists-that-earn-500-and-225-000-yearly-ea60ccdf03d7) [](https://medium.com/databulls/5-important-python-libraries-and-methods-for-data-scientists-491186e9f999) [## æ•°æ®ç§‘å­¦å®¶çš„ 5 ä¸ªé‡è¦ Python åº“å’Œæ–¹æ³•ï¼

### å¤§å¤šæ•° python åº“éƒ½æ˜¯ä¸ºæ•°æ®ç§‘å­¦ç¼–å†™çš„ï¼Œä½†æ˜¯æ•°æ®ç§‘å­¦å’Œæœºå™¨é¢†åŸŸçš„æ–°æ‰‹â€¦

medium.com](https://medium.com/databulls/5-important-python-libraries-and-methods-for-data-scientists-491186e9f999) [](https://medium.com/databulls/olympic-medal-numbers-predictions-with-timeseries-part-2-data-analysis-5d5d7e38fc37) [## ç”¨æ—¶é—´åºåˆ—é¢„æµ‹å¥¥è¿å¥–ç‰Œæ•°ï¼Œç¬¬ 2 éƒ¨åˆ†:æ•°æ®åˆ†æ

### Fbprophetã€Dartsã€AutoTSã€Arimaã€Sarimax å’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ

medium.com](https://medium.com/databulls/olympic-medal-numbers-predictions-with-timeseries-part-2-data-analysis-5d5d7e38fc37)