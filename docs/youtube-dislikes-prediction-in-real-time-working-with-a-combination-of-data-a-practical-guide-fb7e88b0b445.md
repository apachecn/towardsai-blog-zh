# YouTube ä¸å–œæ¬¢å®æ—¶é¢„æµ‹â€”â€”å¤„ç†æ•°æ®ç»„åˆï¼›å®ç”¨æŒ‡å—

> åŸæ–‡ï¼š<https://pub.towardsai.net/youtube-dislikes-prediction-in-real-time-working-with-a-combination-of-data-a-practical-guide-fb7e88b0b445?source=collection_archive---------3----------------------->

å¤§å®¶å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£è¯é¢˜çš„å®ç”¨æŒ‡å—ï¼›ä»Šå¤©ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å¤„ç†æ··åˆæ•°æ®çš„ç»„åˆã€‚å½“æˆ‘ä»¬æµè§ˆæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬éƒ½ç»å†è¿‡å®ƒï¼Œå¹¶ä¸”æœ‰ä¸åŒæ•°æ®ç±»å‹çš„ç‰¹å¾ï¼Œå¹¶ä¸”æƒ³çŸ¥é“æˆ‘ä»¬å¦‚ä½•å°†ä¸¤ç§ç±»å‹ç»“åˆèµ·æ¥ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒå•ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ä»Šå¤©ï¼Œä½ æœ‰äº†ç®€å•çš„å¼•å¯¼å¼ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è®©äº‹æƒ…å˜å¾—æœ‰è¶£ï¼Œæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå®æ—¶é¢„æµ‹ youtube ä¸å–œæ¬¢çš„ä¸œè¥¿ã€‚ä¸€å¹´å‰ï¼Œyoutube åˆ é™¤äº†å®ƒçš„ä¸å–œæ¬¢è®¡æ•°åŠŸèƒ½ï¼Œè¿™å·²ç»ä¸è¶³ä¸ºå¥‡äº†ï¼Œä¹Ÿè®¸æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜æœ‰ç‚¹æ™šäº†ï¼Œä½†æˆ‘ä»¬ä»Šå¤©ä½¿ç”¨çš„æ•°æ®é›†è‚¯å®šä¼šæ»¡è¶³æˆ‘ä»¬ä»Šå¤©çš„å­¦ä¹ éœ€æ±‚ã€‚è¯·è®°ä½ï¼Œç”±äº youtube ä¸å–œæ¬¢æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œæˆ‘ä»¬æ­£åœ¨è§£å†³ä¸€ä¸ª**å›å½’é—®é¢˜**ã€‚ğŸ™‚

**ç›®å½•**

1.  [åŠ è½½æ•°æ®](#cf4c)
2.  [æ•°æ®é¢„å¤„ç†](#ecae)
3.  [å»ºæ¨¡å’Œè®­ç»ƒ](#66e8)
4.  [è¯„ä¼°](#7647)
5.  [å®æ—¶é¢„æµ‹](#e52c)

**ä½ å°†å­¦åˆ°ä»€ä¹ˆ**

*   å¤„ç†æ··åˆæ•°æ®
*   æ¸…é™¤æ–‡æœ¬æ•°æ®
*   å¤„ç†æ–‡æœ¬å’Œæ•°å­—æ•°æ®
*   Keras TensorFlow åŠŸèƒ½ API
*   LSTM (RNN)
*   åˆ›å»ºåŒæ—¶å¤„ç†ä¸åŒç±»å‹æ•°æ®çš„æ¨¡å‹
*   å¦‚ä½•æ£€æŸ¥å›å½’æ¨¡å‹çš„å‡†ç¡®æ€§
*   Youtube API

***ç°å®*** *:åœ¨ youtube ä¸­ï¼Œä¸€ä¸ªè§†é¢‘èƒ½å¤Ÿè·å¾—çš„æµè§ˆé‡ã€å–œæ¬¢å’Œä¸å–œæ¬¢å–å†³äºå¾ˆå¤šä¸œè¥¿ï¼Œæ¯”å¦‚åˆ›ä½œè€…çš„å—æ¬¢è¿ç¨‹åº¦ã€è§†é¢‘çš„è´¨é‡ã€SEOã€ç”¨æˆ·ä»½é¢ï¼Œä»¥åŠå…¶ä»–å¾ˆå¤šè¶…å‡ºæˆ‘ä»¬å¯ç”¨æ•°æ®é›†çš„å¤§è…¿ã€‚ä¸ç®¡æ€æ ·ï¼Œè®©æˆ‘ä»¬å°½åŠ›å‘æŒ¥æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„ä¸€åˆ‡ğŸ˜*

***æ³¨:*** *åœ¨æ•´ä¸ªæ•™ç¨‹ä¸­ï¼Œå¦‚æœæˆ‘é”™è¿‡äº†ä»€ä¹ˆï¼Œæˆ‘ä¼šæåˆ°ä½ å¯ä»¥åšä»€ä¹ˆå’Œå¦‚ä½•åšæ¥æ”¹è¿›è¿™ä¸ªæ¨¡å‹ï¼Œä»¥åŠæˆ‘åœ¨åˆ›å»ºè¿™ä¸ªæ¨¡å‹æ—¶é”™è¿‡çš„ä¸œè¥¿ã€‚è¯´åˆ°æ·»åŠ åº“ï¼Œæˆ‘ä»¬å°†æŒ‰éœ€æ·»åŠ ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å¯¼å…¥æ‰€æœ‰åº“*

**æˆ‘ä»¬å¼€å§‹å§â€¦â€¦**

æ‚¨å¯ä»¥åœ¨[https://www . ka ggle . com/datasets/dmitrynikolaev/YouTube-unlesses-dataset](https://www.kaggle.com/datasets/dmitrynikolaev/youtube-dislikes-dataset)è·å–æ•°æ®é›†

è¯·ä¸‹è½½æ•°æ®é›†å¹¶å°†å…¶æå–åˆ°æ‚¨çš„å·¥ä½œç›®å½•ä¸­ã€‚è¯¥æ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å« 37422 ä¸ªå”¯ä¸€ raw çš„æ··åˆè¯­è¨€æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨æ•°æ®é›†æä¾›çš„è§†é¢‘ id å°†å…¶ç¼©çŸ­ä¸ºåªæœ‰è‹±è¯­ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„æ•°æ®é›†åœ¨å¤„ç†å‰æ€»å…±æœ‰ 15835 ä¸ªå”¯ä¸€ rawã€‚è¿™å®é™…ä¸Šæ˜¯éå¸¸å°‘çš„æ•°æ®é‡ã€‚è¦è§£å†³è¿™æ ·çš„é—®é¢˜ï¼Œè¯·å°è¯•é€šè¿‡å¤„ç†å…¶ä»–è¯­è¨€æ¥å¢åŠ æ•°æ®é‡ï¼Œè¿™è‚¯å®šæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

***æ³¨æ„:*** *è¯¥æ•°æ®é›†æœ€åä¸€æ¬¡æ›´æ–°æ˜¯åœ¨ 2021 å¹´ 13 æœˆ 12 æ—¥ï¼Œå› æ­¤æˆ‘ä»¬å°†å‡è®¾è¿™æ˜¯æå–è¯¥æ•°æ®çš„æ—¥æœŸï¼Œå› ä¸ºæˆ‘ä»¬å°†ä½¿ç”¨è¯¥æ•°æ®æ¥å¤„ç†æ—¶é—´æ®µ*

## **åŠ è½½æ•°æ®é›†å¹¶æ”¶é›†æˆ‘ä»¬éœ€è¦çš„ç‰¹å¾**

å¸Œæœ›ä½ æœ‰ã€‚csv æ–‡ä»¶åœ¨æ‚¨çš„å·¥ä½œç›®å½•ä¸­ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬ä½¿ç”¨ pandas åŠ è½½å®ƒï¼Œå¹¶å°†å…¶ç¼©çŸ­åˆ°åªæœ‰è‹±æ–‡ï¼Œå¹¶ä»æ•°æ®é›†ä¸­è·å¾—æ›´å¤šå¯ç”¨åŠŸèƒ½çš„è¯¦ç»†ä¿¡æ¯ã€‚

å¯¼å…¥ä¸€äº›åº“æ¥å¼€å§‹

```
import numpy as np
import pandas as pd
import tensorflow as tf
```

åŠ è½½æ•°æ®é›†

```
dataset = pd.read_csv('youtube_dislike_dataset.csv')
dataset.head()
```

ä»…æ”¶é›†è‹±æ–‡æ•°æ®

```
file_US_ids = open("video_IDs/unique_ids_US.txt", "r")
US_ids = file_US_ids.read().splitlines()
dataset = dataset[dataset['video_id'].isin(US_ids)]
dataset.shape 
```

æ£€æŸ¥æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚è¦ç´ çš„æ•°æ®ç±»å‹

```
dataset.info()
```

![](img/974fd11b77a7ea6c6e3d64251175e14f.png)

ä½œè€…å›¾ç‰‡

æ•°æ®é›†åŒ…å« 12 ä¸ªè¦ç´ ã€‚æˆ‘ä»¬å°†åªä½¿ç”¨å…¶ä¸­çš„ 7 ä¸ªåŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½æè¿°å¦‚ä¸‹:

**Video_id:** å”¯ä¸€è§†é¢‘ id
**Title:**YouTube è§†é¢‘æ ‡é¢˜
**Channel_id:** å‘å¸ƒè€…é¢‘é“ id
**Channel _ Title:**é¢‘é“åç§°
**å‘å¸ƒæ—¶é—´:**è§†é¢‘å‘å¸ƒæ—¥æœŸ
**View_count:** è§†é¢‘åœ¨ä¸€æ®µæ—¶é—´å†…è·å¾—çš„æµè§ˆé‡
**æ—¶é—´
**Comment_count:** è§†é¢‘æœ‰
**æ ‡ç­¾çš„è¯„è®ºæ•°**:è§†é¢‘æ ‡ç­¾ä¸ºå­—ç¬¦ä¸²
**æè¿°:**è§†é¢‘æè¿°
**è¯„è®º:**è§†é¢‘è¯„è®ºåˆ—è¡¨ä¸ºå­—ç¬¦ä¸²**

å¥½äº†ï¼Œè¿™é‡Œæœ‰æ‰€æœ‰å¯ç”¨çš„åŠŸèƒ½ï¼Œç°åœ¨è®©æˆ‘ä»¬é€‰æ‹©ä¸€äº›æˆ‘ä»¬å°†åœ¨æœ¬å®ç”¨æŒ‡å—ä¸­ä½¿ç”¨çš„åŠŸèƒ½ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ 7 ä¸ªåŠŸèƒ½:
*æŸ¥çœ‹è®¡æ•°ï¼Œå‘å¸ƒè®¡æ•°ï¼Œå–œæ¬¢ï¼Œè¯„è®ºè®¡æ•°ï¼Œæ ‡ç­¾ï¼Œæè¿°ï¼Œä¸å–œæ¬¢*

```
dataset = dataset[['view_count', 'published_at','likes', 'comment_count','tags','description','dislikes']]
```

åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œæˆ‘ä»¬å°†å†åˆ›å»ºä¸€ä¸ªç‰¹å¾ï¼Œå¹¶ä»è¿™é‡Œåˆ é™¤ä¸€ä¸ªç‰¹å¾ã€‚è€å®è¯´ï¼Œä½¿ç”¨æ ‡é¢˜å’Œé¢‘é“åç§°å¯¹æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ”¹è¿›ï¼Œä½ å¯ä»¥è¯•è¯•ã€‚ğŸ™‚

## æ•°æ®é¢„å¤„ç†

è®©æˆ‘ä»¬ä»å¤„ç†æ•°æ®å¼€å§‹ï¼Œåœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†åšäº†è®¸å¤šé‡è¦çš„äº‹æƒ…ï¼Œä¾‹å¦‚å¤„ç†ç¼ºå¤±å€¼ã€åˆ›å»ºç›¸å…³çš„æ—¶é—´å­—æ®µï¼Œä»¥åŠæ¸…ç†å’Œæ ‡è®°æ–‡æœ¬æ•°æ®ã€‚æˆ‘è¾¹åšè¾¹å¤šè§£é‡Šä¸€ä¸‹ã€‚

**å¤„ç†ç¼ºå¤±å€¼**

æ•°æ®é›†æ˜¯ç›¸å½“æ£˜æ‰‹çš„ï¼Œå¦‚æœä½ åªæ˜¯å¯»æ‰¾ä¸¢å¤±çš„å€¼( **nan** )ä½ ä¸ä¼šæ‰¾åˆ°ä»»ä½•ä¸œè¥¿ï¼Œè¿™æ˜¯å› ä¸ºåœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæ•°æ®é›†æ²¡æœ‰ä»»ä½•ä¸¢å¤±çš„å€¼ï¼Œå› ä¸ºæ‰€æœ‰çš„å­—æ®µéƒ½å¡«å……äº†ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥å¤„ç†è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥ç©ºå­—ç¬¦ä¸²å­—æ®µï¼Œå…¶ä¸­æ²¡æœ‰ä¸€ä¸ªå•è¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ª **nan** å€¼ï¼Œå¹¶å†æ¬¡æ£€æŸ¥ï¼Œæˆ‘æ•¢è‚¯å®šï¼Œæˆ‘ä»¬ç°åœ¨ä¼šå¾—åˆ°å¤§é‡çš„ç©ºå€¼ã€‚ä½ çŒœæ€ä¹ˆç€ï¼Ÿæˆ‘ä»¬å°†åˆ é™¤å®ƒä»¬ï¼Œè¿™å°†ä½¿æˆ‘ä»¬çš„æ•°æ®é›†æ€»å…±è¾¾åˆ° 13536 rawsï¼Œè¿™æ˜¯ç›¸å½“å°çš„ğŸ˜‚

![](img/bc83c0581ffd30aa1c6f6d16243a2ae4.png)

ä½œè€…å›¾ç‰‡

**åˆ›å»ºæ–°åŠŸèƒ½â€”å¤„ç†æ—¶é—´**

ä¸Šé¢ï¼Œæˆ‘æåˆ°æˆ‘ä»¬å°†å‡è®¾æ•°æ®æ˜¯åœ¨ 2021 å¹´ 13 æœˆ 12 æ—¥æå–çš„ï¼Œè¿™æ˜¯æ—¥æœŸå¼€å§‹èµ·ä½œç”¨çš„æ—¶é—´ã€‚

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º **timesec** çš„æ–°ç‰¹æ€§ï¼Œè®¡ç®—è§†é¢‘å‘å¸ƒæ—¥æœŸå’Œæå–æ—¶é—´ä¹‹é—´çš„æ—¶é—´ï¼Œä»¥åˆ†é’Ÿä¸ºå•ä½ã€‚

æ‰€ä»¥åŸºæœ¬ä¸Š: *timesec =æå–æ—¥æœŸ-å‘å¸ƒæ—¥æœŸ*

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ—¶é—´(ä»¥åˆ†é’Ÿä¸ºå•ä½),å¹¶ä½¿ç”¨ pandas apply æ–¹æ³•æ¥åˆ›å»ºæ–°è¦ç´ ã€‚

ç”¨äºè®¡ç®—ä»¥ä¸‹æ—¶é—´é—´éš”çš„å‡½æ•°:

```
from datetime import datetime
def calTime(time):
  start = datetime.strptime(time, '%Y-%m-%d %H:%M:%S')
  end =   datetime.strptime('13/12/2021 00:00:00', '%d/%m/%Y %H:%M:%S') # assuming this is the date that this dataset was extracted
  return np.round((end - start).total_seconds() / 60, 2)
```

ä½¿ç”¨ pandas apply æ–¹æ³•è¿è¡Œå‡½æ•°å¹¶åˆ›å»ºæ–°ç‰¹å¾

```
dataset['timesec'] = dataset['published_at'].apply(calTime)
```

**æ¸…ç†æ–‡æœ¬ç‰¹å¾**

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ¸…ç†æˆ‘ä»¬çš„æ–‡æœ¬æ•°æ®ï¼Œç„¶åæ‰§è¡Œä¸€ç³»åˆ—æ­¥éª¤æ¥ä»ä¸­è·å–æ­£ç¡®å’Œé‡è¦çš„å«ä¹‰ã€‚

åŸºæœ¬ä¸Šï¼Œè¯¥åŠŸèƒ½å°†åˆ é™¤ä»»ä½•ä¸å¿…è¦çš„ç½‘å€ï¼Œæ ‡ç‚¹ç¬¦å·ï¼Œæ•°å­—ï¼Œåœç”¨è¯ï¼Œå¹¶éšæœºè¯ï¼Œä¸åœ¨å­—å…¸å°†è¢«åˆ é™¤ã€‚å®ƒè¿˜å°†å¤„ç†ç¼©ç•¥è¯ï¼Œå¹¶å°†å•è¯è¯æ¡åŒ–ï¼Œæœ€åï¼Œå°†å…¨éƒ¨è½¬æ¢ä¸ºå°å†™å¹¶è¿”å›å¤„ç†åçš„å­—ç¬¦ä¸²ã€‚æˆ‘ä»¬å°†åœ¨ä¸€äº›åº“çš„å¸®åŠ©ä¸‹å®Œæˆè¿™é¡¹å·¥ä½œï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»å¯¼å…¥è¿™äº›åº“å¼€å§‹ã€‚

ä¸ºæ­¤å¯¼å…¥æ‰€éœ€çš„åº“

```
import re
from string import punctuation 
from bs4 import BeautifulSoup
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
```

ç¼©ç•¥è¯è¯å…¸å°†æœ‰åŠ©äºå°†ä¸¤ä¸ªè¯çš„ç»„åˆè½¬æ¢æˆå®ƒä»¬å•ç‹¬çš„è¯æ ¹ã€‚(è¿™ä¸æ˜¯å…¨éƒ¨ï¼Œè¿˜æœ‰æ›´å¤š)

```
contraction_map={
    "ain't": "is not",
    "aren't": "are not",
    "can't": "cannot",
...
}
```

æˆ‘ä»¬åˆ›å»ºçš„æ¸…ç†æ–‡æœ¬çš„å‡½æ•°

```
 lemmatizer = WordNetLemmatizer()
in_words = set(nltk.corpus.words.words())
def clean_text(text):
    text = str(text)
    text = BeautifulSoup(text, "lxml").text
    text = re.sub(r'\([^)]*\)', '', text)
    text = re.sub('"','', text)
    text = ' '.join([contraction_map[t] if t in contraction_map else t for t in text.split(" ")])
    text = re.sub(r"'s\b","",text)
    text = re.sub("[^a-zA-Z]", " ", text)
    text = " ".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in in_words or not w.isalpha())

    text = [word for word in text.split( ) if word not in stopwords.words('english')]
    text = [lemmatizer.lemmatize(word) for word in text]
    text = " ".join(text)
    text = text.lower()
    return text
```

æœ€åï¼Œåœ¨è¿è¡Œè¯¥å‡½æ•°åï¼Œåˆ›å»ºä¸¤ä¸ªæ–°åˆ—ï¼Œå¹¶æ¸…é™¤å…¶ä¸­çš„æ–‡æœ¬

```
%time dataset['clean_description'] = dataset['description'].apply(clean_text)
%time dataset['clean_tags'] = dataset['tags'].apply(clean_text)
```

**æ‹†åˆ†æ•°æ®**

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„æ•°æ®ç±»å‹ï¼Œæ‰€ä»¥ä½¿ç”¨é€šå¸¸çš„ train_test_split å‡½æ•°å¹¶ä¸ç†æƒ³ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é¦–å…ˆé€šè¿‡è®­ç»ƒå’Œæµ‹è¯•åœ¨åˆ‡ç‰‡çš„å¸®åŠ©ä¸‹åˆ†å‰²æ•°æ®ï¼Œç„¶åæˆ‘ä»¬å°†ä»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­åˆ†ç¦» X å’Œ Yï¼Œç„¶åå°†æ–‡æœ¬å’Œæ•°å­—ç‰¹å¾åˆ†ç¦»ï¼Œæˆ‘ä»¬å°†ä¿æŒæ ‡ç­¾å’Œæè¿°åˆ†ç¦»ï¼Œä½†å°†æ‰€æœ‰æ•°å­—ç‰¹å¾ä¿æŒä¸ºä¸€ä¸ªã€‚è¿™å¯èƒ½ä¼šä»¤äººå›°æƒ‘ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ï¼Œè¿™å°†ä½¿äº‹æƒ…å˜å¾—æ¸…æ™°ã€‚

***æ³¨:*** *Y æ˜¯åŒæ¶ç‰¹å¾ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬çš„ç›®æ ‡å˜é‡ã€‚è€Œ X å°†æ˜¯è§†å›¾è®¡æ•°ã€å–œæ¬¢ã€è¯„è®ºè®¡æ•°ã€æ—¶é—´é—´éš”ã€æ¸…ç†æ ‡ç­¾ã€æ¸…ç†æè¿°ã€‚æˆ‘ä»¬ä¸éœ€è¦ published_at ç‰¹æ€§ã€‚*

åˆ‡ç‰‡æ¥è®­ç»ƒå’Œæµ‹è¯•

```
# Slicing to train and test
dataset_train = dataset.iloc[:12500,:]
dataset_test = dataset.iloc[12501:,:]

# Creating X and Y variables of train dataset
X_train = dataset_train.loc[:, dataset.columns != 'dislikes']
Y_train = dataset_train['dislikes'].values

#Creating X and Y variables of the test dataset
X_test = dataset_test.loc[:, dataset.columns != 'dislikes']
Y_test = dataset_test['dislikes'].values

# Splitting and organizing the features of train data
X_train_numaric = X_train[['view_count', 'likes', 'comment_count', 'timesec']].values
X_train_tags = X_train['clean_tags'].values
X_train_desc = X_train['clean_description'].values

# Splitting and organizing the features of the test data
X_test_numaric = X_test[['view_count', 'likes', 'comment_count', 'timesec']].values
X_test_tags = X_test['clean_tags'].values
X_test_desc = X_test['clean_description'].values 
```

**æ–‡æœ¬æ ‡è®°åŒ–â€”â€”å¤„ç†æ–‡æœ¬**

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ ‡è®°åŒ–å‡½æ•°ï¼Œä»¥å¤„ç†æˆ‘ä»¬çš„æ–‡æœ¬ç‰¹å¾ã€‚è¯¥å‡½æ•°å°†åˆ›å»ºä¸€ä¸ªæ ‡è®°å™¨å¹¶æ ‡è®°å•è¯ï¼Œå¹¶å°†å‘å…¶æ·»åŠ  100 çš„å¡«å……ï¼Œå¹¶å°†è¿”å›ä¸€ç»„æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¦‚å·²å¤„ç†çš„æµ‹è¯•å’Œè®­ç»ƒæ•°æ®ã€æœ€å¤§å•è¯æ•°ã€è¯æ±‡åŠå…¶å¤§å°ï¼Œä»¥åŠæˆ‘ä»¬ç¨åå°†ç”¨äºè¿›è¡Œå®æ—¶é¢„æµ‹çš„æ ‡è®°å™¨æœ¬èº«ã€‚
æˆ‘ä»¬å°†ä¸å¾—ä¸åœ¨æ ‡ç­¾å’Œæè¿°åŠŸèƒ½ä¸Šä½¿ç”¨å®ƒã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç ã€‚ğŸ™‚

ä¸ºæ­¤å¯¼å…¥æ‰€éœ€çš„åº“

```
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

è¿›è¡Œå¤„ç†çš„å‡½æ•°

```
def Tokenizer_func(train,test, max_words_length=0, max_seq_len=100):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(train)

    max_words = 0
    if max_words_length > 0:
        max_words = max_words_length
    else:
        max_words = len(tokenizer.word_counts.items())

    tokenizer = Tokenizer(num_words=max_words)
    tokenizer.fit_on_texts(train)

    train_sequences = tokenizer.texts_to_sequences(train)
    test_sequences = tokenizer.texts_to_sequences(test)

    train = pad_sequences(train_sequences,maxlen=max_seq_len, padding='post')
    test = pad_sequences(test_sequences,maxlen=max_seq_len, padding='post')

    voc = tokenizer.num_words +1
    return {'train': train, 'test': test, 'voc': voc, 'max_words':max_words, 'tokenizer': tokenizer}
```

ä¸ºæ ‡ç­¾è°ƒç”¨å‡½æ•°

```
X_tags_processed = Tokenizer_func(X_train_tags,X_test_tags)
# Extracting data from itâ€¦
X_train_tags,X_test_tags,x_tags_voc,x_tags_max_words,x_tag_tok = X_tags_processed['train'], X_tags_processed['test'],X_tags_processed['voc'],X_tags_processed['max_words'],X_tags_processed['tokenizer']
```

è°ƒç”¨å‡½æ•°è¿›è¡Œæè¿°

```
X_desc_processed = Tokenizer_func(X_train_desc,X_test_desc)
# Extracting data from itâ€¦
X_train_desc, X_test_desc,x_desc_voc,x_desc_max_words,x_desc_tok = X_desc_processed['train'], X_desc_processed['test'],X_desc_processed['voc'],X_desc_processed['max_words'],X_desc_processed['tokenizer']
```

åœ¨ç»“æŸæœ¬æ•™ç¨‹çš„å¤„ç†æ­¥éª¤ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€ä»¶äº‹è¦åšã€‚

**æ•°å€¼æ•°æ®è§„èŒƒåŒ–**

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ StandardScaler æ¥æ ‡å‡†åŒ–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚å¥½å§ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ã€‚
å¯¼å…¥åº“ï¼Œåˆ›å»ºç¼©æ”¾å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒå¤„ç†æ•°å­—æ•°æ®

```
from sklearn.preprocessing import StandardScaler
Sc = StandardScaler()
X_train_numaric = Sc.fit_transform(X_train_numaric)
X_test_numaric = Sc.transform(X_test_numaric)
```

**è€¶..**æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€åŠâ€¦æ•°æ®é¢„å¤„ç†åˆ°æ­¤ç»“æŸã€‚ğŸ‰ğŸ‰ğŸ‰

## åˆ›å»ºæ¨¡å‹å’ŒåŸ¹è®­

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Keras functional API æ¥åˆ›å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ç§ç§°ä¸º LSTM çš„ RNN æ¥å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œç„¶åå°†å…¶è¿æ¥åˆ°å¸¦æœ‰æ•°å€¼æ•°æ®çš„ MLPã€‚æˆ‘ä¸æ‰“ç®—åœ¨è¿™é‡Œè§£é‡Š LSTMï¼Œå› ä¸ºæˆ‘å·²ç»åœ¨ä¹‹å‰çš„ä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»è¿‡äº†ï¼Œé“¾æ¥åœ¨è¿™é‡Œâ€¦

[](https://medium.com/@nafiu.dev/stock-market-prediction-using-lstm-will-the-price-go-up-tomorrow-practical-guide-d1df2d54a517) [## ç”¨ LSTM é¢„æµ‹è‚¡ç¥¨å¸‚åœº:æ˜å¤©ä»·æ ¼ä¼šä¸Šæ¶¨å—ï¼Ÿå®ç”¨æŒ‡å—

### æœ¬æ•™ç¨‹çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹è‚¡ç¥¨äº¤æ˜“çš„æœªæ¥ä»·å€¼

medium.com](https://medium.com/@nafiu.dev/stock-market-prediction-using-lstm-will-the-price-go-up-tomorrow-practical-guide-d1df2d54a517) 

**è¯·å…è®¸æˆ‘åœ¨è¿™é‡Œè§£é‡Šä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹:**

æ¨¡å‹ä¸­æœ‰ 3 ä¸ªè¾“å…¥:æ ‡ç­¾è¾“å…¥ã€è¾“å…¥å½¢çŠ¶ä¸º none çš„æè¿°è¾“å…¥å’Œè¾“å…¥å¤§å°ä¸º 4 çš„æ•°å€¼è¾“å…¥ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ 4 ä¸ªæ•°å€¼ç‰¹å¾ã€‚æ ‡ç­¾å’Œæè¿°éƒ½ä»¥åµŒå…¥å±‚å¼€å§‹ï¼Œç„¶åæ˜¯ä¸¤ä¸ªå•ä½ä¸º 100 çš„ LSTM å±‚(è®°å¾—æˆ‘ä»¬æ·»åŠ äº† 100 ä½œä¸ºå¡«å……)ï¼Œéšåæ˜¯æ¯ä¸ª LSTM ä¹‹åçš„å½’ä¸€åŒ–å±‚ã€‚å¯¹äºè¿™ä¸¤ä¸ªè¦ç´ ï¼Œç¬¬ä¸€ä¸ª LSTM å›¾å±‚çš„è½å·®ä¸º 0.2ï¼Œè¿”å›åºåˆ—è®¾ç½®ä¸º trueï¼Œç¬¬äºŒä¸ªå›¾å±‚çš„è½å·®ä¸º 0.4ï¼Œè¿”å›åºåˆ—è®¾ç½®ä¸º falseã€‚åœ¨é€šè¿‡è¿™äº›å±‚ä¹‹åï¼Œæ ‡ç­¾å’Œæè¿°éƒ½ä¸æ•°å­—æ•°æ®è¾“å…¥ç›¸ç»“åˆï¼Œç„¶åé€šè¿‡ 4 ä¸ªå¯†é›†å±‚ï¼Œå•ä½ä¸º 256ã€128ã€32 å’Œ 1ï¼Œå‰ 3 å±‚å…·æœ‰ **relu** æ¿€æ´»å‡½æ•°ï¼Œæœ€åä¸€å±‚å…·æœ‰çº¿æ€§æ¿€æ´»å‡½æ•°(å› ä¸ºé—®é¢˜æ˜¯å›å½’)ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ **model.compile()** æ–¹æ³•ç¼–è¯‘æ•´ä¸ªæ¨¡å‹ï¼Œå°† mean_squared_error è®¾ç½®ä¸ºæŸå¤±å‡½æ•°ï¼Œå°† mean_absolute_error è®¾ç½®ä¸ºçŸ©é˜µï¼Œå°† adam è®¾ç½®ä¸ºä¼˜åŒ–å™¨ï¼Œå› ä¸º adam å‡½æ•°çš„å­¦ä¹ ç‡é™ä½åˆ° 0.001ã€‚ç„¶åæœ€åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **model.summary()** æ–¹æ³•æŸ¥çœ‹æ¨¡å‹çš„æ¦‚è¦ã€‚

è®©æˆ‘ä»¬æŸ¥çœ‹ä»£ç â€¦

ä¸ºæ­¤å¯¼å…¥æ‰€éœ€çš„åº“ã€‚

```
from keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, concatenate,LayerNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
```

åˆ›å»ºæ¨¡å‹

```
tagsInput = Input(shape=(None,), name='tags')
descInput = Input(shape=(None,), name='desc')
numaricInput = Input(shape=(4,), name='numaric')

tags = Embedding(input_dim=x_tags_voc,output_dim=8,input_length=x_tags_max_words)(tagsInput)
tags = LSTM(100,dropout=0.2, return_sequences=True)(tags)
tags = LayerNormalization()(tags)
tags = LSTM(100,dropout=0.4, return_sequences=False)(tags)
tags = LayerNormalization()(tags)

desc = Embedding(input_dim=x_desc_voc,output_dim=8,input_length=x_desc_max_words)(descInput)
desc = LSTM(100,dropout=0.2, return_sequences=True)(desc)
desc = LayerNormalization()(desc)
desc = LSTM(100,dropout=0.4, return_sequences=False)(desc)
desc = LayerNormalization()(desc)

combined = concatenate([tags, desc,numaricInput])
x = Dense(256,activation='relu')(combined)
x = Dense(128,activation='relu')(x)
x = Dense(32,activation='relu')(x)
x = Dense(1,use_bias=True,activation='linear')(x)
model = Model([tagsInput, descInput,numaricInput], x)
```

ç¼–è¯‘æ¨¡å‹

```
model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001, decay=0.001 / 20), metrics=['mae'])
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ model.sumary()æ–¹æ³•æ¥æŸ¥çœ‹æ¨¡å‹çš„æ¦‚è¦

![](img/d236d7c67b6171f9c4085e59ef013068.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨æ˜¯æ—¶å€™è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚æˆ‘ä»¬å°†åœ¨ 500 ä¸ªæ—¶æœŸä¸‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ‰¹é‡å¤§å°ä¸º 25ï¼ŒéªŒè¯åˆ†å‰²ä¸º 2 %,å› ä¸ºæˆ‘ä»¬åœ¨å®ƒæ¥è¿‘ 500 ä¸ªæ—¶æœŸä¹‹å‰å¯èƒ½å·²ç»å®Œæˆäº†è®­ç»ƒã€‚

åœ¨å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä»¶äº‹æˆ‘ä»¬å¿…é¡»åšã€‚LSTM æ”¯æŒä¸‰ç»´æ•°æ®ï¼Œä½†æˆ‘ä»¬çš„æ•°æ®æ˜¯äºŒç»´çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»åœ¨å¼€å§‹è®­ç»ƒå‰å°†å…¶è½¬æ¢æˆä¸‰ç»´æ•°æ®

åˆå§‹åŒ– EarlyStopping ä»¥é¿å…åœ¨è®­ç»ƒæ¨¡å‹æ—¶è¿‡åº¦æ‹Ÿåˆ

```
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)
```

å°†æ•°æ®æ•´å½¢ä¸ºä¸‰ç»´æ•°æ®

```
X_train_tags = np.reshape(X_train_tags,(X_train_tags.shape[0],X_train_tags.shape[1],1))
X_train_desc = np.reshape(X_train_desc,(X_train_desc.shape[0],X_train_desc.shape[1],1))
X_train_tags.shape, X_train_desc.shape, X_train_numaric.shape
```

![](img/ee6e73e6f20e2d59a4daa823e0618b36.png)

ä½œè€…å›¾ç‰‡

è®­ç»ƒæ¨¡å‹

```
history = model.fit(
                    x=[X_train_tags, X_train_desc,X_train_numaric],
                    y=Y_train,
                    epochs=500, 
                    batch_size=25,
                    validation_split=0.2,
                    verbose=1,
                    callbacks=[es]
                  )
```

![](img/187a3c5644f067e2bbdc725a35c6f539.png)

ä½œè€…å›¾ç‰‡

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¾¾åˆ° 26 ä¸ªçºªå…ƒåå°±åœæ­¢äº†è®­ç»ƒã€‚

## è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å·²ç»å®Œæˆäº†è®­ç»ƒï¼Œæ˜¯æ—¶å€™æ£€æŸ¥ç»“æœäº†ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å›ç­”ä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼Œå¹¶äº†è§£ä¸€äº›é‡è¦çš„äº‹æƒ…ã€‚
**å¦‚ä½•æ£€æŸ¥å›å½’æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Ÿ**

**æ®‹å·®** â€”é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹å·®ã€‚

**å¹³å‡ç»å¯¹è¯¯å·®** â€”é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„æ®‹å·®çš„å¹³å‡å€¼ã€‚

*   MAE è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½ã€‚
*   **MAE** é€šè¿‡æ·»åŠ æ®‹å·®å¹¶é™¤ä»¥æ•°æ®é›†(æµ‹è¯•æ•°æ®)çš„é•¿åº¦æ¥è®¡ç®—
*   æè¿°å¹³å‡è€Œè¨€é¢„æµ‹å€¼ä¸å®é™…å€¼çš„æ¥è¿‘ç¨‹åº¦ã€‚

**å‡æ–¹è¯¯å·®** â€”æ˜¾ç¤ºå›å½’çº¿ä¸ä¸€ç»„æ•°æ®ç‚¹çš„æ¥è¿‘ç¨‹åº¦

**å‡æ–¹æ ¹è¯¯å·®**â€”â€”æ˜¯æ‰€æœ‰è¯¯å·®å¹³æ–¹å€¼çš„å¹³å‡å€¼çš„å¹³æ–¹æ ¹

*   æŒ‡ç¤ºæ¨¡å‹ç›¸å¯¹äºç»™å®šæ•°æ®çš„ç»å¯¹æ‹Ÿåˆåº¦
*   **RMSE** ä¸**æ¢…ä¼Š**æœ‰å…³ç³»
*   å¦‚æœ**çš„ RMSE** å€¼è¿œè¿œé«˜äº**çš„ MAE** ï¼Œåˆ™æ„å‘³ç€æ•°æ®é›†ä¸­ä¸€äº›è¾ƒå¤§è¯¯å·®çš„ç»“æœã€‚

**R å¹³æ–¹** â€”æ˜¾ç¤ºæ•°æ®ä¸æ¨¡å‹çš„å»åˆç¨‹åº¦

*   è¾“å‡º 0 åˆ° 1 èŒƒå›´å†…çš„å€¼
*   R2**è¶Šé«˜ï¼Œæ¨¡å‹è¶Šå¥½**
*   **R2** åˆ†æ•°å€¼å¯ç”¨äºé€šè¿‡ä¹˜ä»¥æ•°å€¼* 100 å¾—åˆ°ä¸åˆ†ç±»æ¨¡å‹ç›¸åŒçš„æ¨¡å‹çš„ç²¾ç¡®åº¦

**è¦ç‚¹:**

*   **R2** è‚¯å®šé«˜
*   MSE è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½
*   MAE è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½
*   RMSE è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½
*   **MAE** åº”ä½äº **RMSE**

ç°åœ¨è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­å®ç°è¿™äº›ï¼Œçœ‹çœ‹æ¨¡å‹çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥åšåˆ°è¿™ä¸€ç‚¹

```
pprydd = model.predict([X_test_tags, X_test_desc, X_test_numaric])
from sklearn import metrics
print("Mean Absolute Error (MAE) - Test data : ", metrics.mean_absolute_error(Y_test, pprydd))
print("Mean Squared Error (MSE) - Test data : ", metrics.mean_squared_error(Y_test, pprydd))
print("Root Mean Squared Error (RMSE) - Test data : ", np.sqrt(metrics.mean_squared_error(Y_test, pprydd)))
print("Co-efficient of determination (R2 Score): ", metrics.r2_score(Y_test, pprydd))
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹çš„ **R2** å¾—åˆ†ä¸º 0.82 (82%)ï¼Œä½†æ˜¯æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ **MSE** éå¸¸é«˜ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½çš„è¿¹è±¡ğŸ˜¥ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ° **MAE** å’Œ **RMSE** ä¸ **MSEã€**ç›¸æ¯”éƒ½éå¸¸ä½ï¼Œè¿™å¾ˆå¥½ï¼Œä½†æ˜¯ **RMSE** é«˜äº **MAEã€**ï¼Œè¿™å®é™…ä¸Šè¡¨æ˜äº†æˆ‘ä»¬æ•°æ®é›†ä¸­ä¸€äº›é”™è¯¯çš„ç»“æœğŸ¤•â€¦ä¸æ˜¯ä¸–ç•Œä¸Šæœ€å¥½çš„æ¨¡å‹ï¼Œä½†åœ¨æˆ‘çœ‹æ¥ï¼Œä¸æˆ‘ä»¬ç°æœ‰çš„å¯ç”¨æ•°æ®ç›¸æ¯”ï¼Œè¿™å·²ç»ä¸é”™äº†ã€‚

æ³¨æ„:ä½ å¯ä»¥é€šè¿‡å¯¹è¿™ä¸ªé¢†åŸŸåšæ›´å¤šçš„ç ”ç©¶æ¥å°è¯•æ”¹è¿›è¿™ä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚å½“ä½ å¯¹è§†é¢‘è¶Šæ¥è¶Šä¸å–œæ¬¢çš„æ—¶å€™ï¼Œä½ éœ€è¦ä¾èµ–ä»€ä¹ˆï¼Œå¹¶å…³æ³¨å®ƒã€‚æ­¤å¤–ï¼Œå°è¯•ä½¿ç”¨æˆ‘ä»¬åœ¨æœ¬æ•™ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨çš„åŠŸèƒ½â€¦æ— è®ºå¦‚ä½•ï¼Œè®©æˆ‘ä»¬ç»§ç»­ã€‚

## å®æ—¶é¢„æµ‹

ç°åœ¨ï¼Œæˆ‘ä»¬éƒ½åœ¨ç­‰å¾…å®æ—¶é¢„æµ‹ youtube ä¸å–œæ¬¢çš„éƒ¨åˆ†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ youtube API æŒ‰ id è·å–ç›®æ ‡è§†é¢‘æ•°æ®ï¼Œå¹¶ä»ä¸­æ”¶é›†æ‰€éœ€çš„æ•°æ®ï¼Œå¯¹å…¶è¿›è¡Œå¤„ç†å¹¶é€šè¿‡æ¨¡å‹è¿è¡Œã€‚ä¸ºäº†ä½¿äº‹æƒ…å˜å¾—ç®€å•ï¼Œæˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ä½¿è¿™ä¸€åˆ‡åŒæ—¶å‘ç”Ÿã€‚
è®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ã€‚:)

å¯¼å…¥æ‰€éœ€çš„åº“

```
import googleapiclient.discovery
```

åˆ›å»º youtube å®¢æˆ·ç«¯

```
DEVELOPER_KEY = 'YOUR_API_KEY' 
youtube_client = googleapiclient.discovery.build('youtube', 'v3', developerKey=DEVELOPER_KEY)
```

è¯¥å†™å‡½æ•°äº†ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
ä¸€æ—¦æ”¶åˆ°æ•°æ®ï¼Œæˆ‘ä»¬å°†ä»ä¸­æå–æ‰€éœ€çš„å­—æ®µ

å¯¹äºè¿™ä¸¤ä¸ªæ–‡æœ¬å­—æ®µï¼Œæ–‡æœ¬å°†ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„æ¸…ç†å‡½æ•°è¿›è¡Œæ¸…ç†ï¼Œç„¶åé€šè¿‡æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„æ ‡è®°å™¨è¿›è¡Œå¤„ç†ï¼Œæœ€åï¼Œå¡«å……å°†è¢«æ·»åŠ åˆ°æ–‡æœ¬ä¸­ã€‚

å°†ä½¿ç”¨ next å‡½æ•°è®¡ç®— timesec å­—æ®µã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œå®ƒå°†è·å–ç›®æ ‡è§†é¢‘å‘å¸ƒæ—¥æœŸå’Œä»Šå¤©ä¹‹é—´çš„æ—¶é—´æ®µ(ä»¥åˆ†é’Ÿä¸ºå•ä½)ã€‚

æ¥ä¸‹æ¥ï¼Œæ‰€æœ‰æ•°å­—ç‰¹å¾å°†é€šè¿‡ç¼©æ”¾å™¨è¿›è¡Œå½’ä¸€åŒ–ã€‚ä¸€æ—¦å¤„ç†å®Œæˆï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†é€šè¿‡ä¼ é€’æ¨¡å‹æ¥é¢„æµ‹å¯¹è§†é¢‘çš„ä¸å–œæ¬¢ç¨‹åº¦ï¼Œè¯¥å‡½æ•°å°†åœ¨ä¸€äº›å…¶ä»–æœ‰ç”¨çš„ä¿¡æ¯ä¸­è¿”å›é¢„æµ‹çš„ä¸å–œæ¬¢æ•°é‡ã€‚

```
def realtime(youtube,video_id):
  def calTimesss(time):
    start = datetime.strptime(time, '%Y-%m-%dT%H:%M:%S%z')
    end =   datetime.now()
    return np.round((end - start.replace(tzinfo=end.tzinfo)).total_seconds() / 60, 2)
  request = youtube.videos().list(part="snippet, statistics",id=video_id)
  response = request.execute()

  desc = response['items'][0]['snippet']['description']
  desc  =[clean_text(desc)]
  desc = x_desc_tok.texts_to_sequences(desc)
  desc = pad_sequences(desc,maxlen=100, padding='post')

  tags = response['items'][0]['snippet']['tags']
  tags=(" ".join(tags))
  tags  =[clean_text(tags)]
  tags = x_tag_tok.texts_to_sequences(tags)
  tags = pad_sequences(tags,maxlen=100, padding='post')

  publishedAt = response['items'][0]['snippet']['publishedAt']
  timesec = calTimesss(publishedAt)
  viewcount = response['items'][0]['statistics']['viewCount']
  likeCount = response['items'][0]['statistics']['likeCount']
  commentCount = response['items'][0]['statistics']['commentCount']
  numaricdata = [[viewcount, likeCount,commentCount,timesec]]
  numaricdata = Sc.transform(numaricdata)

  pryd = model.predict([tags, desc, numaricdata])

  return {"predicted": int(pryd[0][0]), "info": {
      "video_id": video_id,
      "likes": likeCount,
      "commentCount": commentCount,
      "viewCount": viewcount,
      "publishedAt": publishedAt,
      "dislike": int(pryd[0][0]),
  }}
```

ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œå®ƒï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç»“æœâ€¦

```
video_id = "videoid"
realtime(youtube_client, video_id)
```

![](img/0a27606e0a68fb10fae1d0f5564b1eaa.png)

ä½œè€…å›¾ç‰‡

**å“ˆå“ˆ**ï¼ŒæˆåŠŸäº†ğŸ˜‚ ğŸ‰ã€‚æ ¹æ®æ¨¡å‹ï¼Œè¿™ä¸ªè§†é¢‘æœ‰ *1231* ä¸ªç£ç›˜ã€‚**å“å“Ÿ**ï¼Œå¾ˆå¤šè®¨åŒçš„äººğŸ˜Ÿ

å¥½äº†ï¼Œè¿™æ˜¯å…³äºä½¿ç”¨ç»„åˆæ•°æ®çš„æ•™ç¨‹çš„ç»“å°¾ï¼Œå¾ˆæœ‰è¶£ï¼Œå¸Œæœ›ä½ å–œæ¬¢ã€‚ä¹Ÿå¸Œæœ›ä½ ä¼šå°è¯•æ”¹å–„è¿™ç§æ¨¡å¼â€¦ç»§ç»­å­¦ä¹ ğŸ™‚

**é“¾æ¥ GitHub repo**:[https://GitHub . com/na FIU-dev/YouTube _ dislike _ prediction _ practice](https://github.com/nafiu-dev/youtube_dislike_prediction_practice)

**ä½ å¯ä»¥åœ¨è¿™é‡Œå’Œæˆ‘è”ç³»:**

[https://www.instagram.com/nafiu.dev](https://www.instagram.com/nafiu.dev)

**é¢†è‹±**:[https://www.linkedin.com/in/nafiu-nizar-93a16720b](https://www.linkedin.com/in/nafiu-nizar-93a16720b)

æˆ‘çš„å…¶ä»–å¸–å­:

[](https://medium.com/@nafiu.dev/end-to-end-full-stack-project-from-backend-frontend-and-machine-learning-to-ethical-hacking-series-3e53779e5aff) [## ä»åç«¯ã€å‰ç«¯å’Œæœºå™¨å­¦ä¹ åˆ°é“å¾·é»‘å®¢çš„ç«¯åˆ°ç«¯å…¨æ ˆé¡¹ç›®â€¦

### å—¨ï¼Œæ¬¢è¿å¤§å®¶å‚åŠ è¿™ä¸ªä»åç«¯å¼€å‘ã€å‰ç«¯å¼€å‘åˆ°æ„å»ºç«¯åˆ°ç«¯é¡¹ç›®çš„ç³»åˆ—

medium.com](https://medium.com/@nafiu.dev/end-to-end-full-stack-project-from-backend-frontend-and-machine-learning-to-ethical-hacking-series-3e53779e5aff) [](/create-a-boolean-image-classifier-fast-with-any-data-set-with-a-brief-explanation-of-the-4b1265b5b33f) [## åˆ›å»ºä¸€ä¸ªå¸ƒå°”å›¾åƒåˆ†ç±»å™¨å¿«é€Ÿä¸ä»»ä½•æ•°æ®é›†ï¼Œå¹¶ç®€è¦è¯´æ˜äº†â€¦

### å¤§å®¶å¥½ï¼Œåœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€ç§å«åšå·ç§¯ç¥ç»ç½‘ç»œçš„ç¥ç»ç½‘ç»œâ€¦

pub.towardsai.net](/create-a-boolean-image-classifier-fast-with-any-data-set-with-a-brief-explanation-of-the-4b1265b5b33f) [](https://medium.com/@nafiu.dev/highlighting-the-most-popular-machine-learning-algorithms-implementing-it-4bf18b645163) [## çªå‡ºæœ€æµè¡Œçš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼›å®æ–½å®ƒ

### æ¦‚è¿°æœ€æµè¡Œçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ iris æ•°æ®é›†å®ç°å’Œæ¯”è¾ƒå‡†ç¡®æ€§

medium.com](https://medium.com/@nafiu.dev/highlighting-the-most-popular-machine-learning-algorithms-implementing-it-4bf18b645163)